{"id": "2511.16947", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16947", "abs": "https://arxiv.org/abs/2511.16947", "authors": ["Chenqi Zhao", "Wenfei Wu", "Linhai Song", "Yuchen Xu"], "title": "MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling", "comment": "19 pages", "summary": "Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.\n  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.", "AI": {"tldr": "MicroEP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8de8GPU\u7684\u9ad8\u6548token\u8c03\u5ea6\u5b9e\u73b0MoE\u7cfb\u7edf\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u5747\u8861\u3002MicroMoE\u7cfb\u7edf\u57fa\u4e8e\u6b64\u7b56\u7565\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5c06\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe47.6%\u3002", "motivation": "MoE\u7684\u52a8\u6001\u7279\u6027\u5bfc\u81f4\u4e13\u5bb6\u95f4\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u4e25\u91cd\u5f71\u54cd\u8bad\u7ec3\u6548\u7387\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\uff0c\u8981\u4e48\u5f15\u5165\u989d\u5916\u7cfb\u7edf\u5f00\u9500\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u5747\u8861\u3002", "method": "\u63d0\u51faMicroEP\u5e76\u884c\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u9ad8\u6548\u7684token\u8c03\u5ea6\u5728\u6bcf\u5fae\u6279\u6b21\u4e2d\u5b9e\u73b0\u6700\u4f18\u8d1f\u8f7d\u5747\u8861\u3002\u6784\u5efaMicroMoE\u5206\u5e03\u5f0fMoE\u8bad\u7ec3\u7cfb\u7edf\uff0c\u96c6\u6210MicroEP\u7684\u8d1f\u8f7d\u5747\u8861\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMicroMoE\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5c06\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe47.6%\uff0c\u51e0\u4e4e\u6301\u7eed\u5b9e\u73b0GPU\u95f4\u7684\u6700\u4f18\u8d1f\u8f7d\u5747\u8861\u3002", "conclusion": "MicroEP\u548cMicroMoE\u6210\u529f\u89e3\u51b3\u4e86MoE\u7cfb\u7edf\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16964", "categories": ["cs.MA", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16964", "abs": "https://arxiv.org/abs/2511.16964", "authors": ["Kirill Nagaitsev", "Luka Grbcic", "Samuel Williams", "Costin Iancu"], "title": "Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems", "comment": null, "summary": "Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent systems can effectively perform such tuning, often outperforming existing compilers and eliminating the need for manual kernel development. However, the dynamics of multi-agent systems for this task remain unexplored. In this work, we present a logical framework for comparing multi-agent PyTorch optimization systems. Our evaluation shows that exploit-heavy strategies perform best when paired with error-fixing agents, and that performance correlates with the granularity of optimization steps. The best implementation achieves an average 2.88x speedup on an H100 GPU across diverse tasks in KernelBench, a benchmark suite covering a range of machine learning architectures in PyTorch.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6bd4\u8f83\u591a\u667a\u80fd\u4f53PyTorch\u4f18\u5316\u7cfb\u7edf\u7684\u903b\u8f91\u6846\u67b6\uff0c\u53d1\u73b0\u5229\u7528\u6027\u5f3a\u7684\u7b56\u7565\u4e0e\u9519\u8bef\u4fee\u590d\u667a\u80fd\u4f53\u7ed3\u5408\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u6027\u80fd\u4e0e\u4f18\u5316\u6b65\u9aa4\u7684\u7c92\u5ea6\u76f8\u5173\uff0c\u6700\u4f73\u5b9e\u73b0\u5728H100 GPU\u4e0a\u5e73\u5747\u5b9e\u73b02.88\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edfGPU\u6027\u80fd\u4f18\u5316\u65b9\u6cd5\u9700\u8981\u7f16\u5199\u81ea\u5b9a\u4e49\u5185\u6838\u548c\u4f7f\u7528\u4e13\u7528\u7f16\u8bd1\u5668\uff0c\u800c\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u6709\u6548\u8fdb\u884c\u6b64\u7c7b\u4f18\u5316\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u52a8\u6001\u7279\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u903b\u8f91\u6846\u67b6\u6765\u6bd4\u8f83\u591a\u667a\u80fd\u4f53PyTorch\u4f18\u5316\u7cfb\u7edf\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u7b56\u7565\u4e0e\u9519\u8bef\u4fee\u590d\u667a\u80fd\u4f53\u7684\u7ec4\u5408\u6548\u679c\uff0c\u4ee5\u53ca\u4f18\u5316\u6b65\u9aa4\u7c92\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5229\u7528\u6027\u5f3a\u7684\u7b56\u7565\u4e0e\u9519\u8bef\u4fee\u590d\u667a\u80fd\u4f53\u7ed3\u5408\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u6027\u80fd\u4e0e\u4f18\u5316\u6b65\u9aa4\u7684\u7c92\u5ea6\u76f8\u5173\uff0c\u6700\u4f73\u5b9e\u73b0\u5728KernelBench\u57fa\u51c6\u5957\u4ef6\u7684\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u5e73\u5747\u5b9e\u73b02.88\u500d\u52a0\u901f\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728GPU\u6027\u80fd\u4f18\u5316\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u7279\u5b9a\u7b56\u7565\u7ec4\u5408\u548c\u4f18\u5316\u7c92\u5ea6\u63a7\u5236\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316AI\u63a8\u7406\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.16814", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.16814", "abs": "https://arxiv.org/abs/2511.16814", "authors": ["Silvia Rondini", "Claudia Alvarez-Martin", "Paula Angermair-Barkai", "Olivier Penacchio", "M. Paz", "Matthew Pelowski", "Dan Dediu", "Antoni Rodriguez-Fornells", "Xim Cerda-Company"], "title": "Stable diffusion models reveal a persisting human and AI gap in visual creativity", "comment": null, "summary": "While recent research suggests Large Language Models match human creative performance in divergent thinking tasks, visual creativity remains underexplored. This study compared image generation in human participants (Visual Artists and Non Artists) and using an image generation AI model (two prompting conditions with varying human input: high for Human Inspired, low for Self Guided). Human raters (N=255) and GPT4o evaluated the creativity of the resulting images. We found a clear creativity gradient, with Visual Artists being the most creative, followed by Non Artists, then Human Inspired generative AI, and finally Self Guided generative AI. Increased human guidance strongly improved GenAI's creative output, bringing its productions close to those of Non Artists. Notably, human and AI raters also showed vastly different creativity judgment patterns. These results suggest that, in contrast to language centered tasks, GenAI models may face unique challenges in visual domains, where creativity depends on perceptual nuance and contextual sensitivity, distinctly human capacities that may not be readily transferable from language models.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.16837", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16837", "abs": "https://arxiv.org/abs/2511.16837", "authors": ["Oliver Kramer"], "title": "Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs", "comment": "6 pages, Submitted to ESANN 2026", "summary": "Cognitive BASIC is a minimal, BASIC-style prompting language and in-model interpreter that structures large language model (LLM) reasoning into explicit, stepwise execution traces. Inspired by the simplicity of retro BASIC, we repurpose numbered lines and simple commands as an interpretable cognitive control layer. Modern LLMs can reliably simulate such short programs, enabling transparent multi-step reasoning inside the model. A natural-language interpreter file specifies command semantics, memory updates, and logging behavior. Our mental-model interpreter extracts declarative and procedural knowledge, detects contradictions, and produces resolutions when necessary. A comparison across three LLMs on a benchmark of knowledge extraction, conflict detection, and reasoning tasks shows that all models can execute Cognitive BASIC programs, with overall strong but not uniform performance.", "AI": {"tldr": "Cognitive BASIC\u662f\u4e00\u79cd\u57fa\u4e8eBASIC\u98ce\u683c\u7684\u6700\u5c0f\u5316\u63d0\u793a\u8bed\u8a00\u548c\u6a21\u578b\u5185\u89e3\u91ca\u5668\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u7ed3\u6784\u5316\u4e3a\u663e\u5f0f\u7684\u9010\u6b65\u6267\u884c\u8f68\u8ff9\u3002", "motivation": "\u53d7\u590d\u53e4BASIC\u7b80\u5355\u6027\u7684\u542f\u53d1\uff0c\u91cd\u65b0\u5229\u7528\u7f16\u53f7\u884c\u548c\u7b80\u5355\u547d\u4ee4\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u8ba4\u77e5\u63a7\u5236\u5c42\uff0c\u4f7f\u73b0\u4ee3LLM\u80fd\u591f\u53ef\u9760\u5730\u6a21\u62df\u8fd9\u7c7b\u77ed\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u6a21\u578b\u5185\u90e8\u7684\u900f\u660e\u591a\u6b65\u63a8\u7406\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5668\u6587\u4ef6\u6307\u5b9a\u547d\u4ee4\u8bed\u4e49\u3001\u5185\u5b58\u66f4\u65b0\u548c\u65e5\u5fd7\u884c\u4e3a\uff0c\u901a\u8fc7\u5fc3\u7406\u6a21\u578b\u89e3\u91ca\u5668\u63d0\u53d6\u58f0\u660e\u6027\u548c\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff0c\u68c0\u6d4b\u77db\u76fe\u5e76\u5728\u5fc5\u8981\u65f6\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u4e09\u4e2aLLM\u4e0a\u5bf9\u77e5\u8bc6\u63d0\u53d6\u3001\u51b2\u7a81\u68c0\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u6240\u6709\u6a21\u578b\u90fd\u80fd\u6267\u884cCognitive BASIC\u7a0b\u5e8f\uff0c\u6574\u4f53\u8868\u73b0\u5f3a\u52b2\u4f46\u4e0d\u7edf\u4e00\u3002", "conclusion": "Cognitive BASIC\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u6765\u7ec4\u7ec7LLM\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.17076", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17076", "abs": "https://arxiv.org/abs/2511.17076", "authors": ["Peng Chen", "Jing Liang", "Kang-Jia Qiao", "Hui Song", "Tian-lei Ma", "Kun-Jie Yu", "Cai-Tong Yue", "Ponnuthurai Nagaratnam Suganthan", "Witold Pedryc"], "title": "A segment anchoring-based balancing algorithm for agricultural multi-robot task allocation with energy constraints", "comment": null, "summary": "Multi-robot systems have emerged as a key technology for addressing the efficiency and cost challenges in labor-intensive industries. In the representative scenario of smart farming, planning efficient harvesting schedules for a fleet of electric robots presents a highly challenging frontier problem. The complexity arises not only from the need to find Pareto-optimal solutions for the conflicting objectives of makespan and transportation cost, but also from the necessity to simultaneously manage payload constraints and finite battery capacity. When robot loads are dynamically updated during planned multi-trip operations, a mandatory recharge triggered by energy constraints introduces an unscheduled load reset. This interaction creates a complex cascading effect that disrupts the entire schedule and renders traditional optimization methods ineffective. To address this challenge, this paper proposes the segment anchoring-based balancing algorithm (SABA). The core of SABA lies in the organic combination of two synergistic mechanisms: the sequential anchoring and balancing mechanism, which leverages charging decisions as `anchors' to systematically reconstruct disrupted routes, while the proportional splitting-based rebalancing mechanism is responsible for the fine-grained balancing and tuning of the final solutions' makespans. Extensive comparative experiments, conducted on a real-world case study and a suite of benchmark instances, demonstrate that SABA comprehensively outperforms 6 state-of-the-art algorithms in terms of both solution convergence and diversity. This research provides a novel theoretical perspective and an effective solution for the multi-robot task allocation problem under energy constraints.", "AI": {"tldr": "\u63d0\u51faSABA\u7b97\u6cd5\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u80fd\u91cf\u7ea6\u675f\u4e0b\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6bb5\u951a\u5b9a\u548c\u5e73\u8861\u673a\u5236\u5904\u7406\u5145\u7535\u51b3\u7b56\u5e26\u6765\u7684\u8c03\u5ea6\u4e2d\u65ad\uff0c\u5728\u6536\u655b\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u519c\u4e1a\u7b49\u52b3\u52a8\u5bc6\u96c6\u578b\u4ea7\u4e1a\u4e2d\u9762\u4e34\u6548\u7387\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u519c\u4e1a\u573a\u666f\u4e2d\uff0c\u7535\u52a8\u673a\u5668\u4eba\u7684\u591a\u884c\u7a0b\u6536\u5272\u8c03\u5ea6\u9700\u8981\u8003\u8651\u80fd\u91cf\u7ea6\u675f\u3001\u8d1f\u8f7d\u7ea6\u675f\u548c\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u5145\u7535\u51b3\u7b56\u5e26\u6765\u7684\u590d\u6742\u7ea7\u8054\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u5206\u6bb5\u951a\u5b9a\u5e73\u8861\u7b97\u6cd5(SABA)\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u673a\u5236\uff1a\u987a\u5e8f\u951a\u5b9a\u548c\u5e73\u8861\u673a\u5236\u5229\u7528\u5145\u7535\u51b3\u7b56\u4f5c\u4e3a\u951a\u70b9\u7cfb\u7edf\u91cd\u6784\u4e2d\u65ad\u8def\u5f84\uff1b\u57fa\u4e8e\u6bd4\u4f8b\u5206\u5272\u7684\u518d\u5e73\u8861\u673a\u5236\u8d1f\u8d23\u7cbe\u7ec6\u8c03\u6574\u6700\u7ec8\u89e3\u7684\u65f6\u95f4\u8de8\u5ea6\u3002", "result": "\u5728\u771f\u5b9e\u6848\u4f8b\u548c\u57fa\u51c6\u5b9e\u4f8b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSABA\u5728\u89e3\u6536\u655b\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u5168\u9762\u4f18\u4e8e6\u79cd\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u80fd\u91cf\u7ea6\u675f\u4e0b\u7684\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16842", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16842", "abs": "https://arxiv.org/abs/2511.16842", "authors": ["Sang Truong", "Yuheng Tu", "Michael Hardy", "Anka Reuel", "Zeyu Tang", "Jirayu Burapacheep", "Jonathan Perera", "Chibuike Uwakwe", "Ben Domingue", "Nick Haber", "Sanmi Koyejo"], "title": "Fantastic Bugs and Where to Find Them in AI Benchmarks", "comment": null, "summary": "Benchmarks are pivotal in driving AI progress, and invalid benchmark questions frequently undermine their reliability. Manually identifying and correcting errors among thousands of benchmark questions is not only infeasible but also a critical bottleneck for reliable evaluation. In this work, we introduce a framework for systematic benchmark revision that leverages statistical analysis of response patterns to flag potentially invalid questions for further expert review. Our approach builds on a core assumption commonly used in AI evaluations that the mean score sufficiently summarizes model performance. This implies a unidimensional latent construct underlying the measurement experiment, yielding expected ranges for various statistics for each item. When empirically estimated values for these statistics fall outside the expected range for an item, the item is more likely to be problematic. Across nine widely used benchmarks, our method guides expert review to identify problematic questions with up to 84\\% precision. In addition, we introduce an LLM-judge first pass to review questions, further reducing human effort. Together, these components provide an efficient and scalable framework for systematic benchmark revision.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u4fee\u8ba2\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u54cd\u5e94\u6a21\u5f0f\u7684\u7edf\u8ba1\u7279\u5f81\u6765\u8bc6\u522b\u53ef\u80fd\u65e0\u6548\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u4e13\u5bb6\u5ba1\u67e5\u548cLLM\u521d\u6b65\u7b5b\u9009\uff0c\u63d0\u9ad8\u57fa\u51c6\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u57fa\u51c6\u6d4b\u8bd5\u5bf9AI\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e0\u6548\u7684\u57fa\u51c6\u95ee\u9898\u4f1a\u7834\u574f\u5176\u53ef\u9760\u6027\u3002\u624b\u52a8\u8bc6\u522b\u548c\u4fee\u6b63\u6570\u5343\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0d\u53ef\u884c\uff0c\u6210\u4e3a\u53ef\u9760\u8bc4\u4f30\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u57fa\u4e8eAI\u8bc4\u4f30\u4e2d\u5e38\u7528\u7684\u6838\u5fc3\u5047\u8bbe\uff08\u5747\u503c\u8db3\u4ee5\u603b\u7ed3\u6a21\u578b\u6027\u80fd\uff09\uff0c\u6784\u5efa\u5355\u7ef4\u6f5c\u5728\u7ed3\u6784\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u8ba1\u5f02\u5e38\u68c0\u6d4b\u8bc6\u522b\u95ee\u9898\u9879\uff0c\u5e76\u5f15\u5165LLM\u6cd5\u5b98\u8fdb\u884c\u521d\u6b65\u7b5b\u9009\u3002", "result": "\u57289\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6307\u5bfc\u4e13\u5bb6\u5ba1\u67e5\u8bc6\u522b\u95ee\u9898\u95ee\u9898\u7684\u7cbe\u786e\u5ea6\u9ad8\u8fbe84%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\u4fee\u8ba2\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u4fee\u6b63\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u65e0\u6548\u95ee\u9898\u3002"}}
{"id": "2511.17332", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17332", "abs": "https://arxiv.org/abs/2511.17332", "authors": ["Virginia Dignum", "Frank Dignum"], "title": "Agentifying Agentic AI", "comment": "10 pages; 1 figure", "summary": "Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5c06AAMAS\u793e\u533a\u5f00\u53d1\u7684BDI\u67b6\u6784\u3001\u901a\u4fe1\u534f\u8bae\u3001\u673a\u5236\u8bbe\u8ba1\u548c\u5236\u5ea6\u5efa\u6a21\u7b49\u6982\u5ff5\u5de5\u5177\u4f5c\u4e3a\u5b9e\u73b0\u667a\u80fdAI\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u5c06\u81ea\u9002\u5e94\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8c03\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u5177\u5907\u900f\u660e\u5ea6\u3001\u5408\u4f5c\u6027\u548c\u95ee\u8d23\u5236\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u667a\u80fdAI\u7cfb\u7edf\u7684\u6301\u7eed\u81ea\u4e3b\u6027\u3001\u63a8\u7406\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u9700\u8981\u5c06\u667a\u80fd\u5047\u8bbe\u4e0e\u660e\u786e\u7684\u8ba4\u77e5\u3001\u5408\u4f5c\u548c\u6cbb\u7406\u6a21\u578b\u76f8\u7ed3\u5408\u3002", "method": "\u5229\u7528AAMAS\u793e\u533a\u5f00\u53d1\u7684\u6982\u5ff5\u5de5\u5177\uff0c\u5305\u62ecBDI\u67b6\u6784\u3001\u901a\u4fe1\u534f\u8bae\u3001\u673a\u5236\u8bbe\u8ba1\u548c\u5236\u5ea6\u5efa\u6a21\uff0c\u5c06\u81ea\u9002\u5e94\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8c03\u6a21\u578b\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u63a5\u5f62\u5f0f\u7406\u8bba\u548c\u5b9e\u8df5\u81ea\u4e3b\u6027\u7684\u667a\u80fd\u89c6\u89d2\uff0c\u4e3a\u6784\u5efa\u4e0d\u4ec5\u80fd\u529b\u5f3a\u3001\u7075\u6d3b\uff0c\u800c\u4e14\u900f\u660e\u3001\u5408\u4f5c\u548c\u53ef\u95ee\u8d23\u7684\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "AAMAS\u793e\u533a\u7684\u6982\u5ff5\u5de5\u5177\u4e3a\u667a\u80fdAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u6574\u5408\u8fd9\u4e9b\u5de5\u5177\u53ef\u4ee5\u6784\u5efa\u66f4\u53ef\u9760\u548c\u8d1f\u8d23\u4efb\u7684\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2511.16916", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16916", "abs": "https://arxiv.org/abs/2511.16916", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving", "comment": null, "summary": "In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.", "AI": {"tldr": "\u9488\u5bf9\u591a\u8f66\u8f86\u534f\u540c\u9a7e\u9a76\u4e2d\u4f20\u7edf\u5956\u52b1\u51fd\u6570\u5b58\u5728\u7684\u5956\u52b1\u5dee\u5f02\u6d88\u5931\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5dee\u5206\u5956\u52b1(HDR)\u673a\u5236\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u5dee\u5206\u5956\u52b1\u548c\u52a8\u4f5c\u68af\u5ea6\u5956\u52b1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u68af\u5ea6\u7684\u4fe1\u566a\u6bd4\u548c\u7b97\u6cd5\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u5728\u591a\u8f66\u8f86\u534f\u540c\u9a7e\u9a76\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u57fa\u4e8e\u72b6\u6001\u7684\u5956\u52b1\u51fd\u6570\u5b58\u5728\u5956\u52b1\u5dee\u5f02\u6d88\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u68af\u5ea6\u7684\u4fe1\u566a\u6bd4\u4f4e\uff0c\u4e25\u91cd\u5f71\u54cd\u7b97\u6cd5\u6536\u655b\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5dee\u5206\u5956\u52b1(HDR)\u673a\u5236\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u5168\u5c40\u52bf\u51fd\u6570\u7684\u65f6\u95f4\u5dee\u5206\u5956\u52b1(TRD)\u548c\u76f4\u63a5\u6d4b\u91cf\u52a8\u4f5c\u8fb9\u9645\u6548\u7528\u7684\u52a8\u4f5c\u68af\u5ea6\u5956\u52b1(ARG)\u3002\u5c06\u534f\u540c\u9a7e\u9a76\u95ee\u9898\u5efa\u6a21\u4e3a\u5177\u6709\u65f6\u53d8\u667a\u80fd\u4f53\u96c6\u7684\u591a\u667a\u80fd\u4f53\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u3002", "result": "\u4f7f\u7528\u5728\u7ebf\u89c4\u5212(MCTS)\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(QMIX\u3001MAPPO\u3001MADDPG)\u7b97\u6cd5\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHDR\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u7b56\u7565\u7a33\u5b9a\u6027\u3002", "conclusion": "HDR\u673a\u5236\u80fd\u591f\u5f15\u5bfc\u667a\u80fd\u4f53\u5b66\u4e60\u9ad8\u8d28\u91cf\u7684\u5408\u4f5c\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2511.17006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17006", "abs": "https://arxiv.org/abs/2511.17006", "authors": ["Tengxiao Liu", "Zifeng Wang", "Jin Miao", "I-Hung Hsu", "Jun Yan", "Jiefeng Chen", "Rujun Han", "Fangyuan Xu", "Yanfei Chen", "Ke Jiang", "Samira Daruki", "Yi Liang", "William Yang Wang", "Tomas Pfister", "Chen-Yu Lee"], "title": "Budget-Aware Tool-Use Enables Effective Agent Scaling", "comment": null, "summary": "Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only \"thinking\" in tokens but also \"acting\" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack \"budget awareness\" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to \"dig deeper\" on a promising lead or \"pivot\" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u660e\u786e\u5de5\u5177\u8c03\u7528\u9884\u7b97\u7ea6\u675f\u4e0b\u5982\u4f55\u6709\u6548\u6269\u5c55\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\uff0c\u63d0\u51fa\u4e86\u9884\u7b97\u8ddf\u8e2a\u5668\u548cBATS\u6846\u67b6\uff0c\u4f7f\u667a\u80fd\u4f53\u5177\u5907\u9884\u7b97\u610f\u8bc6\u5e76\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff0c\u4ece\u800c\u6539\u5584\u6210\u672c-\u6027\u80fd\u6269\u5c55\u66f2\u7ebf\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u5728\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u65f6\uff0c\u5355\u7eaf\u589e\u52a0\u5de5\u5177\u8c03\u7528\u9884\u7b97\u65e0\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u7f3a\u4e4f\"\u9884\u7b97\u610f\u8bc6\"\uff0c\u4f1a\u5feb\u901f\u8fbe\u5230\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u9884\u7b97\u8ddf\u8e2a\u5668\u63d2\u4ef6\u63d0\u4f9b\u6301\u7eed\u9884\u7b97\u610f\u8bc6\uff0c\u5e76\u5f00\u53d1BATS\u6846\u67b6\uff0c\u5229\u7528\u9884\u7b97\u610f\u8bc6\u52a8\u6001\u8c03\u6574\u89c4\u5212\u548c\u9a8c\u8bc1\u7b56\u7565\uff0c\u51b3\u5b9a\u662f\u6df1\u5165\u6316\u6398\u6709\u5e0c\u671b\u7684\u7ebf\u7d22\u8fd8\u662f\u8f6c\u5411\u65b0\u8def\u5f84\u3002", "result": "\u9884\u7b97\u611f\u77e5\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u6709\u5229\u7684\u6269\u5c55\u66f2\u7ebf\uff0c\u5e76\u63a8\u52a8\u4e86\u6210\u672c-\u6027\u80fd\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u6269\u5c55\u7684\u66f4\u900f\u660e\u548c\u539f\u5219\u6027\u7406\u89e3\u3002", "conclusion": "\u9884\u7b97\u611f\u77e5\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u6269\u5c55\u6027\u80fd\uff0c\u4e3a\u7406\u89e3\u6b64\u7c7b\u667a\u80fd\u4f53\u7684\u6269\u5c55\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2511.17038", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.17038", "abs": "https://arxiv.org/abs/2511.17038", "authors": ["Hao Chen", "Renzheng Zhang", "Scott S. Howard"], "title": "DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing", "comment": null, "summary": "From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \\textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \\textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u89e3\u91ca\u4e86\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51faDAPS++\u65b9\u6cd5\u5c06\u6269\u6563\u9636\u6bb5\u4e0e\u6570\u636e\u9a71\u52a8\u4f18\u5316\u89e3\u8026\uff0c\u901a\u8fc7\u66f4\u76f4\u63a5\u5730\u5229\u7528\u4f3c\u7136\u9879\u6307\u5bfc\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u65b9\u6cd5\u5728\u89e3\u51b3\u9006\u95ee\u9898\u65f6\uff0c\u5148\u9a8c\u63d0\u4f9b\u7684\u6307\u5bfc\u6709\u9650\uff0c\u91cd\u5efa\u4e3b\u8981\u7531\u6d4b\u91cf\u4e00\u81f4\u6027\u9879\u9a71\u52a8\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6269\u6563\u52a8\u529b\u5b66\u8131\u8282\u3002\u9700\u8981\u6f84\u6e05\u8fd9\u79cd\u7ed3\u6784\u5e76\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5c06\u6269\u6563\u91cd\u65b0\u89e3\u91ca\u4e3a\u671f\u671b\u6700\u5927\u5316\u6846\u67b6\u4e2d\u7684\u521d\u59cb\u5316\u9636\u6bb5\uff0c\u63d0\u51faDAPS++\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u9636\u6bb5\u4e0e\u6570\u636e\u9a71\u52a8\u4f18\u5316\u5b8c\u5168\u89e3\u8026\uff0c\u8ba9\u4f3c\u7136\u9879\u66f4\u76f4\u63a5\u5730\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "DAPS++\u9700\u8981\u66f4\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u548c\u6d4b\u91cf\u4f18\u5316\u6b65\u9aa4\uff0c\u5728\u591a\u79cd\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u7684\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "DAPS++\u901a\u8fc7\u89e3\u8026\u6269\u6563\u4e0e\u6570\u636e\u4f18\u5316\uff0c\u63d0\u4f9b\u4e86\u5bf9\u7edf\u4e00\u6269\u6563\u8f68\u8ff9\u6709\u6548\u6027\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9006\u95ee\u9898\u6c42\u89e3\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.17165", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17165", "abs": "https://arxiv.org/abs/2511.17165", "authors": ["Kesheng Chen", "Wenjian Luo", "Bang Zhang", "Zeping Yin", "Zipeng Ye"], "title": "MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward", "comment": null, "summary": "Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIR\u7684\u4e92\u60e0\u5185\u5728\u5956\u52b1\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u56de\u5408\u5956\u52b1\u573a\u666f\u4e0b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6fc0\u52b1\u667a\u80fd\u4f53\u63a2\u7d22\u5f71\u54cd\u961f\u53cb\u7684\u884c\u4e3a\uff0c\u6709\u6548\u4fc3\u8fdb\u56e2\u961f\u63a2\u7d22\u5e76\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u56de\u5408\u5956\u52b1\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u8054\u5408\u52a8\u4f5c\u8f68\u8ff9\u7684\u6307\u6570\u7ea7\u7a00\u758f\u6027\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u5f71\u54cd\u56e2\u961f\u72b6\u6001\u7684\u8054\u5408\u52a8\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u7a00\u758f\u5956\u52b1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MIR\uff08\u4e92\u60e0\u5185\u5728\u5956\u52b1\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u589e\u5f3a\u7b56\u7565\u3002MIR\u6fc0\u52b1\u4e2a\u4f53\u667a\u80fd\u4f53\u63a2\u7d22\u80fd\u591f\u5f71\u54cd\u961f\u53cb\u7684\u52a8\u4f5c\uff0c\u5f53\u4e0e\u539f\u59cb\u7b56\u7565\u7ed3\u5408\u65f6\uff0c\u80fd\u591f\u6709\u6548\u523a\u6fc0\u56e2\u961f\u63a2\u7d22\u3002", "result": "\u5728\u6269\u5c55\u7684MiniGrid-MA\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cMIR\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e0b\u3002", "conclusion": "MIR\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u901a\u8fc7\u4fc3\u8fdb\u667a\u80fd\u4f53\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\u548c\u56e2\u961f\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b97\u6cd5\u6027\u80fd\u3002"}}
{"id": "2511.17408", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17408", "abs": "https://arxiv.org/abs/2511.17408", "authors": ["Nathalie Kirch", "Samuel Dower", "Adrians Skapars", "Ekdeep Singh Lubana", "Dmitrii Krasheninnikov"], "title": "That's not natural: The Impact of Off-Policy Training Data on Probe Performance", "comment": "10 pages, EurIPS 2025 Workshop on Private AI Governance", "summary": "Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4f7f\u7528\u5408\u6210\u548coff-policy\u6570\u636e\u8bad\u7ec3LLM\u884c\u4e3a\u63a2\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u7f3a\u4e4fon-policy\u6570\u636e\u65f6\uff0c\u4f7f\u7528\u540c\u57dfoff-policy\u6570\u636e\u6bd4\u4e0d\u540c\u57dfon-policy\u6570\u636e\u66f4\u53ef\u9760\uff0c\u4f46\u57df\u504f\u79fb\u4ecd\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u7531\u4e8e\u8bb8\u591aLLM\u884c\u4e3a\u7684\u81ea\u7136\u6837\u672c\u7a00\u5c11\uff0c\u7814\u7a76\u8005\u4e0d\u5f97\u4e0d\u4f9d\u8d56\u5408\u6210\u6216off-policy\u7684LLM\u54cd\u5e94\u6765\u8bad\u7ec3\u63a2\u6d4b\u5668\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u63a2\u6d4b\u5668\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u57288\u79cd\u4e0d\u540cLLM\u884c\u4e3a\u4e0a\u6d4b\u8bd5\u7ebf\u6027\u548c\u6ce8\u610f\u529b\u63a2\u6d4b\u5668\uff0c\u8bc4\u4f30\u4e0d\u540c\u54cd\u5e94\u751f\u6210\u7b56\u7565\u5bf9\u63a2\u6d4b\u5668\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4eceoff-policy\u5230on-policy\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u54cd\u5e94\u751f\u6210\u7b56\u7565\u663e\u8457\u5f71\u54cd\u63a2\u6d4b\u5668\u6027\u80fd\uff0c\u6210\u529f\u4eceoff-policy\u6570\u636e\u6cdb\u5316\u5230\u6fc0\u52b1\u6a21\u578b\u4ea7\u751f\u76ee\u6807\u884c\u4e3a\u7684\u6d4b\u8bd5\u96c6\uff0c\u80fd\u9884\u6d4b\u6210\u529f\u7684on-policy\u6cdb\u5316\u3002Deception\u548cSandbagging\u63a2\u6d4b\u5668\u5728\u771f\u5b9e\u76d1\u63a7\u573a\u666f\u4e2d\u53ef\u80fd\u65e0\u6cd5\u4eceoff-policy\u6cdb\u5316\u5230on-policy\u6570\u636e\u3002", "conclusion": "\u8bad\u7ec3\u6570\u636e\u57df\u7684\u504f\u79fb\u6bd4\u54cd\u5e94\u751f\u6210\u7b56\u7565\u7684\u53d8\u5316\u5bfc\u81f4\u66f4\u5927\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5f3a\u8c03\u9700\u8981\u80fd\u66f4\u597d\u5904\u7406LLM\u76d1\u63a7\u4e2d\u5206\u5e03\u504f\u79fb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.17461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17461", "abs": "https://arxiv.org/abs/2511.17461", "authors": ["Jiaxi Liu", "Chengyuan Ma", "Hang Zhou", "Weizhe Tang", "Shixiao Liang", "Haoyang Ding", "Xiaopeng Li", "Bin Ran"], "title": "SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception", "comment": null, "summary": "Cooperative perception (CP) offers significant potential to overcome the limitations of single-vehicle sensing by enabling information sharing among connected vehicles (CVs). However, existing generic CP approaches need to transmit large volumes of perception data that are irrelevant to the driving safety, exceeding available communication bandwidth. Moreover, most CP frameworks rely on pre-defined communication partners, making them unsuitable for dynamic traffic environments. This paper proposes a Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework to address these challenges. SRA-CP introduces a decentralized protocol where connected agents continuously broadcast lightweight perception coverage summaries and initiate targeted cooperation only when risk-relevant blind zones are detected. A perceptual risk identification module enables each CV to locally assess the impact of occlusions on its driving task and determine whether cooperation is necessary. When CP is triggered, the ego vehicle selects appropriate peers based on shared perception coverage and engages in selective information exchange through a fusion module that prioritizes safety-critical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset against several representative baselines. Results show that SRA-CP achieves less than 1% average precision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of the communication bandwidth. Moreover, it improves the perception performance by 15% over existing selective CP methods that do not incorporate risk awareness.", "AI": {"tldr": "\u63d0\u51faSRA-CP\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u9009\u62e9\u6027\u534f\u4f5c\u611f\u77e5\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u5173\u952e\u5bf9\u8c61\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u5e26\u5bbd\u9700\u6c42", "motivation": "\u73b0\u6709\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u4f20\u8f93\u5927\u91cf\u4e0e\u9a7e\u9a76\u5b89\u5168\u65e0\u5173\u7684\u611f\u77e5\u6570\u636e\uff0c\u8d85\u51fa\u53ef\u7528\u901a\u4fe1\u5e26\u5bbd\uff0c\u4e14\u4f9d\u8d56\u9884\u5b9a\u4e49\u901a\u4fe1\u4f19\u4f34\uff0c\u4e0d\u9002\u5408\u52a8\u6001\u4ea4\u901a\u73af\u5883", "method": "\u5f15\u5165\u53bb\u4e2d\u5fc3\u5316\u534f\u8bae\uff0c\u8f66\u8f86\u6301\u7eed\u5e7f\u64ad\u8f7b\u91cf\u7ea7\u611f\u77e5\u8986\u76d6\u6458\u8981\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u98ce\u9669\u76f8\u5173\u76f2\u533a\u65f6\u542f\u52a8\u9488\u5bf9\u6027\u534f\u4f5c\uff1b\u5305\u542b\u611f\u77e5\u98ce\u9669\u8bc6\u522b\u6a21\u5757\u548c\u9009\u62e9\u6027\u4fe1\u606f\u4ea4\u6362\u878d\u5408\u6a21\u5757", "result": "\u76f8\u6bd4\u901a\u7528\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\uff0c\u5b89\u5168\u5173\u952e\u5bf9\u8c61\u5e73\u5747\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e1%\uff0c\u4ec5\u4f7f\u752820%\u901a\u4fe1\u5e26\u5bbd\uff1b\u76f8\u6bd4\u73b0\u6709\u9009\u62e9\u6027\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\uff0c\u611f\u77e5\u6027\u80fd\u63d0\u534715%", "conclusion": "SRA-CP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u534f\u4f5c\u611f\u77e5\u4e2d\u7684\u901a\u4fe1\u5e26\u5bbd\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387"}}
