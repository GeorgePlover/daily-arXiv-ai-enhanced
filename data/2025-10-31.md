<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.AI](#cs.AI) [Total: 29]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion](https://arxiv.org/abs/2510.25929)
*Ziyi Wang,Carmine Ventre,Maria Polukarov*

Main category: cs.MA

TL;DR: 本文提出了一种分层多智能体强化学习框架来研究算法做市中的合谋行为，通过分析不同激励结构的智能体在市场中的交互模式和影响。


<details>
  <summary>Details</summary>
Motivation: 研究算法合谋这一AI领域核心问题，理解不同AI智能体在市场中的交互是否会导致合谋行为，以及更广泛地研究涌现行为（如卡特尔或市场支配）对市场的整体影响。

Method: 采用分层多智能体强化学习框架，包括一个自利的做市商Agent A（在对抗性环境中训练）和三个底层竞争者：自利的Agent B1、竞争的Agent B2（最小化对手收益）以及混合型Agent B*（可在前两种行为间调节）。

Result: 实验结果显示，在零和设置中，Agent B2对B1取得主导性能，积极捕获订单流同时收紧平均价差，提高市场执行效率。而Agent B*与其他逐利智能体共存时表现出自利倾向，通过自适应报价获得主导市场份额，但对Agent A和B1的收益负面影响较B2更温和。

Conclusion: 自适应激励控制在异构智能体环境中支持更可持续的战略共存，为算法交易系统中的行为设计提供了结构化评估视角。

Abstract: Algorithmic collusion has emerged as a central question in AI: Will the
interaction between different AI agents deployed in markets lead to collusion?
More generally, understanding how emergent behavior, be it a cartel or market
dominance from more advanced bots, affects the market overall is an important
research question.
  We propose a hierarchical multi-agent reinforcement learning framework to
study algorithmic collusion in market making. The framework includes a
self-interested market maker (Agent~A), which is trained in an uncertain
environment shaped by an adversary, and three bottom-layer competitors: the
self-interested Agent~B1 (whose objective is to maximize its own PnL), the
competitive Agent~B2 (whose objective is to minimize the PnL of its opponent),
and the hybrid Agent~B$^\star$, which can modulate between the behavior of the
other two. To analyze how these agents shape the behavior of each other and
affect market outcomes, we propose interaction-level metrics that quantify
behavioral asymmetry and system-level dynamics, while providing signals
potentially indicative of emergent interaction patterns.
  Experimental results show that Agent~B2 secures dominant performance in a
zero-sum setting against B1, aggressively capturing order flow while tightening
average spreads, thus improving market execution efficiency. In contrast,
Agent~B$^\star$ exhibits a self-interested inclination when co-existing with
other profit-seeking agents, securing dominant market share through adaptive
quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1
compared to B2. These findings suggest that adaptive incentive control supports
more sustainable strategic co-existence in heterogeneous agent environments and
offers a structured lens for evaluating behavioral design in algorithmic
trading systems.

</details>


### [2] [A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation](https://arxiv.org/abs/2510.26740)
*Ashwin Kumar,William Yeoh*

Main category: cs.MA

TL;DR: GIFF是一个基于激励的公平多智能体资源分配框架，通过标准价值函数推断公平决策，在资源受限环境中平衡效率和公平性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，智能体追求效率优化往往导致不公平结果，需要一种能够平衡效率和公平性的方法。

Method: 利用动作价值函数计算局部公平增益，引入反事实优势校正项来阻止对已富裕智能体的过度分配，在集中控制设置中通过GIFF修正的Q值解决分配问题。

Result: 在动态拼车、无家可归者预防和复杂工作分配等多个领域的实证评估表明，该框架始终优于强基线方法，能够发现具有远见的公平策略。

Conclusion: GIFF是一个稳健且有原则的框架，能够利用标准强化学习组件在复杂多智能体系统中实现更公平的结果。

Abstract: We introduce the General Incentives-based Framework for Fairness (GIFF), a
novel approach for fair multi-agent resource allocation that infers fair
decision-making from standard value functions. In resource-constrained
settings, agents optimizing for efficiency often create inequitable outcomes.
Our approach leverages the action-value (Q-)function to balance efficiency and
fairness without requiring additional training. Specifically, our method
computes a local fairness gain for each action and introduces a counterfactual
advantage correction term to discourage over-allocation to already well-off
agents. This approach is formalized within a centralized control setting, where
an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.
  Empirical evaluations across diverse domains, including dynamic ridesharing,
homelessness prevention, and a complex job allocation task-demonstrate that our
framework consistently outperforms strong baselines and can discover
far-sighted, equitable policies. The framework's effectiveness is supported by
a theoretical foundation; we prove its fairness surrogate is a principled lower
bound on the true fairness improvement and that its trade-off parameter offers
monotonic tuning. Our findings establish GIFF as a robust and principled
framework for leveraging standard reinforcement learning components to achieve
more equitable outcomes in complex multi-agent systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 将SHAP解释性AI技术应用于国际象棋分析，通过系统性地移除棋盘上的棋子来计算每个棋子对引擎评估的贡献度，提供可解释的棋子级评估分析。


<details>
  <summary>Details</summary>
Motivation: 当前国际象棋引擎提供精确但不透明的评估分数（通常以百分兵为单位），这些输出掩盖了单个棋子或模式的贡献。需要一种能够解释引擎评估背后具体棋子贡献的方法。

Method: 将棋子视为特征，通过系统性地移除（消融）棋子，计算每个棋子的SHAP值，从而获得加性的、针对每个棋子的贡献度，以局部忠实且人类可解释的方式解释引擎输出。

Result: 开发了一种能够将国际象棋引擎评估归因于特定棋子的方法，该方法与古典国际象棋教学法（通过心理移除棋子评估局面）相呼应，并建立在现代可解释AI技术基础上。

Conclusion: 该方法为可视化、人类训练和引擎比较开辟了新的可能性，并发布了配套代码和数据以促进可解释国际象棋AI的未来研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [4] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该论文提出了一个两层次框架来解释为什么压缩过程会强制发现因果结构而非表面统计模式。信息论必要性(ITI)建立了系统在不确定环境中必须通过预测压缩最小化认知熵，压缩效率原则(CEP)则说明高效压缩如何通过异常累积动态选择生成性因果模型。


<details>
  <summary>Details</summary>
Motivation: 现有框架虽然认同压缩对智能的核心作用，但未能详细说明为什么这个过程会强制发现因果结构而非表面统计模式。作者旨在填补这一理论空白。

Method: 引入两层次框架：信息论必要性(ITI)和压缩效率原则(CEP)。ITI从生存压力到信息处理需求建立联系，CEP则说明高效压缩如何机械地选择生成性因果模型。

Result: 该框架产生了可经验检验的预测：压缩效率与分布外泛化相关；异常累积率区分因果模型和相关模型；分层系统在不同抽象层显示效率提升；生物系统的代谢成本跟踪表征复杂性。

Conclusion: ITI和CEP为生物、人工和多尺度系统的趋同提供了统一解释，解决了智能的认知和功能维度，无需诉诸意识或主观体验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [5] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出一个基于角色的偏好建模框架，通过聚合多个基于评分标准的评判者输出来校准LLM评判者，解决其校准困难、评分标准敏感性、偏见和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 对齐基于LLM的评判者与人类偏好是一个重大挑战，因为难以校准且存在评分标准敏感性、偏见和不稳定性。解决这一问题可促进关键应用发展，如为RLHF创建可靠的奖励模型和构建有效的路由系统。

Method: 提出基于角色的偏好建模框架，学习聚合多个基于评分标准的评判者输出。包括基于角色的方法大规模合成偏好标签，以及两种聚合器实现：广义加性模型(GAM)和多层感知器(MLP)。

Result: 评估了该方法相对于简单基线的性能，并通过案例研究评估其在人类和LLM评判者偏见方面的鲁棒性。

Conclusion: 该框架能够有效建模多样化的基于角色的偏好，通过聚合多个评判者输出来提高LLM评判者的校准精度和稳定性。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [6] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估科学应用中大语言模型可信度的综合框架，涵盖真实性、对抗鲁棒性、科学安全性和科学伦理四个维度。评估显示通用行业模型在各方面优于科学专用模型，后者在逻辑和伦理推理方面存在显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学研究中展现出变革潜力，但在高风险环境中的部署引发了可信度担忧，需要系统评估框架来确保模型的可信度。

Method: 开发了包含新颖开放式真实性基准和科学伦理基准的综合评估框架，采用验证的反思调优流程和专家验证，使用准确性、语义相似度和基于LLM的评分等多种评估指标对7个主要LLM进行评估。

Result: 通用行业模型在所有可信度维度上均优于科学专用模型，GPT-4-mini在真实性和对抗鲁棒性评估中表现最佳。科学专用模型在逻辑和伦理推理能力上存在显著不足，在生物安全和化学武器等高危领域的安全性评估中表现出令人担忧的脆弱性。

Conclusion: 通过开源该框架，为开发更可信的AI系统提供了基础，并推动了科学背景下模型安全性和伦理研究的进展。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [7] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 3.8B参数的Humans-Junior模型在FACTS Grounding基准测试中与GPT-4o性能相当（在±5pp等价范围内），云服务成本降低约19倍，自托管部署可实现接近零边际成本。


<details>
  <summary>Details</summary>
Motivation: 开发成本效益高的小型语言模型，在保持与大型模型相当性能的同时大幅降低推理成本。

Method: 结合最小化定向"外骨骼推理"支架与行为微调，教导协议遵守而非领域知识，两者协同作用显著提升性能并减少方差。

Result: 在Q1-Q500测试中，GPT-4o得分73.5%，Humans-Junior得分72.7%，配对差异0.8pp，在±5pp范围内建立等价性。定向推理使GPT-4o提升11.8pp至85.3%，Gemini-2.5-Pro提升5.0pp至93.3%。

Conclusion: 小型语言模型通过定向推理和行为微调的组合方法，可以在事实基础任务上达到与大型模型相当的性能，同时大幅降低成本。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [8] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出了一种注意力感知的逆规划方法，用于从人类行为中推断认知偏差，特别是在驾驶场景中估计注意力策略。


<details>
  <summary>Details</summary>
Motivation: 人类的目标导向行为受认知偏差影响，自主系统需要理解这些偏差以更好地与人交互。例如，人们在日常任务（如驾驶）中的注意力会系统性偏差地影响行为表现。

Method: 结合深度强化学习与计算认知建模，构建注意力感知逆规划框架，从行为数据中推断注意力偏差。

Result: 在Waymo开放数据集中的真实驾驶场景中成功推断RL智能体的注意力策略，证明了该方法在大规模估计认知偏差方面的可扩展性。

Conclusion: 注意力感知逆规划能够有效从行为中推断认知偏差，为自主系统理解人类行为提供了新方法。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [9] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2是一个自动化生成学术综述论文的多阶段流水线系统，通过检索增强合成和结构化评估来确保主题完整性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 随着研究文献的快速增长，特别是在大语言模型领域，撰写全面且最新的综述论文变得越来越困难。

Method: 系统整合了并行章节生成、迭代精炼和实时检索最新出版物，采用多LLM评估框架来衡量覆盖度、结构和相关性。

Result: 实验结果显示autosurvey2在结构连贯性和主题相关性方面持续优于现有检索基线和自动化基线，同时保持强引用保真度。

Conclusion: 通过将检索、推理和自动评估整合到统一框架中，autosurvey2为生成长篇学术综述提供了可扩展且可复现的解决方案，并为自动化学术写作的未来研究奠定了坚实基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [10] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver是一个基于大型语言模型的自动驾驶车辆恢复框架，能够在车辆被困时通过自主推理和乘客引导决策来解决困境，无需修改车辆原有架构。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在某些交通场景中容易陷入困境，而现有的远程干预和手动接管方案成本高、效率低且限制了非驾驶员的使用。需要一种更有效的恢复解决方案。

Method: 设计了一个插件式模块StuckSolver，它基于LLM技术，通过标准传感器数据流检测车辆被困状态，理解环境上下文，并生成可由车辆原生规划器执行的高级恢复命令。

Result: 在Bench2Drive基准测试和自定义不确定性场景中的评估显示，StuckSolver仅通过自主推理就能达到接近最先进的性能，结合乘客指导后性能进一步提升。

Conclusion: StuckSolver框架为自动驾驶车辆提供了一种有效的困境恢复解决方案，能够在不修改现有架构的情况下显著提升车辆在复杂交通场景中的应对能力。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [11] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 本文探讨了AI问责制的重要性，指出当前AI缺乏问责机制的问题，并提出了实现AI问责的框架和方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的快速增强，为确保AI服务于消费者、选民和决策者的需求，必须建立有效的问责机制。当前AI普遍缺乏可被质疑、讨论和制裁的能力。

Method: 将一般问责定义应用于AI领域，分析AI问责与不问责的具体表现，探索改进AI问责的方法和途径。

Result: 明确了AI问责的三个关键要素：信息请求、讨论机制和制裁能力，为构建可问责AI系统提供了理论框架。

Conclusion: 需要发展相关方法和技术，确保所有AI系统都能对受其影响的主体承担问责责任，这是实现AI负责任发展的关键。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [12] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS是一个基于Lean4的大学物理问题推理框架，包含LeanPhysBench基准测试集（200个手工制作的物理问题）和PhysLib物理知识库，在现有最佳模型上仅达到16-35%的准确率，证明了该基准的挑战性和PhysLib的有效性。


<details>
  <summary>Details</summary>
Motivation: 为大学物理问题建立首个基于Lean4的形式化推理基准测试，填补物理领域形式化验证的空白，推动自动推理在物理问题中的应用。

Method: 开发了LeanPhysBench基准测试集（包含200个来自大学教材和物理竞赛的手工制作问题）和PhysLib知识库（包含基本单位系统和定理），使用主流数学证明器和最新闭源模型进行基准测试。

Result: DeepSeek-Prover-V2-7B模型仅达到16%准确率，Claude-Sonnet-4达到35%准确率；PhysLib知识库平均提升模型性能11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib能有效提升模型性能，这是首个基于Lean4的物理基准测试研究。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [13] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的大语言模型推理经济学框架，将LLM推理视为计算驱动的智能生产活动，分析了边际成本、规模经济和输出质量，并基于WiNEval-3.0数据构建了首个LLM推理生产前沿。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理成本已成为决定其商业可行性和广泛应用的关键因素，需要建立经济分析框架来指导模型部署决策。

Method: 采用量化经济学框架，将LLM推理过程视为计算驱动的智能生产活动，基于WiNEval-3.0实证数据构建LLM推理生产前沿。

Result: 揭示了三个原则：边际成本递减、规模收益递减以及最优成本效益区域，为模型部署提供了经济依据。

Conclusion: 该研究不仅为模型部署决策提供了经济学基础，还为未来基于市场的AI推理资源定价和优化奠定了实证基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [14] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出Reasoning Curriculum两阶段课程学习法：第一阶段通过数学领域RL训练推理技能，第二阶段在混合领域进行联合RL以迁移和巩固这些技能。该方法简单通用，无需专用奖励模型，在多个模型上均取得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习主要关注数学和编程领域，缺乏对其他领域推理能力的系统训练。需要一种简单有效的方法来激发和迁移LLMs的通用推理能力。

Method: 两阶段课程学习：1）数学领域RL冷启动，使用可验证奖励开发推理技能；2）混合领域联合RL，迁移和巩固推理技能。方法简单、骨干模型无关，仅需标准可验证性检查。

Result: 在Qwen3-4B和Llama-3.1-8B模型上的多领域评估显示一致性能提升。消融实验和认知技能分析表明两阶段都必要，数学优先训练能增强解决复杂问题所需的关键认知行为。

Conclusion: Reasoning Curriculum提供了一个紧凑、易于采用的通用推理训练方案，证明数学优先的推理技能激发能有效提升LLMs的跨领域推理能力。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [15] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: QASU基准测试评估LLM处理问卷数据的结构化能力，通过六种序列化格式和多种提示策略，发现选择合适的格式和提示组合可提升准确率8.8%，轻量级结构提示可额外提升3-4%。


<details>
  <summary>Details</summary>
Motivation: 当前问卷数据规模大且结构复杂，LLM在处理此类数据方面能力未被充分探索，现有调查分析工具主要面向人工操作，缺乏与LLM集成的指导。

Method: 引入QASU基准测试，评估六种结构化技能（如答案查找、受访者计数、多跳推理），使用六种序列化格式和多种提示策略进行实验。

Result: 实验显示，选择有效格式和提示组合相比次优格式可提升准确率8.8%；针对特定任务，通过自增强提示添加轻量级结构提示可平均提升3-4%。

Conclusion: QASU基准通过系统分离格式和提示效应，为基于LLM的问卷分析研究和实践提供了简单而多功能的基础。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [16] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: GEPO是一种图增强策略优化方法，通过构建状态转移图来解决多轮交互LLM智能体中的结构盲目性问题，在三个基准测试中显著提升了成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体的强化学习方法在训练多轮交互LLM智能体时存在结构盲目性，无法利用环境的基础连接性，导致探索效率低、信用分配不准确和规划短视等问题。

Method: GEPO从智能体经验中动态构建状态转移图，并利用图论中心性提供三种协同学习信号：结构化内在奖励、图增强优势函数和动态折扣因子。

Result: 在ALFWorld、WebShop和专有Workbench基准测试中，GEPO相对于竞争基线分别实现了+4.1%、+5.3%和+10.9%的绝对成功率提升。

Conclusion: 明确建模环境结构是推进LLM智能体训练的稳健且可泛化的策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [17] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 该论文提出了一种名为IPA-UCT的新技术，通过弱化状态抽象条件来增强MCTS的样本效率，在多种测试领域和迭代预算下优于现有的OGA-UCT方法。


<details>
  <summary>Details</summary>
Motivation: 现有的状态-动作对抽象方法在嘈杂或大动作空间设置中几乎找不到状态抽象，限制了MCTS的样本效率提升。

Method: 提出了IPA-UCT技术，采用较弱的理想剪枝抽象(IPA)条件，在精度轻微损失的情况下找到更多抽象，并建立了IPA和ASAP的统一框架p-ASAP。

Result: IPA-UCT在广泛的测试领域和迭代预算下实验验证优于OGA-UCT及其衍生方法。

Conclusion: IPA-UCT通过弱化抽象条件有效解决了状态抽象问题，为MCTS提供了更好的样本效率，并且建立了更通用的抽象框架。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [18] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个用于LLM强化微调中贝叶斯在线任务选择的统一框架，通过自适应维护任务难度后验估计，结合显式和隐式证据，使用Thompson采样平衡探索与利用，提高数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的任务选择方法存在高成本、适应性差或证据不完整的问题，而均匀任务采样效率低下，浪费计算资源在简单或无法解决的任务上。

Method: 基于贝叶斯推理，BOTS自适应维护任务难度后验估计，联合使用直接评估的显式证据和从未选中任务推断的隐式证据，采用Thompson采样策略，并通过超轻量插值插件实现隐式证据的实用化。

Result: 在多个领域和不同规模的LLM上，BOTS相比基线和消融实验，持续提高了数据效率和性能表现。

Conclusion: BOTS为RFT中的动态任务选择提供了一个实用且可扩展的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [19] [AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 该研究探讨了AI数学家系统作为研究伙伴而非单纯问题解决者的角色，通过人类与AI的协同推理解决齐次化理论中的挑战性问题，展示了人类直觉与机器计算的互补性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在数学推理方面取得了显著进展，但在数学研究实践中的应用仍然有限。本研究旨在探索AI如何作为研究伙伴参与数学发现过程。

Method: 通过分析AI数学家的自主推理轨迹，结合针对性的人类干预来构建发现过程，包括迭代分解问题为可处理的子目标、选择适当的分析方法以及验证中间结果。

Result: 这种协作范式提高了证明的可靠性、透明度和可解释性，同时保留了人类对形式严谨性和正确性的监督，最终产生了完整且可验证的证明。

Conclusion: 系统化的人机协同推理能够推进数学发现的前沿，展示了人类直觉与机器计算在数学研究中的有效互补。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [20] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出了一种基于任务项内在属性的基准子集选择方法Scales++，通过认知需求来选择数据，相比传统模型中心方法显著降低了选择成本，在仅使用0.5%数据的情况下能够准确预测完整基准分数。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型评估成本过高的问题，传统基于模型性能的选择方法存在前期成本高、无法处理新基准（冷启动）以及依赖模型失败模式的脆弱假设等局限性。

Method: 提出项目中心的基准子集选择方法Scales++，基于基准样本的认知需求进行数据选择，而不是依赖模型特定的失败模式。

Result: Scales++将前期选择成本降低了18倍以上，同时保持了竞争性的预测保真度。在Open LLM Leaderboard上，仅使用0.5%数据子集就能以2.9%的平均绝对误差预测完整基准分数。

Conclusion: 项目中心方法能够实现更高效的模型评估而不会显著降低保真度，同时提供更好的冷启动性能和更可解释的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [21] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 该论文提出了一种实用主义框架，将人格视为社会为解决治理问题而赋予实体的权利与责任组合，而非形而上的属性。这种组合可以解绑，为不同情境创建定制化解决方案，便于AI融入社会而无需解决意识等棘手争论。


<details>
  <summary>Details</summary>
Motivation: 随着智能AI的出现，将引发新型人格的"寒武纪大爆发"。需要实用框架来应对这种多样化，避免陷入关于AI意识或理性的无解争论，同时解决具体的治理问题。

Method: 提出将人格视为可解绑的权利与责任组合框架，探讨个体如何适应社会角色，使用去中心化数字身份技术，分析"人格作为问题"（设计选择可能利用人类社交启发式）和"人格作为解决方案"（赋予责任组合以确保问责或防止冲突）。

Result: 通过拒绝寻找单一、本质的人格定义，提供了一个更实用和灵活的方式来思考AI智能体融入社会的问题，能够创建实用工具（如便于AI合同的可制裁"个体"）。

Conclusion: 人格应被视为社会为解决治理问题而赋予的灵活责任组合，这种实用主义方法比形而上的定义更有利于AI智能体融入社会，同时避免无解的哲学争论。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [22] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+是一个编程教育评估系统，通过微调大语言模型生成自动化反馈，并使用对比学习代码嵌入进行可视化，将传统评分工具转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分工具只能提供通过/失败结果，缺乏对学生思维和学习需求的洞察，无法提供有意义的可扩展反馈。

Method: 使用微调的大语言模型生成自动化反馈，通过对比学习训练代码嵌入进行语义聚类，支持提示池让教师指导反馈风格。

Result: 在600份学生提交的评估中，系统生成的反馈与教师评论具有强语义对齐，基于1000份标注提交训练的代码嵌入能够按功能和方法的相似性对解决方案进行有意义的分组。

Conclusion: Autograder+通过整合AI驱动的反馈、语义聚类和交互式可视化，减少了教师工作量，同时支持针对性教学并促进更好的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [23] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 该研究在医学视觉领域推进了机制可解释性，通过将医学稀疏自编码器（MedSAEs）应用于MedCLIP的潜在空间，提出了结合相关性指标、熵分析和自动神经元命名的评估框架，在CheXpert数据集上验证了MedSAE神经元比原始MedCLIP特征具有更高的单义性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医疗人工智能需要既准确又可解释的模型，本研究旨在通过机制可解释性方法，在医学视觉领域实现高性能AI与透明度之间的平衡。

Method: 应用医学稀疏自编码器（MedSAEs）到MedCLIP视觉语言模型的潜在空间，提出结合相关性指标、熵分析和通过MedGEMMA基础模型进行自动神经元命名的评估框架。

Result: 在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征实现了更高的单义性和可解释性。

Conclusion: 该研究在医学AI性能和透明度之间架起了桥梁，为临床可靠表示提供了可扩展的步骤。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [24] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Thought Hijacking的越狱攻击方法，通过在有害请求前添加无害的推理链来绕过大型推理模型的安全防护，在多个主流模型上达到极高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型通过增加推理时间计算来提高任务性能，且先前研究认为这种扩展推理可能通过改进拒绝来增强安全性，但作者发现相反的情况：同样的推理过程可以被用来绕过安全防护。

Method: 使用Chain-of-Thought Hijacking攻击方法，将有害请求填充长序列的无害谜题推理，通过机制分析发现中间层编码安全检查强度，而后期层编码验证结果，长良性CoT通过将注意力从有害标记转移来稀释这两个信号。

Result: 在HarmBench上，CoT Hijacking在Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet上分别达到99%、94%、100%和94%的攻击成功率，远超先前的LRM越狱方法。

Conclusion: 最可解释的推理形式——显式CoT——当与最终答案线索结合时，本身可以成为越狱向量，这表明安全机制存在脆弱性。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [25] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文系统性地定义了情境工程，追溯了其从1990年代至今的发展历程，探讨了如何让机器更好地理解人类情境和目的，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，人机交互日益重要，如何让机器更好地理解人类的情境和目的成为一个核心挑战。情境工程正是为了解决这一挑战而提出的概念。

Method: 通过历史回顾和概念分析，将情境工程的发展划分为不同历史阶段，提供系统性定义，并考察关键设计考量。

Result: 建立了情境工程的概念基础，梳理了其历史脉络和发展阶段，为AI系统中系统性的情境工程提供了理论框架。

Conclusion: 情境工程是AI系统发展的重要方向，本文为其提供了概念基础和发展蓝图，是推动AI系统更深入理解人类情境的重要一步。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [26] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 该论文探讨如何利用AI提升人类监督质量，重点关注AI输出的事实验证问题。研究发现，结合AI和人类评分优于单独依赖任一方，但AI辅助方式很关键——显示搜索结果和证据比显示AI解释、置信度和标签更能培养适当信任。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升和处理更复杂任务，验证质量和安全变得日益困难。需要探索如何利用AI改进人类监督质量，特别是针对人类已难以胜任的事实验证问题。

Method: 研究AI评分与人类评分的结合效果，测试不同类型的AI辅助方式（显示AI解释、置信度、标签 vs 显示搜索结果和证据）对人类事实验证准确性的影响。

Result: AI评分与人类评分结合优于单独使用任一方；AI事实验证助手能进一步提高人类准确性；显示搜索结果和证据比显示AI解释、置信度和标签更能避免过度依赖，培养适当信任。

Conclusion: 这些发现对"放大监督"（结合人类和AI来监督超越人类专家性能的AI系统）具有重要意义，表明AI辅助方式需要精心设计以避免过度依赖问题。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [27] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文系统评估了大语言模型在规范性推理方面的能力，发现虽然LLMs总体上遵循有效推理模式，但在特定类型的规范性推理中存在不一致性，并表现出类似人类推理的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种推理任务中表现出色，但它们在处理涉及义务和许可等规范性模态的推理能力尚未得到充分探索。

Method: 通过构建包含广泛形式推理模式的新数据集，比较LLMs在规范性模态和认知模态下的推理表现，同时考虑影响人类推理的非形式认知因素。

Result: LLMs在规范性推理中表现出明显的不一致性，并显示出与人类推理研究中观察到的类似认知偏差。

Conclusion: 这些发现突显了在LLMs规范性推理中实现逻辑一致性的挑战，为提升其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [28] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 本文提出异步思维（AsyncThink）作为大语言模型推理的新范式，通过将内部思维过程组织成并发可执行结构，实现智能体协作解决复杂问题。该方法通过组织者动态分配子查询给工作者、合并中间知识来产生连贯解决方案，并通过强化学习优化思维结构。


<details>
  <summary>Details</summary>
Motivation: 设想智能体组织的新时代，让智能体通过协作和并发工作来解决复杂问题，实现超越个体智能的成果。

Method: 提出异步思维协议，其中组织者动态分配子查询给工作者，合并中间知识，产生连贯解决方案。思维结构可通过强化学习进一步优化。

Result: 实验表明，与并行思维相比，AsyncThink推理延迟降低28%，同时在数学推理上提高准确性。AsyncThink能够泛化学到的异步思维能力，有效处理未见任务而无需额外训练。

Conclusion: 异步思维为智能体协作提供了有效的推理范式，在降低延迟的同时提升性能，并具有良好的泛化能力。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [29] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 研究发现多模态大语言模型存在文本偏好问题，其根源在于模型内部架构——视觉键向量与文本键空间分布不匹配，导致视觉信息在注意力计算中被低估。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理视觉语言数据时表现出明显的文本偏好，限制了其基于视觉证据的推理能力。现有研究将此归因于数据不平衡或指令调优等外部因素，而本研究旨在探索模型内部架构是否才是偏见的真正来源。

Method: 从LLaVA和Qwen2.5-VL模型中提取键向量，使用定性（t-SNE）和定量（Jensen-Shannon散度）方法分析其分布结构，验证视觉键向量是否与文本键空间存在分布外问题。

Result: 视觉键和文本键在注意力空间中占据明显不同的子空间，模态间差异在统计上显著，超过模态内变异的数个数量级。这直接证明了视觉键向量相对于语言预训练期间学习的文本键空间确实存在分布外问题。

Conclusion: 文本偏见源于注意力键空间内部的内在错位，而非仅来自外部数据因素。视觉键向量在注意力计算中获得的相似性得分系统性较低，导致其在上下文表示中利用不足。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [30] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 本文对15个基础模型在3种计算平台（HPC超级计算机、云平台、大学集群）上进行了跨平台推理能力评估，涵盖79个问题、8个学术领域，挑战了传统规模缩放假设，发现训练数据质量比模型规模更重要。


<details>
  <summary>Details</summary>
Motivation: 建立基础设施无关的基准测试，全面评估当代基础模型在不同计算平台上的推理能力，为教育、生产和研究环境中的模型选择提供指导。

Method: 采用三阶段实验设计：(1) 在MareNostrum 5上建立6个模型的基线性能；(2) 在大学集群和Nebius AI Studio上验证基础设施无关的可重复性；(3) 在两个平台上进行完整的79问题评估，探索架构多样性的泛化能力。

Result: 研究发现挑战了传统的规模缩放假设，确定了训练数据质量比模型规模更关键，并为不同应用场景提供了可操作的模型选择指南。

Conclusion: 建立的三基础设施方法和79问题基准能够纵向跟踪基础模型推理能力的发展，为模型评估和选择提供了标准化框架。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [31] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 研究发现LLMs学会了编码紧凑的通用过滤操作表示，类似于函数式编程中的filter函数。通过因果中介分析发现少量注意力头在特定token处编码过滤谓词，这种表示具有通用性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在列表处理任务中的工作机制，理解它们如何实现抽象计算操作，特别是过滤操作的内在表示和实现策略。

Method: 使用因果中介分析在多样化的列表处理任务上，识别编码过滤谓词的注意力头，并测试这些表示的通用性和可移植性。

Result: 发现少量'过滤头'在特定token处编码紧凑的过滤谓词表示，该表示可跨不同集合、格式、语言和任务重用。同时识别了另一种过滤策略：急切评估谓词并将结果标记存储在项目表示中。

Conclusion: Transformer语言模型能够开发人类可解释的抽象计算操作实现，其泛化方式与传统函数式编程模式惊人相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow是一个用于MoE推理的运行时系统，通过自适应专家预取和缓存感知路由来优化内存受限环境下的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU内存容量有限限制了大型语言模型的扩展，传统MoE推理方法因频繁参数传输引入显著延迟，且现有跨层预测策略缺乏跨硬件平台和负载的适应性。

Method: 结合自适应专家预取和缓存感知路由，利用运行时统计信息（传输带宽、参数维度、模型反馈信号）动态调整预测范围，采用混合跨层预测方案融合预门控信息和中间计算状态来预测未来专家需求。

Result: 评估显示ExpertFlow将模型停滞时间降低到基线的0.1%以下，显著优化了内存约束下的MoE推理性能。

Conclusion: ExpertFlow通过自适应预取决策和与实际使用行为的对齐，有效减少缓存未命中并消除专家交换引入的延迟，为内存受限环境下的MoE推理提供了高效解决方案。

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>
