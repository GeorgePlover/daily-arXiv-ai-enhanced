{"id": "2512.03565", "categories": ["cs.DC", "cs.CE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.03565", "abs": "https://arxiv.org/abs/2512.03565", "authors": ["Luis Gall", "Samuel James Newcome", "Fabio Alexander Gratl", "Markus M\u00fchlh\u00e4u\u00dfer", "Manish Kumar Mishra", "Hans-Joachim Bungartz"], "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas", "comment": "20 pages, 8 figures. Submitted to the 5th International Conference on Computational Engineering (ICCE 2024). No changes were made after the peer review process", "summary": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.\n  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.\n  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728AutoPas\u7c92\u5b50\u6a21\u62df\u5e93\u4e2d\uff0c\u901a\u8fc7SIMD\u5411\u91cf\u5316\u6280\u672f\u4f18\u5316\u5206\u5b50\u95f4\u6210\u5bf9\u529b\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u987a\u5e8f\u5bf9\u6027\u80fd\u548c\u80fd\u8017\u7684\u5f71\u54cd\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5728\u539f\u5b50\u5c3a\u5ea6\u4e0a\u4e3a\u7269\u7406\u8fc7\u7a0b\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u6700\u4f18MD\u7b97\u6cd5\u53ef\u80fd\u5728\u8fd0\u884c\u65f6\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7SIMD\u5411\u91cf\u5316\u4f18\u5316\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u8ba1\u7b97\uff0c\u7279\u522b\u662f\u8003\u8651\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u5c45\u8bc6\u522b\u7b97\u6cd5\u7b49\u6a21\u62df\u7279\u5b9a\u53c2\u6570\u7684\u5f71\u54cd\u3002", "method": "1. \u63a2\u7d22\u591a\u79cdSIMD\u5411\u91cf\u5316\u6280\u672f\uff0c\u4f18\u5316\u7c92\u5b50\u95f4\u6210\u5bf9\u529b\u8ba1\u7b97\uff1b2. \u7814\u7a76\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u4e0d\u540c\u987a\u5e8f\uff1b3. \u5206\u6790\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u5c45\u8bc6\u522b\u7b97\u6cd5\u7b49\u6a21\u62df\u53c2\u6570\u7684\u5f71\u54cd\uff1b4. \u6269\u5c55AutoPas\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u5728\u8fd0\u884c\u65f6\u9009\u62e9\u6700\u4f18\u5411\u91cf\u5316\u987a\u5e8f\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5728\u8fd0\u884c\u65f6\u8003\u8651\u4e0d\u540c\u7684\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u987a\u5e8f\uff0c\u76f8\u6bd4AutoPas\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u529b\u8ba1\u7b97\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684SIMD\u5411\u91cf\u5316\u987a\u5e8f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u529b\u8ba1\u7b97\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u6a21\u62df\u7279\u5b9a\u53c2\u6570\uff08\u5982\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u5c45\u8bc6\u522b\u7b97\u6cd5\uff09\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u4e3aAutoPas\u5e93\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u3002"}}
{"id": "2512.03180", "categories": ["cs.MA", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03180", "abs": "https://arxiv.org/abs/2512.03180", "authors": ["Rafflesia Khan", "Declan Joyce", "Mansura Habiba"], "title": "AGENTSAFE: A Unified Framework for Ethical Assurance and Governance in Agentic AI", "comment": "12 pages, 1 figure", "summary": "The rapid deployment of large language model (LLM)-based agents introduces a new class of risks, driven by their capacity for autonomous planning, multi-step tool integration, and emergent interactions. It raises some risk factors for existing governance approaches as they remain fragmented: Existing frameworks are either static taxonomies driven; however, they lack an integrated end-to-end pipeline from risk identification to operational assurance, especially for an agentic platform. We propose AGENTSAFE, a practical governance framework for LLM-based agentic systems. The framework operationalises the AI Risk Repository into design, runtime, and audit controls, offering a governance framework for risk identification and assurance. The proposed framework, AGENTSAFE, profiles agentic loops (plan -> act -> observe -> reflect) and toolchains, and maps risks onto structured taxonomies extended with agent-specific vulnerabilities. It introduces safeguards that constrain risky behaviours, escalates high-impact actions to human oversight, and evaluates systems through pre-deployment scenario banks spanning security, privacy, fairness, and systemic safety. During deployment, AGENTSAFE ensures continuous governance through semantic telemetry, dynamic authorization, anomaly detection, and interruptibility mechanisms. Provenance and accountability are reinforced through cryptographic tracing and organizational controls, enabling measurable, auditable assurance across the lifecycle of agentic AI systems. The key contributions of this paper are: (1) a unified governance framework that translates risk taxonomies into actionable design, runtime, and audit controls; (2) an Agent Safety Evaluation methodology that provides measurable pre-deployment assurance; and (3) a set of runtime governance and accountability mechanisms that institutionalise trust in agentic AI ecosystems.", "AI": {"tldr": "AGENTSAFE\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b9e\u7528\u6cbb\u7406\u6846\u67b6\uff0c\u5c06AI\u98ce\u9669\u5e93\u8f6c\u5316\u4e3a\u8bbe\u8ba1\u3001\u8fd0\u884c\u65f6\u548c\u5ba1\u8ba1\u63a7\u5236\uff0c\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u98ce\u9669\u8bc6\u522b\u548c\u4fdd\u969c\u673a\u5236\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u7684\u5feb\u901f\u90e8\u7f72\u5e26\u6765\u4e86\u65b0\u7684\u98ce\u9669\u7c7b\u522b\uff0c\u5305\u62ec\u81ea\u4e3b\u89c4\u5212\u3001\u591a\u6b65\u9aa4\u5de5\u5177\u96c6\u6210\u548c\u6d8c\u73b0\u4ea4\u4e92\u7b49\u80fd\u529b\u3002\u73b0\u6709\u7684\u6cbb\u7406\u6846\u67b6\u8981\u4e48\u662f\u9759\u6001\u5206\u7c7b\u9a71\u52a8\uff0c\u7f3a\u4e4f\u4ece\u98ce\u9669\u8bc6\u522b\u5230\u64cd\u4f5c\u4fdd\u969c\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u667a\u80fd\u4f53\u5e73\u53f0\u3002", "method": "\u63d0\u51faAGENTSAFE\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u667a\u80fd\u4f53\u5faa\u73af\uff08\u8ba1\u5212->\u884c\u52a8->\u89c2\u5bdf->\u53cd\u601d\uff09\u548c\u5de5\u5177\u94fe\uff0c\u5c06\u98ce\u9669\u6620\u5c04\u5230\u6269\u5c55\u4e86\u667a\u80fd\u4f53\u7279\u5b9a\u6f0f\u6d1e\u7684\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\u4e2d\u3002\u5f15\u5165\u5b89\u5168\u7ea6\u675f\u673a\u5236\uff0c\u9650\u5236\u98ce\u9669\u884c\u4e3a\uff0c\u5c06\u9ad8\u5f71\u54cd\u884c\u52a8\u5347\u7ea7\u5230\u4eba\u5de5\u76d1\u7763\uff0c\u5e76\u901a\u8fc7\u6db5\u76d6\u5b89\u5168\u3001\u9690\u79c1\u3001\u516c\u5e73\u6027\u548c\u7cfb\u7edf\u5b89\u5168\u7684\u9884\u90e8\u7f72\u573a\u666f\u5e93\u8bc4\u4f30\u7cfb\u7edf\u3002", "result": "AGENTSAFE\u5728\u90e8\u7f72\u671f\u95f4\u901a\u8fc7\u8bed\u4e49\u9065\u6d4b\u3001\u52a8\u6001\u6388\u6743\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u4e2d\u65ad\u673a\u5236\u786e\u4fdd\u6301\u7eed\u6cbb\u7406\u3002\u901a\u8fc7\u52a0\u5bc6\u8ffd\u6eaf\u548c\u7ec4\u7ec7\u63a7\u5236\u52a0\u5f3a\u6765\u6e90\u548c\u95ee\u8d23\uff0c\u5b9e\u73b0\u53ef\u6d4b\u91cf\u3001\u53ef\u5ba1\u8ba1\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5168\u751f\u547d\u5468\u671f\u4fdd\u969c\u3002", "conclusion": "\u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a(1) \u5c06\u98ce\u9669\u5206\u7c7b\u6cd5\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u3001\u8fd0\u884c\u65f6\u548c\u5ba1\u8ba1\u63a7\u5236\u7684\u7edf\u4e00\u6cbb\u7406\u6846\u67b6\uff1b(2) \u63d0\u4f9b\u53ef\u6d4b\u91cf\u9884\u90e8\u7f72\u4fdd\u969c\u7684\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff1b(3) \u4e00\u5957\u8fd0\u884c\u65f6\u6cbb\u7406\u548c\u95ee\u8d23\u673a\u5236\uff0c\u5236\u5ea6\u5316\u667a\u80fd\u4f53AI\u751f\u6001\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002"}}
{"id": "2512.03048", "categories": ["cs.AI", "cs.CY", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03048", "abs": "https://arxiv.org/abs/2512.03048", "authors": ["Austin Spizzirri"], "title": "Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation", "comment": "Approx. 3,000 words, 10 pages. Philosophical analysis of AI alignment (process-based / syntropy framework)", "summary": "I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u6784\u60f3\u4e3a\u901a\u8fc7\u8fc7\u7a0b\u6027\u3001\u591a\u667a\u80fd\u4f53\u3001\u53d1\u5c55\u6027\u673a\u5236\u6784\u5efa\u5177\u6709\u71b5\u51cf\u548c\u7406\u6027\u54cd\u5e94\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u800c\u975e\u7f16\u7801\u56fa\u5b9a\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\u5185\u5bb9\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5185\u5bb9\u7684\u4ef7\u503c\u89c2\u89c4\u8303\u65b9\u6cd5\u5b58\u5728\"\u89c4\u8303\u9677\u9631\"\u95ee\u9898\uff0c\u7531\u4e8e\u4e8b\u5b9e-\u4ef7\u503c\u9e3f\u6c9f\u3001\u4ef7\u503c\u591a\u5143\u4e3b\u4e49\u548c\u6269\u5c55\u6846\u67b6\u95ee\u9898\u7684\u7ed3\u5408\uff0c\u5bfc\u81f4\u5185\u5bb9\u89c4\u8303\u5728\u7ed3\u6784\u4e0a\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u5bfb\u627e\u66f4\u53ef\u9760\u7684AI\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u54f2\u5b66\u8d21\u732e\uff1a1) \u9610\u8ff0\"\u89c4\u8303\u9677\u9631\"\u8bba\u8bc1\uff1b2) \u63d0\u51fa\"\u71b5\u51cf\"\u4f5c\u4e3a\u7406\u89e3\u591a\u667a\u80fd\u4f53\u5bf9\u9f50\u52a8\u6001\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff1b3) \u57fa\u4e8e\u76f8\u5bb9\u8bba\u6307\u5bfc\u63a7\u5236\u7406\u8bba\u5efa\u7acb\u771f\u5b9e\u4e0e\u6a21\u62df\u9053\u5fb7\u80fd\u529b\u7684\u529f\u80fd\u533a\u5206\uff0c\u5e76\u8bbe\u8ba1\u5177\u8eab\u5b9e\u9a8c\u8303\u5f0f\u548c\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5efa\u7acb\u4e86AI\u5bf9\u9f50\u7684\u65b0\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u5173\u4e8e\u4eba\u5de5\u7cfb\u7edf\u4e2d\u4ef7\u503c\u6d8c\u73b0\u548c\u9053\u5fb7\u80fd\u52a8\u6027\u7684\u5177\u4f53\u3001\u53ef\u8bc1\u4f2a\u9884\u6d4b\uff0c\u4f46\u5b9e\u8bc1\u9a8c\u8bc1\u4ecd\u5728\u8fdb\u884c\u4e2d\u3002", "conclusion": "AI\u5bf9\u9f50\u5e94\u8f6c\u5411\u8fc7\u7a0b\u6027\u3001\u53d1\u5c55\u6027\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u51cf\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u72b6\u6001\u5bf9\u9f50\uff0c\u800c\u975e\u8bd5\u56fe\u7f16\u7801\u56fa\u5b9a\u7684\u4ef7\u503c\u89c2\u5185\u5bb9\u3002\u8fd9\u4e3aAI\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u54f2\u5b66\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\u65b9\u5411\u3002"}}
{"id": "2512.03697", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.03697", "abs": "https://arxiv.org/abs/2512.03697", "authors": ["Rafael Ravedutti Lucio Machado", "Jan Eitzinger", "Georg Hager", "Gerhard Wellein"], "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs", "comment": "8 pages, 4 figures, conference", "summary": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\u5206\u6790\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u8f6f\u4ef6\u5305\u80fd\u6548\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u4f7f\u7528Intel Ice Lake/Sapphire Rapids CPU\u548cNvidia A40/A100 GPU\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u5b9e\u9a8c\u548c\u5206\u6790\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e0a\u80fd\u6548\u5206\u6790\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\uff08CPU\u548cGPU\uff09\u548c\u8f6f\u4ef6\u5de5\u5177\u8fdb\u884c\u80fd\u6548\u6d4b\u91cf\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7684\u80fd\u6548\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\u4f7f\u7528MPI\u5e76\u884c\u5316\uff0c\u5728\u5b8c\u6574\u7684Intel Ice Lake\u548cSapphire Rapids CPU\u63d2\u69fd\u4ee5\u53caNvidia A40\u548cA100 GPU\u4e0a\u8fd0\u884c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u8f6f\u4ef6\u5305\uff0c\u4f7f\u7528Likwid\u548cNvidia\u5206\u6790\u5de5\u5177\u8fdb\u884c\u6307\u6807\u6d4b\u91cf\u3002", "result": "\u5c55\u793a\u4e86\u4f7f\u7528Likwid\u548cNvidia\u5206\u6790\u5de5\u5177\u83b7\u5f97\u7684\u80fd\u6548\u6307\u6807\u548c\u6d4b\u91cf\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u5728\u5b9e\u9a8c\u548c\u5206\u6790\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5404\u79cd\u6311\u6218\u548c\u9677\u9631\uff0c\u5305\u62ec\u5de5\u5177\u4f7f\u7528\u3001\u6570\u636e\u6536\u96c6\u548c\u7ed3\u679c\u89e3\u91ca\u65b9\u9762\u7684\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u8fdb\u884c\u80fd\u6548\u5206\u6790\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u80fd\u6548\u5206\u6790\u7814\u7a76\u7684\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7ecf\u9a8c\u6559\u8bad\u548c\u6307\u5bfc\u3002"}}
{"id": "2512.03072", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.03072", "abs": "https://arxiv.org/abs/2512.03072", "authors": ["Hu Keyi"], "title": "Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI", "comment": null, "summary": "Current AI paradigms, as \"architects of experience,\" face fundamental challenges in explainability and value alignment. This paper introduces \"Weight-Calculatism,\" a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.", "AI": {"tldr": "\u63d0\u51faWeight-Calculatism\u8ba4\u77e5\u67b6\u6784\uff0c\u57fa\u4e8e\u903b\u8f91\u539f\u5b50\u548c\u57fa\u672c\u64cd\u4f5c\u6784\u5efa\u53ef\u89e3\u91ca\u7684AGI\u7cfb\u7edf\uff0c\u901a\u8fc7\u6743\u91cd\u8ba1\u7b97\u6a21\u578b\u5b9e\u73b0\u900f\u660e\u63a8\u7406\u548c\u4ef7\u503c\u5bf9\u9f50\u3002", "motivation": "\u5f53\u524dAI\u8303\u5f0f\u4f5c\u4e3a\"\u7ecf\u9a8c\u67b6\u6784\u5e08\"\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u548c\u4ef7\u503c\u5bf9\u9f50\u7684\u6839\u672c\u6311\u6218\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u4e8e\u7b2c\u4e00\u539f\u7406\u7684\u8ba4\u77e5\u67b6\u6784\u6765\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u3002", "method": "\u63d0\u51faWeight-Calculatism\u67b6\u6784\uff0c\u5c06\u8ba4\u77e5\u5206\u89e3\u4e3a\u4e0d\u53ef\u5206\u5272\u7684\u903b\u8f91\u539f\u5b50\u548c\u4e24\u4e2a\u57fa\u672c\u64cd\u4f5c\uff1a\u6307\u5411\u548c\u6bd4\u8f83\u3002\u51b3\u7b56\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u6743\u91cd\u8ba1\u7b97\u6a21\u578b\uff08\u6743\u91cd=\u6536\u76ca\u00d7\u6982\u7387\uff09\u5f62\u5f0f\u5316\uff0c\u6240\u6709\u503c\u53ef\u8ffd\u6eaf\u5230\u53ef\u5ba1\u8ba1\u7684\u521d\u59cb\u6743\u91cd\u96c6\u3002\u5b9e\u73b0\u57fa\u4e8e\u56fe\u7b97\u6cd5\u8ba1\u7b97\u5f15\u64ce\u548c\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u5de5\u4f5c\u6d41\u3002", "result": "\u8be5\u67b6\u6784\u5b9e\u73b0\u4e86\u900f\u660e\u3001\u7c7b\u4eba\u7684\u63a8\u7406\uff0c\u5e76\u5728\u524d\u6240\u672a\u6709\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u548c\u5bf9\u9f50\u7684AGI\u5960\u5b9a\u4e86\u5b9e\u8df5\u548c\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "Weight-Calculatism\u662f\u57fa\u4e8e\u7b2c\u4e00\u539f\u7406\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u539f\u5b50\u5206\u89e3\u5b9e\u73b0\u6839\u672c\u53ef\u89e3\u91ca\u6027\u3001\u5185\u5728\u901a\u7528\u6027\u548c\u53ef\u8ffd\u6eaf\u4ef7\u503c\u5bf9\u9f50\uff0c\u4e3aAGI\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2512.03825", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03825", "abs": "https://arxiv.org/abs/2512.03825", "authors": ["Aingeru Ramos", "Jose A Pascual", "Javier Navaridas", "Ivan Coluzza"], "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods", "comment": "14 pages, 7 figures (5 of them composed by 2 subfigures)", "summary": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e76\u884c\u56de\u706bMetropolis-Hastings\u7b97\u6cd5\u7684\u5e76\u884c\u5b9e\u73b0\uff0c\u4f7f\u7528OpenMP\u548cCUDA\u5206\u522b\u5728CPU\u548cGPU\u4e0a\u8fdb\u884c\u5e76\u884c\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\u3002", "motivation": "\u4f20\u7edfMCMC\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6784\u578b\u7a7a\u95f4\u65f6\u91c7\u6837\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u5e76\u884c\u56de\u706b\u6280\u672f\u867d\u7136\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u4f46\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\uff0c\u9700\u8981\u901a\u8fc7\u5e76\u884c\u5316\u6765\u514b\u670d\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u4e86\u5e76\u884c\u56de\u706bMetropolis-Hastings\u7b97\u6cd5\u7684\u5e76\u884c\u5b9e\u73b0\uff0c\u4f7f\u7528OpenMP\u5728CPU\u591a\u6838\u4e0a\u8fdb\u884c\u5e76\u884c\u5316\uff0c\u540c\u65f6\u4f7f\u7528CUDA\u5728GPU\u4e0a\u8fdb\u884c\u5e76\u884c\u52a0\u901f\u3002", "result": "\u4f7f\u752848\u6838CPU\u7684OpenMP\u7248\u672c\u5b9e\u73b0\u4e8652\u500d\u52a0\u901f\uff0cCUDA\u7248\u672c\u5b9e\u73b0\u4e86986\u500d\u52a0\u901f\uff0c\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u63d0\u4f9b\u4e86\u57fa\u51c6\u6bd4\u8f83\u3002", "conclusion": "\u5e76\u884c\u5316\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86MCMC/PT\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u5f97\u80fd\u591f\u7814\u7a76\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u5e76\u4e3a\u91cf\u5b50\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2512.03285", "categories": ["cs.MA", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03285", "abs": "https://arxiv.org/abs/2512.03285", "authors": ["Nafiul I. Khan", "Mansura Habiba", "Rafflesia Khan"], "title": "A Gossip-Enhanced Communication Substrate for Agentic AI: Toward Decentralized Coordination in Large-Scale Multi-Agent Systems", "comment": null, "summary": "As agentic platforms scale, agents are moving beyond fixed roles and predefined toolchains, creating an urgent need for flexible and decentralized coordination. Current structured communication protocols such as direct agent-to-agent messaging or MCP-style tool calls offer reliability, but they struggle to support the emergent and swarm-like intelligence required in large adaptive systems. Distributed agents must learn continuously, share context fluidly, and coordinate without depending solely on central planners.\n  This paper revisits gossip protocols as a complementary substrate for agentic communication. Gossip mechanisms, long valued in distributed systems for their decentralized and fault-tolerant properties, provide scalable and adaptive diffusion of knowledge and fill gaps that structured protocols alone cannot efficiently address. However, gossip also introduces challenges, including semantic relevance, temporal staleness, and limited guarantees on action consistency in rapidly changing environments.\n  We examine how gossip can support context-rich state propagation, resilient coordination under uncertainty, and emergent global awareness. We also outline open problems around semantic filtering, trust, and knowledge decay. Rather than proposing a complete framework, this paper presents a research agenda for integrating gossip into multi-agent communication stacks and argues that gossip is essential for future agentic ecosystems that must remain robust, adaptive, and self-organizing as their scale and autonomy increase.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5c06gossip\u534f\u8bae\u4f5c\u4e3a\u667a\u80fd\u4f53\u901a\u4fe1\u7684\u8865\u5145\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u81ea\u9002\u5e94\u7cfb\u7edf\u4e2d\u7ed3\u6784\u5316\u901a\u4fe1\u534f\u8bae\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u3001\u5bb9\u9519\u7684\u77e5\u8bc6\u4f20\u64ad\u548c\u534f\u8c03\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53\u5e73\u53f0\u89c4\u6a21\u6269\u5927\uff0c\u667a\u80fd\u4f53\u9700\u8981\u8d85\u8d8a\u56fa\u5b9a\u89d2\u8272\u548c\u9884\u5b9a\u4e49\u5de5\u5177\u94fe\uff0c\u5b9e\u73b0\u7075\u6d3b\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u3002\u73b0\u6709\u7ed3\u6784\u5316\u901a\u4fe1\u534f\u8bae\uff08\u5982\u76f4\u63a5\u6d88\u606f\u4f20\u9012\u6216MCP\u5f0f\u5de5\u5177\u8c03\u7528\uff09\u867d\u7136\u53ef\u9760\uff0c\u4f46\u96be\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u81ea\u9002\u5e94\u7cfb\u7edf\u6240\u9700\u7684\u6d8c\u73b0\u548c\u7fa4\u4f53\u667a\u80fd\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6gossip\u534f\u8bae\u4f5c\u4e3a\u667a\u80fd\u4f53\u901a\u4fe1\u7684\u8865\u5145\u57fa\u7840\u3002gossip\u673a\u5236\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4ee5\u5176\u53bb\u4e2d\u5fc3\u5316\u548c\u5bb9\u9519\u7279\u6027\u8457\u79f0\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u7684\u77e5\u8bc6\u4f20\u64ad\uff0c\u586b\u8865\u7ed3\u6784\u5316\u534f\u8bae\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u7684\u7a7a\u767d\u3002", "result": "\u5206\u6790\u4e86gossip\u5982\u4f55\u652f\u6301\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u72b6\u6001\u4f20\u64ad\u3001\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5f39\u6027\u534f\u8c03\u4ee5\u53ca\u6d8c\u73b0\u7684\u5168\u5c40\u610f\u8bc6\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u8bed\u4e49\u76f8\u5173\u6027\u3001\u65f6\u95f4\u9648\u65e7\u6027\u3001\u5feb\u901f\u53d8\u5316\u73af\u5883\u4e2d\u884c\u52a8\u4e00\u81f4\u6027\u4fdd\u969c\u6709\u9650\u7b49\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u5c06gossip\u96c6\u6210\u5230\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u6808\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u8ba4\u4e3agossip\u5bf9\u4e8e\u672a\u6765\u9700\u8981\u4fdd\u6301\u9c81\u68d2\u6027\u3001\u81ea\u9002\u5e94\u6027\u548c\u81ea\u7ec4\u7ec7\u6027\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u800c\u975e\u63d0\u51fa\u5b8c\u6574\u6846\u67b6\u3002"}}
{"id": "2512.03272", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03272", "abs": "https://arxiv.org/abs/2512.03272", "authors": ["Zhiyuan He", "Dingmin Wang"], "title": "When Do Symbolic Solvers Enhance Reasoning in Large Language Models?", "comment": null, "summary": "Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models \"overthink\" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u4f55\u65f6\u80fd\u589e\u5f3a\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u4ec5\u5728\u95ee\u9898\u9700\u8981\u6709\u9650\u9690\u5f0f\u63a8\u7406\u4f46\u6d89\u53ca\u5145\u5206\u641c\u7d22\u7a7a\u95f4\u65f6\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u91cd\u590d\u56de\u6eaf\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u751f\u6210\u957f\u601d\u7ef4\u94fe\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u5927\u91cftoken\u5f00\u9500\uff0c\u751a\u81f3\u56e0\"\u8fc7\u5ea6\u601d\u8003\"\u4ea7\u751f\u5197\u957f\u63a8\u7406\u94fe\u800c\u5bfc\u81f4\u9519\u8bef\u7b54\u6848\u3002\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u5229\u7528LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5c06\u63a8\u7406\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u7136\u540e\u7528\u7b26\u53f7\u6c42\u89e3\u5668\u89e3\u51b3\uff0c\u4f46\u4f55\u65f6\u8fd9\u79cd\u65b9\u6cd5\u80fd\u589e\u5f3a\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5c06\u63a8\u7406\u4efb\u52a1\u7ffb\u8bd1\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u7136\u540e\u4f7f\u7528\u7b26\u53f7\u6c42\u89e3\u5668\u89e3\u51b3\u3002\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u65b9\u6cd5\u4e0e\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1) \u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u4ec5\u5728\u95ee\u9898\u9700\u8981\u6709\u9650\u9690\u5f0f\u63a8\u7406\u4f46\u6d89\u53ca\u5145\u5206\u641c\u7d22\u7a7a\u95f4\u65f6\u6709\u6548\uff1b2) \u6700\u65b0LLM\uff08\u5982GPT-4o\uff09\u5728\u63a8\u7406\u6df1\u5ea6\u8f83\u6d45\u7684\u6f14\u7ece\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\uff1b3) \u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u5728\u9700\u8981\u91cd\u590d\u56de\u6eaf\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff1b4) \u5f53\u63d0\u4f9b\u58f0\u660e\u6027\u793a\u4f8b\u65f6\uff0c\u5373\u4f7f\u662fCodeLlama-13B\u4e5f\u80fd\u5728\u56f0\u96be\u7684\u6591\u9a6c\u8c1c\u9898\u4e0a\u8d85\u8d8aGPT-4o\u3002", "conclusion": "\u7b26\u53f7\u6c42\u89e3\u5668\u96c6\u6210\u65b9\u6cd5\u5728\u7279\u5b9a\u7c7b\u578b\u7684\u95ee\u9898\u4e0a\u80fd\u6709\u6548\u589e\u5f3a\u4f20\u7edf\u957f\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5927\u91cf\u641c\u7d22\u548c\u56de\u6eaf\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u4e0a\u3002\u8be5\u65b9\u6cd5\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8865\u5145\u9014\u5f84\uff0c\u4f46\u9700\u8981\u6839\u636e\u95ee\u9898\u7c7b\u578b\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.", "AI": {"tldr": "OD-MoE\u662f\u4e00\u4e2a\u5206\u5e03\u5f0fMoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5b8c\u5168\u6309\u9700\u52a0\u8f7d\u4e13\u5bb6\u53c2\u6570\uff0c\u6d88\u9664\u4e86\u4e13\u5bb6\u7f13\u5b58\u9700\u6c42\uff0c\u4f7fMoE\u6a21\u578b\u80fd\u591f\u5728GPU\u5185\u5b58\u5c0f\u4e8e1GB\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684\u4e13\u5bb6\u5378\u8f7d\u65b9\u6cd5\u867d\u7136\u5c06\u4e13\u5bb6\u53c2\u6570\u5b58\u50a8\u5728CPU\u5185\u5b58\u4e2d\uff0c\u4f46GPU\u5185\u5b58\u4e2d\u4fdd\u7559\u7684\u4e13\u5bb6\u7f13\u5b58\u5229\u7528\u7387\u8f83\u4f4e\uff0c\u65e0\u6cd5\u5728\u8d44\u6e90\u6781\u5ea6\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6709\u6548\u8fd0\u884c\u3002", "method": "OD-MoE\u91c7\u7528\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8282\u70b9\u95f4\u5e76\u884c\u5316\u4e13\u5bb6\u52a0\u8f7d\u548c\u4e13\u5bb6\u8ba1\u7b97\uff1b2) \u8d85\u51c6\u786e\u7684\u4eff\u771f\u9884\u6d4b\u5668\uff0c\u5728\u4e13\u5bb6\u8ba1\u7b97\u8fdb\u884c\u65f6\u63d0\u524d\u591a\u5c42\u9884\u6d4b\u4e13\u5bb6\u6fc0\u6d3b\u3002\u901a\u8fc7\u8fd9\u4e9b\u521b\u65b0\uff0cOD-MoE\u80fd\u591f\u5728\u4e13\u5bb6\u6fc0\u6d3b\u524d\u5373\u65f6\u5c06\u76ee\u6807\u4e13\u5bb6\u52a0\u8f7d\u5230\u5206\u5e03\u5f0f\u8282\u70b9\uff0c\u5e76\u5728\u4f7f\u7528\u540e\u7acb\u5373\u9a71\u9010\uff0c\u91ca\u653eGPU\u5185\u5b58\u4f9b\u540e\u7eed\u4e13\u5bb6\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1) OD-MoE\u8fbe\u523099.94%\u7684\u4e13\u5bb6\u6fc0\u6d3b\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff1b2) \u4ec5\u4f7f\u75281/3\u7684GPU\u5185\u5b58\u5c31\u80fd\u5b9e\u73b0\u5b8c\u5168GPU\u7f13\u5b58MoE\u90e8\u7f72\u7ea675%\u7684\u89e3\u7801\u901f\u5ea6\uff1b3) \u80fd\u591f\u5728GPU\u5185\u5b58\u5c0f\u4e8e1GB\u7684\u8fb9\u7f18\u8282\u70b9\u4e0a\u8fd0\u884cMoE\u63a8\u7406\u3002", "conclusion": "OD-MoE\u901a\u8fc7\u6d88\u9664\u4e13\u5bb6\u7f13\u5b58\u9700\u6c42\uff0c\u4f7fMoE\u6a21\u578b\u80fd\u591f\u5728\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u9645\u90e8\u7f72\uff0c\u4e3aLLM\u65f6\u4ee3\u8fb9\u7f18\u7269\u8054\u7f51\u8bbe\u5907\u7684MoE\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.03303", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03303", "abs": "https://arxiv.org/abs/2512.03303", "authors": ["Michael Luby", "Sandy Irani"], "title": "Local Dominance in Mixed-Strength Populations -- Fast Maximal Independent Set", "comment": "11 pages", "summary": "In many natural and engineered systems, agents interact through local contests that determine which individuals become dominant within their neighborhoods. These interactions are shaped by inherent differences in strength, and they often lead to stable dominance patterns that emerge surprisingly quickly relative to the size of the population. This motivates the search for simple mathematical models that capture both heterogeneous agent strength and rapid convergence to stable local dominance.\n  A widely studied abstraction of local dominance is the Maximal Independent Set (MIS) problem. In the Luby MIS protocol that provably converges quickly to an MIS, each agent repeatedly generates a strength value chosen uniformly and becomes locally dominant if its value is smaller than those of its neighbors. This provides a theoretical explanation for fast dominance convergence in populations of equal-strength agents and naturally raises the question of whether fast convergence also holds in the more realistic setting where agents are inherently mixed-strength.\n  To investigate this question, we introduce the mixed-strength agents model, in which each agent draws its strength from its own distribution. We prove that the extension of the Luby MIS protocol where each agent repeatedly generates a strength value from its own distribution still exhibits fast dominance convergence, providing formal confirmation of the rapid convergence observed in many mixed-strength natural processes.\n  We also show that heterogeneity can significantly change the dynamics of the process. In contrast to the equal-strength setting, a constant fraction of edges need not be eliminated per round. We construct a population and strength profile in which progress per round is asymptotically smaller, illustrating how inherent strength asymmetry produces qualitatively different global behavior.", "AI": {"tldr": "\u7814\u7a76\u6df7\u5408\u5f3a\u5ea6\u667a\u80fd\u4f53\u5728\u5c40\u90e8\u652f\u914d\u7ade\u4e89\u4e2d\u7684\u5feb\u901f\u6536\u655b\u95ee\u9898\uff0c\u6269\u5c55Luby MIS\u534f\u8bae\u5230\u975e\u5747\u5300\u5f3a\u5ea6\u5206\u5e03\uff0c\u8bc1\u660e\u6df7\u5408\u5f3a\u5ea6\u4e0b\u4ecd\u80fd\u5feb\u901f\u6536\u655b\uff0c\u4f46\u52a8\u6001\u7279\u6027\u4e0e\u5747\u5300\u5f3a\u5ea6\u60c5\u51b5\u6709\u672c\u8d28\u5dee\u5f02\u3002", "motivation": "\u8bb8\u591a\u81ea\u7136\u548c\u5de5\u7a0b\u7cfb\u7edf\u4e2d\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u5c40\u90e8\u7ade\u4e89\u5f62\u6210\u652f\u914d\u5173\u7cfb\uff0c\u8fd9\u4e9b\u7ade\u4e89\u53d7\u5185\u5728\u5f3a\u5ea6\u5dee\u5f02\u5f71\u54cd\uff0c\u5e38\u80fd\u5feb\u901f\u6536\u655b\u5230\u7a33\u5b9a\u652f\u914d\u6a21\u5f0f\u3002\u73b0\u6709Luby MIS\u534f\u8bae\u4e3a\u5747\u5300\u5f3a\u5ea6\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5feb\u901f\u6536\u655b\u7684\u7406\u8bba\u89e3\u91ca\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u667a\u80fd\u4f53\u901a\u5e38\u5177\u6709\u6df7\u5408\u5f3a\u5ea6\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u66f4\u771f\u5b9e\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u7279\u6027\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u5f3a\u5ea6\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4ece\u81ea\u8eab\u5206\u5e03\u4e2d\u62bd\u53d6\u5f3a\u5ea6\u503c\u3002\u6269\u5c55Luby MIS\u534f\u8bae\uff0c\u4f7f\u6bcf\u4e2a\u667a\u80fd\u4f53\u91cd\u590d\u4ece\u81ea\u8eab\u5206\u5e03\u751f\u6210\u5f3a\u5ea6\u503c\uff0c\u800c\u975e\u7edf\u4e00\u5747\u5300\u5206\u5e03\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8be5\u6269\u5c55\u534f\u8bae\u4ecd\u80fd\u5feb\u901f\u6536\u655b\uff0c\u5e76\u6784\u9020\u7279\u5b9a\u79cd\u7fa4\u548c\u5f3a\u5ea6\u5206\u5e03\u5c55\u793a\u5f02\u8d28\u6027\u5982\u4f55\u6539\u53d8\u52a8\u6001\u7279\u6027\u3002", "result": "\u8bc1\u660e\u6df7\u5408\u5f3a\u5ea6\u667a\u80fd\u4f53\u6a21\u578b\u4e0b\uff0c\u6269\u5c55\u7684Luby MIS\u534f\u8bae\u4ecd\u80fd\u5feb\u901f\u6536\u655b\u5230\u7a33\u5b9a\u652f\u914d\u6a21\u5f0f\uff0c\u4e3a\u89c2\u5bdf\u5230\u7684\u81ea\u7136\u8fc7\u7a0b\u5feb\u901f\u6536\u655b\u63d0\u4f9b\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002\u540c\u65f6\u53d1\u73b0\u5f02\u8d28\u6027\u663e\u8457\u6539\u53d8\u8fc7\u7a0b\u52a8\u6001\uff1a\u4e0e\u5747\u5300\u5f3a\u5ea6\u60c5\u51b5\u4e0d\u540c\uff0c\u6bcf\u8f6e\u4e0d\u4e00\u5b9a\u6d88\u9664\u6052\u5b9a\u6bd4\u4f8b\u7684\u8fb9\uff0c\u6784\u9020\u4e86\u8fdb\u5c55\u901f\u5ea6\u6e10\u8fd1\u66f4\u6162\u7684\u5b9e\u4f8b\uff0c\u5c55\u793a\u5185\u5728\u5f3a\u5ea6\u4e0d\u5bf9\u79f0\u5982\u4f55\u4ea7\u751f\u8d28\u53d8\u5168\u5c40\u884c\u4e3a\u3002", "conclusion": "\u6df7\u5408\u5f3a\u5ea6\u667a\u80fd\u4f53\u5728\u5c40\u90e8\u652f\u914d\u7ade\u4e89\u4e2d\u4ecd\u80fd\u5feb\u901f\u6536\u655b\uff0c\u4f46\u5f02\u8d28\u6027\u4f1a\u663e\u8457\u6539\u53d8\u6536\u655b\u52a8\u6001\u7279\u6027\u3002\u7814\u7a76\u4e3a\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u4e2d\u975e\u5747\u5300\u5f3a\u5ea6\u4e2a\u4f53\u7684\u5feb\u901f\u652f\u914d\u6a21\u5f0f\u5f62\u6210\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f3a\u5ea6\u5206\u5e03\u5f02\u8d28\u6027\u5bf9\u5168\u5c40\u884c\u4e3a\u7684\u5b9a\u6027\u5f71\u54cd\u3002"}}
{"id": "2512.03293", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.03293", "abs": "https://arxiv.org/abs/2512.03293", "authors": ["Filippo Torresan", "Ryota Kanai", "Manuel Baltieri"], "title": "Prior preferences in active inference agents: soft, hard, and goal shaping", "comment": "41 pages, 23 figures", "summary": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e3b\u52a8\u63a8\u7406\u4e2d\u504f\u597d\u5206\u5e03\u7684\u56db\u79cd\u5b9a\u4e49\u65b9\u5f0f\u53ca\u5176\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5728\u7f51\u683c\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u6bd4\u8f83\u53d1\u73b0\uff0c\u76ee\u6807\u5851\u9020\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u4f1a\u727a\u7272\u73af\u5883\u52a8\u6001\u5b66\u4e60\u3002", "motivation": "\u4e3b\u52a8\u63a8\u7406\u4f7f\u7528\u671f\u671b\u81ea\u7531\u80fd\u4f5c\u4e3a\u89c4\u5212\u51b3\u7b56\u76ee\u6807\uff0c\u4f46\u6587\u732e\u4e2d\u5f88\u5c11\u5173\u6ce8\u504f\u597d\u5206\u5e03\u5e94\u5982\u4f55\u6307\u5b9a\u4ee5\u53ca\u4e0d\u540c\u6307\u5b9a\u65b9\u5f0f\u5982\u4f55\u5f71\u54cd\u4e3b\u52a8\u63a8\u7406\u667a\u80fd\u4f53\u7684\u63a8\u7406\u548c\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u8003\u8651\u4e86\u56db\u79cd\u5b9a\u4e49\u504f\u597d\u5206\u5e03\u7684\u65b9\u5f0f\uff1a\u786c\u76ee\u6807\u4e0e\u8f6f\u76ee\u6807\u3001\u6d89\u53ca\u6216\u4e0d\u6d89\u53ca\u76ee\u6807\u5851\u9020\uff08\u4e2d\u95f4\u76ee\u6807\uff09\uff0c\u5728\u7f51\u683c\u4e16\u754c\u5bfc\u822a\u4efb\u52a1\u4e2d\u6bd4\u8f83\u4e86\u56db\u79cd\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "result": "\u76ee\u6807\u5851\u9020\u80fd\u5b9e\u73b0\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff08\u4fc3\u8fdb\u5229\u7528\uff09\uff0c\u4f46\u4f1a\u727a\u7272\u5bf9\u73af\u5883\u8f6c\u79fb\u52a8\u6001\u7684\u5b66\u4e60\uff08\u963b\u788d\u63a2\u7d22\uff09\u3002", "conclusion": "\u504f\u597d\u5206\u5e03\u7684\u5b9a\u4e49\u65b9\u5f0f\u5bf9\u4e3b\u52a8\u63a8\u7406\u667a\u80fd\u4f53\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u76ee\u6807\u5851\u9020\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u4f1a\u9650\u5236\u63a2\u7d22\u80fd\u529b\uff0c\u9700\u8981\u5728\u5229\u7528\u548c\u63a2\u7d22\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002"}}
{"id": "2512.03318", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03318", "abs": "https://arxiv.org/abs/2512.03318", "authors": ["Chandler Smith", "Marwa Abdulhai", "Manfred Diaz", "Marko Tesic", "Rakshit S. Trivedi", "Alexander Sasha Vezhnevets", "Lewis Hammond", "Jesse Clifton", "Minsuk Chang", "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n", "John P. Agapiou", "Jayd Matyas", "Danny Karmon", "Akash Kundu", "Aliaksei Korshuk", "Ananya Ananya", "Arrasy Rahman", "Avinaash Anand Kulandaivel", "Bain McHale", "Beining Zhang", "Buyantuev Alexander", "Carlos Saith Rodriguez Rojas", "Caroline Wang", "Chetan Talele", "Chenao Liu", "Chichen Lin", "Diana Riazi", "Di Yang Shi", "Emanuel Tewolde", "Elizaveta Tennant", "Fangwei Zhong", "Fuyang Cui", "Gang Zhao", "Gema Parre\u00f1o Piqueras", "Hyeonggeun Yun", "Ilya Makarov", "Jiaxun Cui", "Jebish Purbey", "Jim Dilkes", "Jord Nguyen", "Lingyun Xiao", "Luis Felipe Giraldo", "Manuela Chacon-Chamorro", "Manuel Sebastian Rios Beltran", "Marta Emili Garc\u00eda Segura", "Mengmeng Wang", "Mogtaba Alim", "Nicanor Quijano", "Nico Schiavone", "Olivia Macmillan-Scott", "Oswaldo Pe\u00f1a", "Peter Stone", "Ram Mohan Rao Kadiyala", "Rolando Fernandez", "Ruben Manrique", "Sunjia Lu", "Sheila A. McIlraith", "Shamika Dhuri", "Shuqing Shi", "Siddhant Gupta", "Sneheel Sarangi", "Sriram Ganapathi Subramanian", "Taehun Cha", "Toryn Q. Klassen", "Wenming Tu", "Weijian Fan", "Wu Ruiyang", "Xue Feng", "Yali Du", "Yang Liu", "Yiding Wang", "Yipeng Kang", "Yoonchang Sung", "Yuxuan Chen", "Zhaowei Zhang", "Zhihan Wang", "Zhiqiang Wu", "Ziang Chen", "Zilong Zheng", "Zixia Jia", "Ziyan Wang", "Dylan Hadfield-Menell", "Natasha Jaques", "Tim Baarslag", "Jose Hernandez-Orallo", "Joel Z. Leibo"], "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia", "comment": "Published at NeurIPS Datasets and Benchmarks 2025, 10 pages", "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u5408\u4f5c\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528Concordia\u81ea\u7136\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u6a21\u62df\u73af\u5883\uff0c\u53d1\u73b0\u5f53\u524d\u667a\u80fd\u4f53\u5728\u9700\u8981\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7684\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u793e\u4ea4\u4e92\u52a8\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u5e76\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u4e0e\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u4f53\u4e92\u52a8\u7684\u573a\u666f\u4e2d\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u8861\u91cf\u8fd9\u4e9b\u80fd\u529b\u5728\u65b0\u9896\u793e\u4ea4\u60c5\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4ee3\u8868\u4e86LLM\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u524d\u6cbf\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u6df7\u5408\u52a8\u673a\u73af\u5883\u4e2d\u5408\u4f5c\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528Concordia\u81ea\u7136\u8bed\u8a00\u591a\u667a\u80fd\u4f53\u6a21\u62df\u73af\u5883\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u5408\u4f5c\u4f19\u4f34\u548c\u60c5\u5883\u4e2d\u8bc6\u522b\u548c\u5229\u7528\u4e92\u5229\u673a\u4f1a\u7684\u80fd\u529b\u6765\u8861\u91cf\u4e00\u822c\u5408\u4f5c\u667a\u80fd\u3002", "result": "\u57fa\u4e8eNeurIPS 2024 Concordia\u7ade\u8d5b\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u667a\u80fd\u4f53\u80fd\u529b\u4e0e\u5b9e\u73b0\u53ef\u9760\u5408\u4f5c\u6240\u9700\u7684\u7a33\u5065\u6cdb\u5316\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bf4\u670d\u548c\u89c4\u8303\u6267\u884c\u7684\u573a\u666f\u4e2d\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u5728\u5408\u4f5c\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u91cd\u8981\u6539\u8fdb\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u793e\u4ea4\u60c5\u5883\u65f6\u3002\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u63d0\u9ad8\u667a\u80fd\u4f53\u5728\u591a\u6837\u5316\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u80fd\u529b\u3002"}}
{"id": "2512.03694", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03694", "abs": "https://arxiv.org/abs/2512.03694", "authors": ["Shuang Guo", "Zihui Li"], "title": "SRPG: Semantically Reconstructed Privacy Guard for Zero-Trust Privacy in Educational Multi-Agent Systems", "comment": null, "summary": "Multi-Agent Systems (MAS) with large language models (LLMs) enable personalized education but risk leaking minors personally identifiable information (PII) via unstructured dialogue. Existing privacy methods struggle to balance security and utility: role-based access control fails on unstructured text, while naive masking destroys pedagogical context. We propose SRPG, a privacy guard for educational MAS, using a Dual-Stream Reconstruction Mechanism: a strict sanitization stream ensures zero PII leakage, and a context reconstruction stream (LLM driven) recovers mathematical logic. This decouples instructional content from private data, preserving teaching efficacy. Tests on MathDial show SRPG works across models; with GPT-4o, it achieves 0.0000 Attack Success Rate (ASR) (zero leakage) and 0.8267 Exact Match, far outperforming the zero trust Pure LLM baseline (0.2138). SRPG effectively protects minors privacy without sacrificing mathematical instructional quality.", "AI": {"tldr": "SRPG\u662f\u4e00\u79cd\u7528\u4e8e\u6559\u80b2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u6d41\u91cd\u5efa\u673a\u5236\u5728\u4fdd\u62a4\u672a\u6210\u5e74\u4eba\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u7684\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u4e2a\u6027\u5316\u6559\u80b2\uff0c\u4f46\u5b58\u5728\u901a\u8fc7\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u6cc4\u9732\u672a\u6210\u5e74\u4eba\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u7684\u98ce\u9669\u3002\u73b0\u6709\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff1a\u57fa\u4e8e\u89d2\u8272\u7684\u8bbf\u95ee\u63a7\u5236\u65e0\u6cd5\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u800c\u7b80\u5355\u7684\u63a9\u7801\u4f1a\u7834\u574f\u6559\u5b66\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faSRPG\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u91c7\u7528\u53cc\u6d41\u91cd\u5efa\u673a\u5236\uff1a\u4e25\u683c\u7684\u51c0\u5316\u6d41\u786e\u4fdd\u96f6PII\u6cc4\u9732\uff0c\u4e0a\u4e0b\u6587\u91cd\u5efa\u6d41\uff08\u7531LLM\u9a71\u52a8\uff09\u6062\u590d\u6570\u5b66\u903b\u8f91\u3002\u8fd9\u79cd\u65b9\u6cd5\u5c06\u6559\u5b66\u5185\u5bb9\u4e0e\u79c1\u4eba\u6570\u636e\u89e3\u8026\uff0c\u540c\u65f6\u4fdd\u6301\u6559\u5b66\u6548\u679c\u3002", "result": "\u5728MathDial\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0cSRPG\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u5747\u6709\u6548\uff1b\u4f7f\u7528GPT-4o\u65f6\uff0c\u5b9e\u73b0\u4e860.0000\u653b\u51fb\u6210\u529f\u7387\uff08\u96f6\u6cc4\u9732\uff09\u548c0.8267\u7cbe\u786e\u5339\u914d\u7387\uff0c\u8fdc\u4f18\u4e8e\u96f6\u4fe1\u4efb\u7eafLLM\u57fa\u7ebf\u76840.2138\u3002", "conclusion": "SRPG\u80fd\u591f\u6709\u6548\u4fdd\u62a4\u672a\u6210\u5e74\u4eba\u9690\u79c1\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6570\u5b66\u6559\u5b66\u8d28\u91cf\uff0c\u4e3a\u6559\u80b2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03528", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.03528", "abs": "https://arxiv.org/abs/2512.03528", "authors": ["Guang Yang", "Tianpei Yang", "Jingwen Qiao", "Yanqing Wu", "Jing Huo", "Xingguo Chen", "Yang Gao"], "title": "Multi-Agent Reinforcement Learning with Communication-Constrained Priors", "comment": null, "summary": "Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u4fe1\u53d7\u9650\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u6709\u635f\u548c\u65e0\u635f\u6d88\u606f\uff0c\u5229\u7528\u53cc\u91cd\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668\u91cf\u5316\u901a\u4fe1\u5f71\u54cd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u901a\u4fe1\u901a\u5e38\u5b58\u5728\u635f\u8017\u95ee\u9898\uff0c\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u901a\u4fe1\u65b9\u6cd5\u7531\u4e8e\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u6709\u9650\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u590d\u6742\u52a8\u6001\u7684\u771f\u5b9e\u73af\u5883\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u901a\u4fe1\u7ea6\u675f\u6a21\u578b\u7edf\u4e00\u63cf\u8ff0\u4e0d\u540c\u573a\u666f\u7684\u901a\u4fe1\u6761\u4ef6\uff0c\u4f5c\u4e3a\u5b66\u4e60\u5148\u9a8c\u533a\u5206\u6709\u635f\u548c\u65e0\u635f\u6d88\u606f\uff1b\u4f7f\u7528\u53cc\u91cd\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668\u89e3\u8026\u6709\u635f\u548c\u65e0\u635f\u6d88\u606f\u5bf9\u5206\u5e03\u5f0f\u51b3\u7b56\u7684\u5f71\u54cd\uff1b\u5f15\u5165\u901a\u4fe1\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u901a\u4fe1\u6d88\u606f\u5f71\u54cd\u91cf\u5316\u5230\u5168\u5c40\u5956\u52b1\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u901a\u4fe1\u53d7\u9650\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u901a\u4fe1\u7ea6\u675f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u901a\u4fe1\u635f\u8017\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.03571", "categories": ["cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03571", "abs": "https://arxiv.org/abs/2512.03571", "authors": ["Zhening Li", "Armando Solar-Lezama", "Yisong Yue", "Stephan Zheng"], "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "comment": "65 pages, 2 figures, published in NeurIPS 2025", "summary": "We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u667a\u80fd\u4f53\u7f16\u7a0b\u65b9\u6cd5PAN\uff0c\u901a\u8fc7\u5206\u79bb\u6838\u5fc3\u5de5\u4f5c\u6d41\u903b\u8f91\u548c\u63a8\u7406\u65f6\u7b56\u7565\uff0c\u4f7f\u7528Python\u6846\u67b6EnCompass\u5b9e\u73b0\uff0c\u8ba9\u7a0b\u5e8f\u5458\u80fd\u5feb\u901f\u6539\u8fdb\u667a\u80fd\u4f53\u53ef\u9760\u6027\u5e76\u8f7b\u677e\u5207\u6362\u4e0d\u540c\u63a8\u7406\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7f16\u7a0b\u65b9\u6cd5\u901a\u5e38\u5c06\u6838\u5fc3\u5de5\u4f5c\u6d41\u903b\u8f91\u548c\u63a8\u7406\u65f6\u7b56\u7565\uff08\u5982\u6811\u641c\u7d22\uff09\u8026\u5408\u5728\u4e00\u8d77\uff0c\u8fd9\u79cd\u8026\u5408\u9650\u5236\u4e86\u7a0b\u5e8f\u5458\u7684\u7075\u6d3b\u6027\u548c\u5b9e\u9a8c\u80fd\u529b\uff0c\u96be\u4ee5\u72ec\u7acb\u8c03\u6574\u4e0d\u540c\u7ec4\u4ef6\u3002", "method": "\u63d0\u51fa\"\u6982\u7387\u5929\u4f7f\u975e\u786e\u5b9a\u6027\"\uff08PAN\uff09\u7f16\u7a0b\u6a21\u578b\uff0c\u5206\u79bb\u5de5\u4f5c\u6d41\u548c\u63a8\u7406\u7b56\u7565\uff1b\u5b9e\u73b0EnCompass\u6846\u67b6\uff0c\u4f7f\u7528Python\u88c5\u9970\u5668\u5c06\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u5e8f\u7f16\u8bd1\u4e3a\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u8ba9\u7a0b\u5e8f\u5458\u80fd\u7528\u5c11\u91cf\u989d\u5916\u4ee3\u7801\u5feb\u901f\u6539\u8fdb\u667a\u80fd\u4f53\u53ef\u9760\u6027\uff0c\u5e76\u8f7b\u677e\u5728\u4e0d\u540c\u63a8\u7406\u65f6\u7b56\u7565\u4e4b\u95f4\u5207\u6362\u3002", "conclusion": "PAN\u7f16\u7a0b\u6a21\u578b\u548cEnCompass\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u8026\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7f16\u7a0b\u7075\u6d3b\u6027\u548c\u5b9e\u9a8c\u6548\u7387\uff0c\u4e3aLLM-based\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.03607", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03607", "abs": "https://arxiv.org/abs/2512.03607", "authors": ["Yusen Wu", "Xiaotie Deng"], "title": "DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization", "comment": null, "summary": "This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.\n  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.", "AI": {"tldr": "DeepRule\u662f\u4e00\u4e2a\u7528\u4e8e\u96f6\u552e\u54c1\u7c7b\u548c\u5b9a\u4ef7\u4f18\u5316\u7684\u81ea\u52a8\u5316\u4e1a\u52a1\u89c4\u5219\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7LLM\u89e3\u6790\u975e\u7ed3\u6784\u5316\u6587\u672c\u3001\u535a\u5f08\u8bba\u7ea6\u675f\u4f18\u5316\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u84b8\u998f\uff0c\u89e3\u51b3\u7406\u8bba\u6a21\u578b\u4e0e\u73b0\u5b9e\u7ecf\u6d4e\u590d\u6742\u6027\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u9519\u4f4d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u6a21\u578b\u4e0e\u73b0\u5b9e\u7ecf\u6d4e\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u4f4d\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u4e09\u4e2a\u5173\u952e\u5dee\u8ddd\uff1a1) \u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u6a21\u6001\u4e0d\u5339\u914d\u963b\u788d\u51c6\u786e\u5ba2\u6237\u753b\u50cf\uff1b2) \u52a8\u6001\u7279\u5f81\u7ea0\u7f20\u6311\u6218\u975e\u7ebf\u6027\u4ef7\u683c\u5f39\u6027\u548c\u65f6\u53d8\u5c5e\u6027\u5efa\u6a21\uff1b3) \u591a\u5c42\u6b21\u4e1a\u52a1\u7ea6\u675f\u5bfc\u81f4\u64cd\u4f5c\u4e0d\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a1) \u6df7\u5408\u77e5\u8bc6\u878d\u5408\u5f15\u64ce\u4f7f\u7528LLM\u6df1\u5ea6\u8bed\u4e49\u89e3\u6790\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u5c06\u5206\u9500\u534f\u8bae\u548c\u9500\u552e\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7279\u5f81\uff1b2) \u535a\u5f08\u8bba\u7ea6\u675f\u4f18\u5316\u673a\u5236\u901a\u8fc7\u53cc\u8fb9\u6548\u7528\u51fd\u6570\u52a8\u6001\u534f\u8c03\u4f9b\u5e94\u94fe\u5229\u76ca\uff0c\u5728\u5c42\u6b21\u7ea6\u675f\u4e0b\u7f16\u7801\u5236\u9020\u5546-\u5206\u9500\u5546\u5229\u6da6\u518d\u5206\u914d\uff1b3) \u53ef\u89e3\u91ca\u51b3\u7b56\u84b8\u998f\u63a5\u53e3\u5229\u7528LLM\u5f15\u5bfc\u7684\u7b26\u53f7\u56de\u5f52\u4f18\u5316\u5b9a\u4ef7\u7b56\u7565\u548c\u53ef\u5ba1\u8ba1\u4e1a\u52a1\u89c4\u5219\uff0c\u5c06\u7ecf\u6d4e\u5148\u9a8c\u4f5c\u4e3a\u786c\u7ea6\u675f\u5d4c\u5165\u6570\u5b66\u8868\u8fbe\u5f0f\u641c\u7d22\u3002", "result": "\u5728\u771f\u5b9e\u96f6\u552e\u73af\u5883\u4e2d\u9a8c\u8bc1\u6846\u67b6\uff0c\u76f8\u6bd4\u7cfb\u7edf\u6027B2C\u57fa\u7ebf\u5b9e\u73b0\u66f4\u9ad8\u5229\u6da6\uff0c\u540c\u65f6\u786e\u4fdd\u64cd\u4f5c\u53ef\u884c\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u95ed\u73af\u7ba1\u9053\uff0c\u7edf\u4e00\u4e86\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u6ce8\u5165\u3001\u591a\u667a\u80fd\u4f53\u4f18\u5316\u548c\u53ef\u89e3\u91ca\u7b56\u7565\u5408\u6210\uff0c\u4e3a\u771f\u5b9e\u7ecf\u6d4e\u667a\u80fd\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03762", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03762", "abs": "https://arxiv.org/abs/2512.03762", "authors": ["Jiawei Xu", "Fengfeng Wei", "Weineng Chen"], "title": "RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design", "comment": null, "summary": "Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.", "AI": {"tldr": "RoCo\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u89d2\u8272\u534f\u4f5c\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u95e8\u5316\u7684LLM\u667a\u80fd\u4f53\uff08\u63a2\u7d22\u8005\u3001\u5229\u7528\u8005\u3001\u6279\u8bc4\u8005\u3001\u6574\u5408\u8005\uff09\u534f\u540c\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7814\u7a76\u901a\u5e38\u53ea\u8003\u8651\u5355\u4e00\u89d2\u8272\uff0c\u9650\u5236\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u591a\u89d2\u8272\u534f\u4f5c\u6765\u589e\u5f3a\u542f\u53d1\u5f0f\u8bbe\u8ba1\u591a\u6837\u6027\u548c\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRoCo\u591a\u667a\u80fd\u4f53\u89d2\u8272\u534f\u4f5c\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u4e13\u95e8\u5316\u7684LLM\u667a\u80fd\u4f53\uff1a\u63a2\u7d22\u8005\uff08\u521b\u9020\u6027\u3001\u591a\u6837\u6027\u9a71\u52a8\uff09\u3001\u5229\u7528\u8005\uff08\u4fdd\u5b88\u6027\u3001\u6548\u7387\u5bfc\u5411\uff09\u3001\u6279\u8bc4\u8005\uff08\u8bc4\u4f30\u8fdb\u5316\u6b65\u9aa4\u5e76\u63d0\u4f9b\u53cd\u9988\uff09\u3001\u6574\u5408\u8005\uff08\u5e73\u8861\u521b\u65b0\u4e0e\u5229\u7528\uff09\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u8f6e\u8fc7\u7a0b\u8fdb\u884c\u4ea4\u4e92\uff0c\u5305\u542b\u53cd\u9988\u3001\u7cbe\u70bc\u548c\u7cbe\u82f1\u53d8\u5f02\uff0c\u540c\u65f6\u8003\u8651\u77ed\u671f\u548c\u957f\u671f\u53cd\u601d\u3002", "result": "\u5728\u4e94\u79cd\u4e0d\u540c\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u767d\u76d2\u548c\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0cRoCo\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u751f\u6210\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ecReEvo\u548cHSEvo\uff09\u3002", "conclusion": "\u57fa\u4e8e\u89d2\u8272\u7684\u534f\u4f5c\u8303\u5f0f\u4e3a\u9c81\u68d2\u4e14\u9ad8\u6027\u80fd\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u901a\u8fc7\u591a\u89d2\u8272\u534f\u540c\u5de5\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2512.03783", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.03783", "abs": "https://arxiv.org/abs/2512.03783", "authors": ["Dongchao Yang", "Songxiang Liu", "Disong Wang", "Yuanyuan Wang", "Guanglu Wan", "Helen Meng"], "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "comment": null, "summary": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "AI": {"tldr": "Omni-AutoThink\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u63a8\u7406\u6df1\u5ea6\u6765\u89e3\u51b3\u73b0\u6709Omni\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u50f5\u5316\u7684\u95ee\u9898\uff0c\u5305\u542b\u81ea\u9002\u5e94\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709Omni\u6a21\u578b\u5728\u63a8\u7406\u884c\u4e3a\u4e0a\u5b58\u5728\u50f5\u5316\u95ee\u9898\uff1a\u5bf9\u4e8e\u7b80\u5355\u95ee\u9898\u8fc7\u5ea6\u63a8\u7406\uff0c\u800c\u5728\u9700\u8981\u63a8\u7406\u65f6\u53c8\u65e0\u6cd5\u6709\u6548\u63a8\u7406\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u81ea\u9002\u5e94\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u63a8\u7406\u589e\u5f3a\u6570\u636e\u8d4b\u4e88Omni\u6a21\u578b\u57fa\u672c\u63a8\u7406\u80fd\u529b\uff1b2) \u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u5956\u52b1\u53cd\u9988\u4f18\u5316\u63a8\u7406\u884c\u4e3a\u3002\u540c\u65f6\u6784\u5efa\u4e86\u6db5\u76d6\u6587\u672c\u3001\u6587\u672c-\u97f3\u9891\u3001\u6587\u672c-\u89c6\u89c9\u3001\u6587\u672c-\u97f3\u9891-\u89c6\u89c9\u591a\u6a21\u6001\u7684\u81ea\u9002\u5e94\u63a8\u7406\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u5148\u524d\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u81ea\u9002\u5e94\u63a8\u7406\u6027\u80fd\u3002\u6240\u6709\u57fa\u51c6\u6570\u636e\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "Omni-AutoThink\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u6709\u6548\u89e3\u51b3\u4e86Omni\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u50f5\u5316\u95ee\u9898\uff0c\u5728\u591a\u6a21\u6001\u81ea\u9002\u5e94\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2512.03955", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03955", "abs": "https://arxiv.org/abs/2512.03955", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "comment": "This work has been submitted to IFAC for possible publication", "summary": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4e1a\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u5f00\u53d1\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u53ef\u6267\u884c\u6a21\u62df\u73af\u5883\u548c\u4e94\u4e2a\u590d\u6742\u5ea6\u7c7b\u522b\u7684Blocksworld\u95ee\u9898\uff0c\u901a\u8fc7MCP\u534f\u8bae\u5b9e\u73b0\u4e0d\u540c\u67b6\u6784\u7684\u65e0\u7f1d\u96c6\u6210\u8bc4\u4f30\u3002", "motivation": "\u5de5\u4e1a\u81ea\u52a8\u5316\u9700\u8981\u80fd\u591f\u9002\u5e94\u53d8\u5316\u4efb\u52a1\u548c\u73af\u5883\u7684\u7075\u6d3b\u63a7\u5236\u7b56\u7565\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5177\u6709\u81ea\u9002\u5e94\u89c4\u5212\u548c\u6267\u884c\u7684\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u7684\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u5f15\u5165\u5305\u542b\u53ef\u6267\u884c\u6a21\u62df\u73af\u5883\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91c7\u7528Blocksworld\u95ee\u9898\u5e76\u63d0\u4f9b\u4e94\u4e2a\u590d\u6742\u5ea6\u7c7b\u522b\uff0c\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u4f5c\u4e3a\u6807\u51c6\u5316\u5de5\u5177\u63a5\u53e3\uff0c\u4f7f\u4e0d\u540c\u667a\u80fd\u4f53\u67b6\u6784\u65e0\u9700\u7279\u5b9a\u4fee\u6539\u5373\u53ef\u8fde\u63a5\u548c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5355\u667a\u80fd\u4f53\u5b9e\u73b0\u5c55\u793a\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u9002\u7528\u6027\uff0c\u5efa\u7acb\u4e86\u7528\u4e8e\u6bd4\u8f83\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u548c\u6267\u884c\u65b9\u6cd5\u7684\u5b9a\u91cf\u6307\u6807\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u4e3a\u8bc4\u4f30\u548c\u6bd4\u8f83\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u4e1a\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u81ea\u9002\u5e94\u89c4\u5212\u548c\u6267\u884c\u65b9\u6cd5\u7684\u7814\u7a76\u4e0e\u53d1\u5c55\u3002"}}
