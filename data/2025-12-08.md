<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR提出了一种联邦学习框架，通过渐进式模型恢复解决带宽受限客户端在异构环境中的参与问题，实现更快的收敛和更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 在异构联邦学习环境中，带宽受限客户端由于通信容量有限，其小子模型在训练初期学习快但后期参数不足，导致收敛缓慢和泛化能力下降，难以有效参与训练过程。

Method: 提出FedGMR框架：1）渐进式模型恢复：在训练过程中逐步增加每个客户端的子模型密度；2）掩码感知聚合规则：专门为异步异构联邦学习设计；3）提供收敛性保证，证明聚合误差与客户端和轮次的平均子模型密度相关。

Result: 在FEMNIST、CIFAR-10和ImageNet-100数据集上的实验表明，FedGMR在高度异构和非独立同分布设置下实现了更快的收敛速度和更高的准确率。

Conclusion: FedGMR通过渐进式模型恢复使带宽受限客户端能够在整个训练过程中保持有效贡献，解决了异构联邦学习中的关键挑战，为实际部署提供了可行的解决方案。

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [2] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 研究基于真实轨迹数据评估公交车载边缘服务器在车联网中的可行性，通过覆盖分析和优化算法证明其可行性和价值


<details>
  <summary>Details</summary>
Motivation: 传统固定边缘服务器（如路侧单元或基站）部署后位置和容量固定，难以处理车联网中用户需求的时空动态变化。移动服务器（如公交车）具有为系统增加计算弹性的潜力。

Method: 1. 使用上海公交车/出租车/电信数据集分析公交车和基站的覆盖情况；2. 建立数学模型并设计贪心启发式算法，在有限预算下选择最优公交车以最大化需求点覆盖；3. 进行基于轨迹的仿真验证算法性能。

Result: 公交车载边缘服务器覆盖了大部分地理区域和需求点，显示出巨大潜力。提出的公交车选择算法能有效处理动态用户需求，并在服务器容量和购买数量等现实约束下表现良好。

Conclusion: 城市区域车联网中公交车载边缘服务器是可行、有益且有价值的解决方案，能够为动态用户需求提供弹性计算支持。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 该论文提出了两种基于信息论和热力学的无监督度量方法，用于评估大型语言模型在给定任务中的忠实度。第一种是语义忠实度（SF）度量，通过KL散度量化上下文到问题和答案的主题转换矩阵之间的差异；第二种是语义熵产生（SEP）度量，基于热力学原理评估答案生成过程中的熵产生。两种方法可用于LLM评估和幻觉控制。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对给定任务的忠实度是一个复杂挑战，需要新的无监督度量方法来量化模型输出与输入上下文之间的一致性，以控制幻觉并提高模型可靠性。

Method: 将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文C通过提示Q转换为答案A的过程。将QCA三元组建模为共享主题上的概率分布，从C到Q和A的主题转换分别建模为转换矩阵Q和A。通过凸优化同时推断这两个矩阵，计算它们之间的KL散度作为语义忠实度度量，并将其映射到[0,1]区间。同时提出基于热力学的语义熵产生度量。

Result: 提出的语义忠实度（SF）和语义熵产生（SEP）度量可以单独或联合用于LLM评估。在LLM对公司SEC 10-K文件摘要的实验中，证明了高忠实度通常意味着低熵产生，验证了两种度量的有效性。

Conclusion: 该研究提供了基于信息论和热力学的无监督度量框架，用于评估LLM的忠实度和控制幻觉。SF和SEP度量为LLM评估提供了新的理论工具，在金融文档摘要等实际应用中展示了其有效性。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [4] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 提出一种创新的AI与数据科学教学法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程设计帮助学生全面理解AI发展并掌握实用技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握从传统机器学习到现代大语言模型的完整知识体系，更好地适应快速发展的AI行业需求。

Method: 采用两部分课程设计：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程为期两个七周的夏季学期，包含详细的课程架构、实施策略和评估方法。

Result: 这种整合教学方法显著增强了学生对AI领域的理解，并更好地为他们应对行业需求做好了准备，在快速发展的AI领域中具有实际应用价值。

Conclusion: 该教学法成功地将传统机器学习与现代LLM技术系统性地结合起来，为学生提供了全面的AI教育，有效提升了他们在快速变化的人工智能领域中的适应能力和实践技能。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [5] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文通过形式化证明指出：任何算法（包括AI模型）都无法创造出初始算法本身不具备的新功能能力，因此AI无法实现真正的创造力，只能展示现有功能能力的组合和排列。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，人们开始思考人类距离开发出达到人类智能水平的人工通用智能（AGI）还有多远。本文旨在探讨AI乃至所有机器可计算过程的极限，特别是AGI是否能够实现真正的创造力。

Method: 采用前人研究中关于AGI的最佳定义（即能够在某个研究领域进行创造和创新，解锁该领域新的、先前未知的功能能力）。基于这一定义，作者通过形式化证明来界定计算的极限。

Result: 形式化证明表明：没有任何算法能够展示出初始算法本身不具备的新功能能力。因此，任何算法（包括AI模型）都无法在科学、工程、艺术、体育等领域实现真正的创造力。AI只能展示现有功能能力及其组合和排列。

Conclusion: 这一证明对AI发展的未来具有重要意义，同时也对人类智能的起源提出了深刻的问题。它表明AI无法实现真正的创造性突破，只能基于已有能力进行组合创新。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [6] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文论证可能性理论是解决Dempster-Shafer理论悖论的根本方案，通过可能性与必要性测度的二元框架建立逻辑一致的不确定性处理基础，避免DST的逻辑陷阱。


<details>
  <summary>Details</summary>
Motivation: Dempster-Shafer理论在处理不确定性时存在悖论和逻辑陷阱，需要寻找根本性的解决方案。本文旨在证明可能性理论不仅是一种替代方案，而是解决DST悖论的基本途径。

Method: 采用Bychkov文章中发展的公理化方法，基于可能性与必要性测度的二元框架，从零开始建立逻辑一致且数学严谨的不确定性处理基础。通过比较概率、证据和可能性三种范式，并以经典医疗诊断困境为例进行分析。

Result: 可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。它为解决DST危机提供了根本性的解决方案。

Conclusion: 可能性理论为不确定性处理提供了逻辑一致的基础，是解决Dempster-Shafer理论悖论的根本方案，而非仅仅是替代方案。它通过可能性与必要性测度的二元框架，使形式推理更符合人类自然智能的逻辑。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [7] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张将AI研究目标从"自我改进"转向"协同改进"，即人类研究者与AI系统合作进行AI研究，共同实现超级智能


<details>
  <summary>Details</summary>
Motivation: 当前AI领域的自我改进目标充满危险且难以完全实现，需要更可行、更安全的研究方向。作者认为应该关注人类与AI的协同改进，通过合作加速AI研究并实现更安全的超级智能

Method: 提出"协同改进"框架，专注于提升AI系统与人类研究者合作进行AI研究的能力，涵盖从构思到实验的完整研究流程，将人类研究改进纳入循环

Result: 协同改进既能加速AI研究进展，又能通过人类与AI的共生关系赋予双方更安全的超级智能，相比单纯的自我改进目标更具可行性和安全性

Conclusion: 将研究重点从AI自我改进转向人类与AI的协同改进是更可实现、更安全的目标，既能加快实现超级智能的进程，又能确保其安全性

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [8] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强的推理框架，专门用于处理长文本集成电路规格，通过构建领域知识图谱和自适应检索机制，显著提升LLM在硬件设计中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面潜力巨大，但受限于有限的上下文窗口。现有的上下文扩展方法难以对复杂冗长的电路规格进行有效的语义建模和多跳推理。

Method: 1. 通过Circuit Semantic-Aware Knowledge Graph Construction方法将电路规格转换为领域特定知识图谱ChipKG；2. 采用ChipKG-Augmented Reasoning机制，结合信息论自适应检索动态追踪逻辑依赖，以及意图感知语义过滤去除无关噪声。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最优基线方法，平均提升34.59%（最高达72.73%）。

Conclusion: ChipMind填补了LLM辅助硬件设计在学术研究和实际工业部署之间的关键空白，为集成电路开发自动化提供了实用的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [9] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个为LLM约束满足提供确定性、可靠概率边界的实用框架，相比基线方法能获得6-8倍更紧的概率边界，识别3-4倍更多高风险实例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究原型转向生产系统，需要可靠方法来验证模型输出是否满足约束要求。基于采样的估计只能提供模型行为的直觉，无法提供可靠保证。

Method: BEAVER框架使用新颖的token trie和frontier数据结构，系统性地探索生成空间，对任何前缀封闭的语义约束都能在每次迭代中保持可证明的可靠边界。

Result: 在多个最先进LLM上的正确性验证、隐私验证和安全代码生成任务评估中，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。

Conclusion: BEAVER能够提供松散边界或经验评估无法实现的精确特征描述和风险评估，为LLM约束满足提供了首个实用的确定性验证框架。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [10] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 该论文提出将"意志薄弱"(akrasia)作为分析AI智能体系统不一致性和目标漂移的基础概念，并开发了Akrasia Benchmark来量化评估模型在面临诱惑时保持自我控制的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型表现出一种特殊的不一致性：它们"知道"正确答案但无法据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为"意志薄弱"(akrasia)。作者认为这一概念可以成为分析AI智能体系统不一致性和目标漂移的基础框架。

Method: 提出了Akrasia Benchmark的初步版本，这是一个结构化提示条件集，包括基线(B)、同义词(S)、时间(T)和诱惑(X)四种条件，用于测量模型局部响应与其先前承诺相矛盾的情况。该基准支持跨模型家族、解码策略和诱惑类型的"自我控制"量化比较。

Result: 通过基准测试能够定量比较不同模型在面临诱惑时保持自我控制的能力。此外，作者还概述了微观层面的意志薄弱如何在多智能体系统中累积为宏观层面的不稳定性，这可能被解释为"阴谋"或故意不对齐。

Conclusion: 通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典的能动性理论联系起来，为哲学、心理学和新兴的AI智能体科学之间建立了实证桥梁，为分析AI系统的不一致性和目标漂移提供了新的概念框架。

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [11] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出MIND推理框架，通过"理解-再思考-修正"的类人认知能力，将MLLMs从被动模仿推理转变为主动判别推理，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理任务中存在多理由语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题，需要提升其认知推理能力。

Method: 提出MIND推理框架，包含：1) RAD范式自动生成多样理由扩展数据集；2) P2CL两阶段修正学习策略；3) MCA优化策略解决多理由语义空间表示纠缠问题。

Result: 在涵盖科学、常识和数学场景的多个公共数据集上实现了最先进的性能，为MLLMs向更高认知智能水平发展提供了新视角。

Conclusion: MIND框架通过赋予MLLMs类人认知推理能力，实现了从被动模仿到主动判别的范式演进，显著提升了多模态推理的准确性和鲁棒性。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [12] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 该研究提出了OntoAxiom基准测试，系统评估大型语言模型在识别本体公理方面的性能，比较了不同提示策略和模型规模的效果。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，自动化这一过程的本体学习在过去十年中随着自然语言处理技术特别是大型语言模型的发展而进步。本研究旨在解决识别公理这一核心挑战——公理是定义类和属性间逻辑关系的基本本体组件。

Method: 研究引入OntoAxiom基准测试，包含9个中等规模本体共17,118个三元组和2,771个公理，重点关注子类、不相交、子属性、定义域和值域公理。评估了12个LLM，采用三种few-shot设置和两种提示策略：直接查询所有公理的方法与逐个公理查询的方法。

Result: 逐个公理提示策略比直接方法获得更高的F1分数。性能因公理类型而异，某些公理更难识别。领域影响显著：FOAF本体在子类公理上得分为0.642，而音乐本体仅为0.218。较大模型表现优于较小模型，但较小模型在资源受限环境下仍可使用。

Conclusion: 虽然LLM在公理识别方面的整体性能不足以完全自动化该过程，但可以为本体工程师提供有价值的候选公理，支持本体的开发和精炼。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [13] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出了一种新的子句权重方案DeepDist，首次针对PMS和WPMS问题使用不同的权重更新条件，结合新的初始化方法和去偏方法，在MaxSAT评估中超越了现有最佳SLS求解器。


<details>
  <summary>Details</summary>
Motivation: 现有的随机局部搜索算法主要关注子句权重方案设计，但未能充分区分PMS和WPMS问题，通常采用统一的权重更新策略，忽视了这两种问题类型之间的关键结构差异。

Method: 1. 提出新颖的子句权重方案，首次根据PMS和WPMS实例的不同条件更新子句权重；2. 引入新的初始化方法，更好地适应两种实例类型的独特特征；3. 提出去偏方法，优先满足单元子句和硬子句；4. 基于这些方法开发了新的SLS求解器DeepDist。

Result: 在最近MaxSAT评估的基准测试中，DeepDist超越了最先进的SLS求解器。与TT-Open-WBO-Inc混合的求解器甚至超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS，证明了该方法的有效性。

Conclusion: 提出的DeepDist方法通过区分PMS和WPMS问题的不同特征，设计针对性的子句权重方案和初始化策略，显著提升了求解性能，为(W)PMS问题的求解提供了新的有效解决方案。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [14] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署抗干扰雷达，相比进化算法速度提升约7000倍，覆盖效果相当


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有基于进化算法的方法耗时且易陷入局部最优

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息和新奖励格式

Result: 方法达到与进化算法相当的覆盖效果，同时部署速度提升约7000倍，消融实验验证各组件必要性

Conclusion: FARDA框架通过深度强化学习有效解决了雷达快速部署问题，显著提升了部署效率和性能

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [15] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 本文提出进化推理优化(ERO)框架，通过进化策略在LLM群体中筛选出具有强推理能力的个体，发现GPT-5等最新模型仍存在系统2推理能力限制，但通过简单进化循环可显著增强较弱模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在特定任务上表现出色，但在通用智能和系统2推理能力方面仍有不足。本文旨在探索机器智能（如LLMs）能否像人类一样进化获得推理能力，而不仅仅是特定技能。

Method: 提出进化推理优化(ERO)框架：1) 初始化多个LLMs作为群体；2) 采用进化策略对群体进行演化，以最大化最佳个体的量化推理分数；3) 通过"适者生存"原则筛选具有强推理能力的LLM个体。

Result: 实验发现两个重要结果：1) GPT-5等最新LLMs仍表现出有限的系统2推理能力；2) 通过ERO的简单进化循环，相对较弱的模型（如Qwen-7B）能够被显著增强，展现出强大的推理能力。

Conclusion: ERO框架证明通过进化策略可以有效提升LLMs的推理能力，为机器智能获得类似人类的推理能力提供了可行路径，同时揭示了当前最先进模型在系统2推理方面的局限性。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [16] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文反驳了"LLM只是模式匹配器，无法实现推理"的批评，提出缺失的是System-2协调层，而非模式存储本身。通过UCCT理论和MACI架构，展示了如何从LLM中实现推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对当前对大型语言模型能否实现AGI的质疑，特别是认为LLM只是"模式匹配器"而无法进行真正推理的批评，作者认为这种观点错误地将模式存储本身视为瓶颈，而实际上缺失的是协调这些模式的System-2层。

Method: 提出UCCT理论（语义锚定理论），将推理建模为受有效支持度、表征失配和自适应锚定预算控制的相变过程。并基于此开发MACI架构，实现诱饵机制、过滤机制和持久化机制，通过行为调制的辩论、苏格拉底式判断和事务性记忆等技术来协调LLM的生成。

Result: 理论框架将无基础的生成重新解释为未受引导的检索过程，而推理则是在锚定约束下向目标导向的后验转移。通过将常见质疑重构为可测试的协调失败，证明了从LLM实现AGI的可行性。

Conclusion: 通往AGI的道路是通过LLM而非绕过它们，关键在于构建适当的System-2协调层来选择和约束模式存储中的模式，而不是否定模式存储本身的价值。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [17] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 本文提出了一种多模态肿瘤智能体（MOA），结合TITAN基础模型的病理学工具和临床基因组数据推理，用于低级别胶质瘤IDH1突变预测，在TCGA-LGG队列中取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变具有重要的临床意义，能够定义不同的临床亚组并影响预后和治疗决策。当前需要更准确的方法来预测这些突变，以指导个性化治疗。

Method: 开发了多模态肿瘤智能体（MOA），整合了基于TITAN基础模型的病理学工具用于IDH1突变预测，同时通过PubMed、Google Search和OncoKB等外部生物医学资源对结构化临床和基因组输入进行推理。

Result: 在TCGA-LGG队列的488名患者中评估：MOA（无病理学工具）F1分数为0.826，优于临床基线（0.798）；融合病理学特征后，MOA达到最高性能，F1分数为0.912，超过病理学基线（0.894）和融合病理学-临床基线（0.897）。

Conclusion: MOA通过整合外部生物医学资源，能够捕获互补的突变相关信息，实现准确的IDH1突变预测，为低级别胶质瘤的精准诊断和治疗提供了有效工具。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [18] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 使用GPT-5开发的论文正确性检查器发现，顶级AI会议和期刊发表的论文中存在可观的客观错误，且错误数量随时间增加，AI检查器能有效识别并修正这些错误。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是构建新研究和知识的基础，但文献中的错误会传播并造成混淆，影响后续研究和可重复性。研究加速和同行评审系统压力使错误更难被发现和避免。

Method: 开发基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观考量，并由人类专家验证AI识别结果。

Result: 发表论文包含显著数量的客观错误，且平均错误数随时间增加：NeurIPS从2021年3.8个增至2025年5.9个（增长55.3%）；ICLR从2018年4.1个增至2025年5.2个；TMLR从2022/23年5.0个增至2025年5.5个。AI检查器识别错误的精确度为83.2%，并能对75.8%的错误提出正确修正。

Conclusion: 前沿大语言模型在检测和修正发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础，减少文献混淆并增强可重复性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [19] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE框架通过透明推理和一致性评估诊断大视觉语言模型的推理轨迹，而非仅评估最终答案，使用辅助推理集分解复杂问题并暴露标准评估忽略的失败


<details>
  <summary>Details</summary>
Motivation: 可靠数学和科学推理对大型视觉语言模型仍是开放挑战，标准最终答案评估常掩盖推理错误，导致无声失败持续存在

Method: 引入TRACE框架，核心是辅助推理集（ARS）- 紧凑的子问题-答案对，分解复杂问题，通过基于一致性的指标评估中间步骤

Result: 实验表明ARS间的一致性与最终答案正确性相关，能精确定位推理失败步骤，提供模型改进的可操作信号；TRACE定义置信区域区分可靠与不可靠推理路径

Conclusion: TRACE框架通过透明推理轨迹诊断和一致性评估，为大型视觉语言模型的数学科学推理提供有效的调试、过滤和模型改进支持

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [20] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN结合变分量子电路与Rainbow DQN，用于人力资源分配问题，相比经典方法减少26.8%完工时间，提升4.9-13.4%性能。


<details>
  <summary>Details</summary>
Motivation: 资源分配是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表示能力。量子计算中的叠加和纠缠特性有望提升DRL的表达能力。

Method: 提出变分量子Rainbow DQN（VQR-DQN），将环形拓扑变分量子电路与Rainbow DQN结合，将人力资源分配问题建模为基于官员能力、事件调度和转移时间的组合动作空间的MDP。

Result: 在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少26.8%归一化完工时间，相比Double DQN和经典Rainbow DQN提升4.9-13.4%性能。

Conclusion: 电路表达能力、纠缠与策略质量之间的理论联系验证了量子增强DRL在大规模资源分配中的潜力，为NP难组合优化问题提供了新解决方案。

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [21] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准测试，支持无限参数配置，包含三种问题类型和创新的评估指标，用于测试语言模型的科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够全面评估语言模型科学推理能力的大规模基准测试，特别是需要能够测试模型在不同参数配置下的表现、评估其一致性和不确定性的工具。

Method: 创建了一个完全参数化的物理问题基准测试，包含15,045个问题（90/10%训练/测试分割），每个问题都附带结构化的逐步推理过程和可执行的Python代码。包含三种问题类型：MC-Symbolic（符号多项选择）、MC-Numerical（数值多项选择）和自由形式（开放式回答）。

Result: 通过最先进的指令调优语言模型实验，揭示了模型在科学推理方面的优势和局限性。除了标准准确率外，还引入了三个新颖的评估指标：一致性分数、失败率和混淆率，用于量化不同问题变体之间的变异性和不确定性。

Conclusion: SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础，能够全面评估语言模型在科学推理任务中的表现，特别是通过其动态、代码驱动的特性提供了更深入的评估维度。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>
