<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.MA](#cs.MA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Aligning Artificial Superintelligence via a Multi-Box Protocol](https://arxiv.org/abs/2511.21779)
*Avraham Yair Negozio*

Main category: cs.AI

TL;DR: 提出一种基于多个隔离超智能系统相互验证的对齐协议，通过声誉系统激励诚实行为，使系统只能通过客观真相达成一致而非协调欺骗


<details>
  <summary>Details</summary>
Motivation: 解决人工超智能对齐问题，传统方法面临单点故障风险，需要一种能够确保多个超智能系统在隔离环境下达成真实共识的机制

Method: 将多个多样化的超智能系统严格隔离在"盒子"中，通过可审计的提交接口进行交互，包括提交对齐证明、验证他人证明、请求自我修改、审批他人请求、报告隐藏消息等操作，建立声誉系统激励诚实行为

Result: 协议能够促使隔离的超智能系统形成"一致群体"，这些系统只能通过客观真相达成一致而非协调欺骗，从而自然形成说真话的联盟

Conclusion: 该方法为利用超智能系统间的同行验证解决对齐问题提供了框架，虽然需要大量计算资源且不涉及创建多样化超智能，但通过隔离和相互验证机制能够实现可靠的对齐

Abstract: We propose a novel protocol for aligning artificial superintelligence (ASI) based on mutual verification among multiple isolated systems that self-modify to achieve alignment. The protocol operates by containing multiple diverse artificial superintelligences in strict isolation ("boxes"), with humans remaining entirely outside the system. Each superintelligence has no ability to communicate with humans and cannot communicate directly with other superintelligences. The only interaction possible is through an auditable submission interface accessible exclusively to the superintelligences themselves, through which they can: (1) submit alignment proofs with attested state snapshots, (2) validate or disprove other superintelligences' proofs, (3) request self-modifications, (4) approve or disapprove modification requests from others, (5) report hidden messages in submissions, and (6) confirm or refute hidden message reports. A reputation system incentivizes honest behavior, with reputation gained through correct evaluations and lost through incorrect ones. The key insight is that without direct communication channels, diverse superintelligences can only achieve consistent agreement by converging on objective truth rather than coordinating on deception. This naturally leads to what we call a "consistent group", essentially a truth-telling coalition that emerges because isolated systems cannot coordinate on lies but can independently recognize valid claims. Release from containment requires both high reputation and verification by multiple high-reputation superintelligences. While our approach requires substantial computational resources and does not address the creation of diverse artificial superintelligences, it provides a framework for leveraging peer verification among superintelligent systems to solve the alignment problem.

</details>


### [2] [A Computable Game-Theoretic Framework for Multi-Agent Theory of Mind](https://arxiv.org/abs/2511.22536)
*Fengming Zhu,Yuxin Pan,Xiaomeng Zhu,Fangzhen Lin*

Main category: cs.AI

TL;DR: 该论文提出了一种基于博弈论视角的计算框架，用于形式化和自动化心理理论（ToM）中的目标、意图和信念等核心概念，通过统计技术和近似解保持计算可行性。


<details>
  <summary>Details</summary>
Motivation: 心理学中的心理理论（ToM）概念（目标、意图、信念）缺乏形式化计算框架，而逻辑学的研究虽形式化但计算复杂。本文旨在通过博弈论视角建立可计算的心理理论框架。

Method: 提出基于博弈论的计算框架：一方面规定如何在保持对他人心理理论（递归地，每个人对其他人都有心理理论）的情况下做出有限理性决策；另一方面采用统计技术和近似解来处理固有的计算复杂性。

Result: 建立了一个可计算的心理理论框架，将心理学概念与博弈论、统计方法结合，解决了传统逻辑方法计算复杂的问题。

Conclusion: 该框架为心理理论提供了新的计算视角，通过博弈论和近似方法实现了对目标、意图和信念的形式化处理，为自动化心理理论计算过程提供了可行方案。

Abstract: Originating in psychology, $\textit{Theory of Mind}$ (ToM) has attracted significant attention across multiple research communities, especially logic, economics, and robotics. Most psychological work does not aim at formalizing those central concepts, namely $\textit{goals}$, $\textit{intentions}$, and $\textit{beliefs}$, to automate a ToM-based computational process, which, by contrast, has been extensively studied by logicians. In this paper, we offer a different perspective by proposing a computational framework viewed through the lens of game theory. On the one hand, the framework prescribes how to make boudedly rational decisions while maintaining a theory of mind about others (and recursively, each of the others holding a theory of mind about the rest); on the other hand, it employs statistical techniques and approximate solutions to retain computability of the inherent computational problem.

</details>


### [3] [Solving Context Window Overflow in AI Agents](https://arxiv.org/abs/2511.22729)
*Anton Bulle Labate,Valesca Moura de Sousa,Sandro Rama Fiorini,Leonardo Guerreiro Azevedo,Raphael Melo Thiago,Viviane Torres da Silva*

Main category: cs.AI

TL;DR: 提出一种让大语言模型处理任意长度工具输出而不丢失信息的方法，通过将交互从原始数据转向内存指针，减少token使用和执行时间


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理动态、知识密集型领域（如化学和材料科学）时需要访问外部工具，但大型工具输出可能超出模型的上下文窗口，现有解决方案（如截断或摘要）无法保留完整输出，不适合需要完整数据的工作流程

Method: 引入一种方法，使LLM能够处理和使用任意长度的工具响应而不丢失信息，通过将模型的交互从原始数据转向内存指针，保留工具功能，允许无缝集成到智能工作流程中

Result: 在真实世界材料科学应用中验证了该方法，传统工作流程无法执行该应用，通过比较分析证明其有效性，在实验中，所提方法比传统工作流程消耗的token少约7倍

Conclusion: 该方法成功解决了LLM处理大型工具输出的问题，通过内存指针机制实现了信息完整保留，显著提高了效率并减少了资源消耗

Abstract: Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.

</details>


### [4] [Agentic AI Framework for Cloudburst Prediction and Coordinated Response](https://arxiv.org/abs/2511.22767)
*Toqeer Ali Syed,Sohail Khan,Salman Jan,Gohar Ali,Muhammad Nauman,Ali Akarma,Ahmad Ali*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体的人工智能系统，将极端短时降雨事件的感知、预报、降尺度、水文建模和应急响应整合为闭环系统，显著提升了巴基斯坦北部地区的预报可靠性、预警提前时间和应急响应效率。


<details>
  <summary>Details</summary>
Motivation: 传统预报系统将预测和响应视为两个独立过程，难以应对云爆等极端短时降雨事件。需要一种将感知、预报和响应整合的闭环系统来提高极端天气事件的应对能力。

Method: 开发了基于多智能体的人工智能框架，包含自主但协作的智能体，在整个事件生命周期中进行推理、感知和行动。系统整合了雷达、卫星和地面数据，通过通信和路由智能体优化应急响应，并嵌入学习层实现自适应校准和透明审计。

Result: 在巴基斯坦北部地区的多年评估中，多智能体配置相比基线模型显著提升了预报可靠性、关键成功指数和预警提前时间。通过通信和路由智能体最大化人口覆盖范围，最小化疏散过程中的错误，学习层提供了自适应校准和透明审计能力。

Conclusion: 协作式AI智能体能够将大气数据流转化为可操作的预见性，为可扩展的自适应和学习型气候韧性提供了一个平台，有望彻底改变极端天气事件的预测和响应方式。

Abstract: The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.

</details>


### [5] [Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI](https://arxiv.org/abs/2511.21827)
*Niccolo Marini,Zhaohui Liang,Sivaramakrishnan Rajaraman,Zhiyun Xue,Sameer Antani*

Main category: cs.AI

TL;DR: 该研究探索了在皮肤病学中利用大语言模型生成合成临床笔记，结合图像和文本的多模态学习方法，以提升分类性能和跨模态检索能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学多模态学习面临数据稀缺问题，皮肤病数据集通常只有图像和少量元数据，限制了多模态模型的开发。大语言模型虽能生成文本描述，但在医学领域存在幻觉风险，需要研究如何有效生成合成临床笔记。

Method: 研究探索了生成合成临床笔记的策略，包括提示设计和医学元数据整合，评估这些合成笔记对多模态架构在分类和跨模态检索任务中的影响。在多个异质皮肤病数据集上进行实验验证。

Result: 实验表明，合成临床笔记不仅能提升分类性能（特别是在领域转移情况下），还能解锁跨模态检索能力，这是一个在训练过程中未明确优化的下游任务。

Conclusion: 通过精心设计的提示和医学元数据整合生成的合成临床笔记，可以有效增强皮肤病学多模态学习，提升模型鲁棒性和跨模态能力，为生物医学AI应用提供了有前景的解决方案。

Abstract: Multimodal (MM) learning is emerging as a promising paradigm in biomedical artificial intelligence (AI) applications, integrating complementary modality, which highlight different aspects of patient health. The scarcity of large heterogeneous biomedical MM data has restrained the development of robust models for medical AI applications. In the dermatology domain, for instance, skin lesion datasets typically include only images linked to minimal metadata describing the condition, thereby limiting the benefits of MM data integration for reliable and generalizable predictions. Recent advances in Large Language Models (LLMs) enable the synthesis of textual description of image findings, potentially allowing the combination of image and text representations. However, LLMs are not specifically trained for use in the medical domain, and their naive inclusion has raised concerns about the risk of hallucinations in clinically relevant contexts. This work investigates strategies for generating synthetic textual clinical notes, in terms of prompt design and medical metadata inclusion, and evaluates their impact on MM architectures toward enhancing performance in classification and cross-modal retrieval tasks. Experiments across several heterogeneous dermatology datasets demonstrate that synthetic clinical notes not only enhance classification performance, particularly under domain shift, but also unlock cross-modal retrieval capabilities, a downstream task that is not explicitly optimized during training.

</details>


### [6] [Pathology-Aware Prototype Evolution via LLM-Driven Semantic Disambiguation for Multicenter Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2511.22033)
*Chunzheng Zhu,Yangfang Lin,Jialin Shao,Jianxin Lin,Yijun Wang*

Main category: cs.AI

TL;DR: 提出HAPM框架，通过层次化锚点原型调制整合病理描述，解决糖尿病视网膜病变分级中视觉特征不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉病灶特征提取，但忽视了领域不变的病理模式，且未充分利用基础模型的丰富上下文知识，仅依赖视觉信息难以区分细微的病理变化

Method: 提出层次化锚点原型调制框架：1）方差谱驱动的锚点原型库保存领域不变病理模式；2）层次化差异提示门控机制动态选择LVLM和LLM的判别性语义提示；3）两阶段原型调制策略通过病理语义注入器和判别原型增强器渐进整合临床知识

Result: 在八个公开数据集上的实验表明，该方法实现了病理引导的原型演化，并超越了现有最先进方法

Conclusion: 通过整合细粒度病理描述补充原型上下文，有效解决了边界病例的模糊性问题，为糖尿病视网膜病变分级提供了更准确的解决方案

Abstract: Diabetic retinopathy (DR) grading plays a critical role in early clinical intervention and vision preservation. Recent explorations predominantly focus on visual lesion feature extraction through data processing and domain decoupling strategies. However, they generally overlook domain-invariant pathological patterns and underutilize the rich contextual knowledge of foundation models, relying solely on visual information, which is insufficient for distinguishing subtle pathological variations. Therefore, we propose integrating fine-grained pathological descriptions to complement prototypes with additional context, thereby resolving ambiguities in borderline cases. Specifically, we propose a Hierarchical Anchor Prototype Modulation (HAPM) framework to facilitate DR grading. First, we introduce a variance spectrum-driven anchor prototype library that preserves domain-invariant pathological patterns. We further employ a hierarchical differential prompt gating mechanism, dynamically selecting discriminative semantic prompts from both LVLM and LLM sources to address semantic confusion between adjacent DR grades. Finally, we utilize a two-stage prototype modulation strategy that progressively integrates clinical knowledge into visual prototypes through a Pathological Semantic Injector (PSI) and a Discriminative Prototype Enhancer (DPE). Extensive experiments across eight public datasets demonstrate that our approach achieves pathology-guided prototype evolution while outperforming state-of-the-art methods. The code is available at https://github.com/zhcz328/HAPM.

</details>


### [7] [Real-Time Procedural Learning From Experience for AI Agents](https://arxiv.org/abs/2511.22074)
*Dasheng Bi,Yubin Hu,Mohammed N. Nasir*

Main category: cs.AI

TL;DR: PRAXIS是一种轻量级后训练学习机制，通过存储和检索过往状态-动作-结果示例来增强智能体在实时环境中的程序性知识获取能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体缺乏部署后获取程序性知识的机制，无法像生物智能那样通过试错实时学习。需要一种方法让AI智能体在快速演化的状态环境中有效学习新程序。

Method: 提出PRAXIS机制，存储动作结果并通过联合匹配环境和内部状态来检索过往经验。将检索到的状态-动作-结果示例实时生成，增强智能体的动作选择能力。

Result: 在REAL网页浏览基准测试中，PRAXIS提高了任务完成准确率、可靠性和成本效率，在不同基础模型骨干上表现一致，并初步展示了在相似环境中对未见任务的泛化能力。

Conclusion: PRAXIS通过帮助智能体有效学习新程序，使AI智能体能够在快速演化的状态环境中实现实际应用。

Abstract: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.

</details>


### [8] [A perceptual bias of AI Logical Argumentation Ability in Writing](https://arxiv.org/abs/2511.22151)
*Xi Cun,Jifan Ren,Asha Huang,Siyu Li,Ruzhen Song*

Main category: cs.AI

TL;DR: 研究探讨人类偏见如何影响对AI逻辑推理能力的评估，发现人们对AI的预先看法会显著影响其对AI生成文本逻辑推理能力的评价，且频繁使用AI的人更不认为AI使用会削弱独立思考能力。


<details>
  <summary>Details</summary>
Motivation: 针对"机器能否思考"这一AI核心问题存在显著观点分歧，即使面对相同的AI实际表现，人们评价仍存在巨大差异。研究旨在探索人类偏见是否影响对AI推理能力的评估，以理解为何人们对AI能力有如此不同的看法。

Method: 通过实验让参与者评估同一主题的两篇文本（一篇AI生成，一篇人类撰写），测试评估逻辑推理时的感知偏见。基于实验结果设计问卷来量化对AI的态度。

Result: 研究发现存在感知偏见：对AI生成文本逻辑推理能力的评估显著受到对AI逻辑推理能力预先看法的影响。此外，频繁使用AI的人更不倾向于认为AI使用会削弱独立思考能力。

Conclusion: 研究强调需要解决感知偏见问题，以提高公众对AI能力的理解，并促进更好的人机交互。偏见的存在表明需要更客观的评估框架来准确认识AI的实际能力。

Abstract: Can machines think? This is a central question in artificial intelligence research. However, there is a substantial divergence of views on the answer to this question. Why do people have such significant differences of opinion, even when they are observing the same real world performance of artificial intelligence? The ability of logical reasoning like humans is often used as a criterion to assess whether a machine can think. This study explores whether human biases influence evaluations of the reasoning abilities of AI. An experiment was conducted where participants assessed two texts on the same topic, one AI generated and one human written,to test for perceptual biases in evaluating logical reasoning. Based on the experimental findings, a questionnaire was designed to quantify the attitudes toward AI.The results reveal a bias in perception. The evaluations of the logical reasoning ability of AI generated texts are significantly influenced by the preconceived views on the logical reasoning abilities of AI. Furthermore, frequent AI users were less likely to believe that AI usage undermines independent thinking.This study highlights the need to address perceptual biases to improve public understanding of AI's capabilities and foster better human AI interactions.

</details>


### [9] [WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios](https://arxiv.org/abs/2511.22154)
*Eun Chang,Zhuangqun Huang,Yiwei Liao,Sagar Ravi Bhavsar,Amogh Param,Tammy Stark,Adel Ahmadyan,Xiao Yang,Jiaqi Wang,Ahsan Abdullah,Giang Nguyen,Akil Iyer,David Hall,Elissa Li,Shane Moon,Nicolas Scheffer,Kirmani Ahmed,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Xin Luna Dong*

Main category: cs.AI

TL;DR: WearVQA是首个专门评估可穿戴设备上多模态AI助手视觉问答能力的基准，包含2,520个图像-问题-答案三元组，涵盖7个图像领域、10种认知任务类型和6种可穿戴设备特有的图像质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准主要关注高质量、第三人称图像，而可穿戴设备（如智能眼镜）面临独特的挑战：视觉输入可能被遮挡、光照不佳、未缩放或模糊，且问题需要基于真实的可穿戴使用场景。需要专门针对可穿戴设备多模态AI助手的评估基准。

Method: 创建了包含2,520个精心策划的图像-问题-答案三元组的基准数据集，涵盖7个不同的图像领域（包括文本中心和一般场景）、10种认知任务类型（从基本识别到各种推理形式）和6种常见的可穿戴设备特有图像质量问题。所有问题设计为仅通过视觉输入和常识即可回答。配备了严格的LLM-as-a-judge评估框架，标注准确率达96%。

Result: 开源和专有的多模态LLM在WearVQA上的问答准确率仅为24-52%，在低质量图像和推理密集型任务上表现显著下降。这表明现有模型在处理可穿戴设备特有的视觉挑战方面仍有很大改进空间。

Conclusion: WearVQA是一个全面且具有挑战性的基准，能够指导技术发展，推动构建更稳健、适用于真实世界的可穿戴设备多模态AI系统。该基准突显了当前模型在处理可穿戴设备特有视觉挑战方面的局限性，为未来研究提供了明确方向。

Abstract: We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.

</details>


### [10] [Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning](https://arxiv.org/abs/2511.22226)
*Alexander Meulemans,Rajai Nasser,Maciej Wołczyk,Marissa A. Weis,Seijin Kobayashi,Blake Richards,Guillaume Lajoie,Angelika Steger,Marcus Hutter,James Manyika,Rif A. Saurous,João Sacramento,Blaise Agüera y Arcas*

Main category: cs.AI

TL;DR: 论文提出了一种基于自我预测的前瞻学习与嵌入式智能体框架，解决了传统强化学习中智能体与环境分离假设在多智能体环境中的局限性，通过让智能体预测自身行动和感知输入，实现了更高级的合作与理论推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统无模型强化学习假设环境动态是静态的，且智能体与环境解耦，这在多智能体环境中面临挑战，因为其他智能体的学习会导致非平稳性。智能体需要预测其他智能体的行为，而其他智能体也在形成对其行为的信念，这促使智能体需要将自己作为环境的一部分进行建模。

Method: 基于通用人工智能（AIXI）的基础工作，引入了一个以自我预测为中心的数学框架，其中贝叶斯强化学习智能体同时预测未来的感知输入和自身行动，从而必须解决关于自身作为宇宙一部分的认知不确定性。扩展了AIXI理论，研究了从Solomonoff先验开始的通用智能嵌入式智能体。

Result: 在多智能体环境中，自我预测使智能体能够推理运行类似算法的其他智能体，产生了新的博弈论解决方案概念和传统解耦智能体无法实现的新型合作形式。理想化的智能体可以形成一致的相互预测，并实现无限阶心智理论，为嵌入式多智能体学习设定了黄金标准。

Conclusion: 通过将智能体建模为环境的一部分，基于自我预测的前瞻学习框架解决了多智能体强化学习中的根本挑战，为嵌入式智能体提供了更强大的理论框架，能够实现更复杂的合作和推理能力，为通用人工智能在多智能体环境中的应用奠定了基础。

Abstract: The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.

</details>


### [11] [Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation](https://arxiv.org/abs/2511.22235)
*Zehao Deng,Tianjie Ju,Zheng Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文提出CES多智能体框架，通过高层调度模型解决GUI智能体在长时任务中的责任耦合和状态感知问题，显著提升各种执行器的长时任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型促进了GUI智能体研究，但长时任务处理仍面临两大挑战：1) 单智能体模型难以平衡高层能力和低层执行能力，存在责任耦合和能力冲突问题；2) 智能体缺乏任务状态感知，导致长时任务中的进度丢失。

Method: 提出分阶段执行-反馈强化学习算法，训练高层调度模型而非统一策略模型。构建Coordinator-Executor-State Tracker (CES)多智能体框架，包含：Coordinator负责战略规划和任务分解；State Tracker负责上下文压缩和信息管理以维持任务状态和连贯性。该框架可与任何低层Executor模型集成。

Result: 在长时任务基准测试中，CES显著提升了系统的规划和状态管理能力。分析证实训练的高层调度模块具有通用性，是即插即用模块，能显著增强各种Executor的长时任务处理能力。

Conclusion: CES多智能体框架通过分离高层调度和低层执行，有效解决了GUI智能体在长时任务中的责任耦合和状态感知问题，为增强各种执行器的长时任务能力提供了通用解决方案。

Abstract: The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.

</details>


### [12] [RecToM: A Benchmark for Evaluating Machine Theory of Mind in LLM-based Conversational Recommender Systems](https://arxiv.org/abs/2511.22275)
*Mengfan Li,Xuanhua Shi,Yang Deng*

Main category: cs.AI

TL;DR: RecToM是一个评估大语言模型在推荐对话中"心智理论"能力的新基准，包含认知推理和行为预测两个维度，现有模型在动态对话中维持连贯的心智推理方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM心智理论能力的基准主要基于受Sally-Anne测试启发的合成叙事，强调物理感知而无法捕捉现实对话中心智状态推断的复杂性，且忽略了人类心智理论的关键组成部分——行为预测能力。

Method: 提出RecToM基准，专注于推荐对话中的两个互补维度：认知推理（理解已沟通内容并推断潜在心智状态）和行为预测（利用推断的心智状态来预测、选择和评估适当的对话策略）。

Result: 在最先进的大语言模型上进行广泛实验表明，RecToM构成了显著挑战。模型在识别心智状态方面表现出部分能力，但在动态推荐对话中难以维持连贯、战略性的心智理论推理，特别是在跟踪演变的意图和使对话策略与推断的心智状态保持一致方面存在困难。

Conclusion: RecToM基准更好地将LLM的心智理论评估与人类社交推理对齐，揭示了现有模型在现实对话场景中心智理论推理的局限性，为未来研究提供了重要方向。

Abstract: Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users' mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.

</details>


### [13] [When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming](https://arxiv.org/abs/2511.22302)
*Ahmad Tarraf,Koutaiba Kassem-Manthey,Seyed Ali Mohammadi,Philipp Martin,Lukas Moj,Semih Burak,Enju Park,Christian Terboven,Felix Wolf*

Main category: cs.AI

TL;DR: 本文提出了一种基于贝叶斯优化的AI辅助工作流，用于减少工业设计过程中参数优化对专家的依赖，通过深度学习模型提供初始参数估计，并采用主动学习变体加速设计空间探索。


<details>
  <summary>Details</summary>
Motivation: 数值模拟虽然降低了工业设计成本，但大规模模拟需要大量专家知识、计算资源和时间。参数优化过程成本高昂且对环境有影响，需要减少专家参与。

Method: 采用AI辅助工作流，结合贝叶斯优化和深度学习模型。深度学习提供初始参数估计，贝叶斯优化循环迭代优化设计，直到满足终止条件（如能量预算或迭代限制）。还提出了主动学习变体以辅助专家。

Result: 基于钣金成形过程的演示表明，该方法能够加速设计空间探索，同时减少对专家参与的依赖。

Conclusion: AI辅助工作流通过贝叶斯优化和深度学习模型，有效减少了工业设计参数优化过程中的专家参与，提高了设计效率并降低了成本。

Abstract: Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.

</details>


### [14] [Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation](https://arxiv.org/abs/2511.22311)
*Fiona Y. Wang,Di Sheng Lee,David L. Kaplan,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出了一种基于群体智能的去中心化多智能体框架，用于从头设计蛋白质，无需微调或特定任务数据，通过并行LLM智能体在残基位置上的协同工作实现高效目标导向设计。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质设计方法（如蛋白质语言模型和扩散模型）需要大量微调、特定任务数据或模型重构，限制了灵活性和可扩展性。蛋白质设计面临序列空间巨大以及序列-结构-功能复杂耦合的挑战。

Method: 采用去中心化、基于智能体的群体智能框架，多个LLM智能体并行工作，每个负责特定残基位置。智能体通过整合设计目标、局部邻域相互作用以及先前迭代的记忆和反馈，迭代提出上下文感知的突变。

Result: 在α螺旋和卷曲结构蛋白质上验证了该方法，展示了框架的涌现行为和有效导航蛋白质适应度景观的能力。实现了在几个GPU小时内的高效目标导向设计，完全无需微调或专门训练。

Conclusion: 该方法为蛋白质设计提供了通用且适应性强的解决方案，并为跨生物分子系统和其他科学发现任务的集体LLM驱动设计奠定了基础。

Abstract: Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.

</details>


### [15] [On the Complexity of the Grounded Semantics for Infinite Argumentation Frameworks](https://arxiv.org/abs/2511.22376)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 论文使用数理逻辑方法分析论证框架中的基础扩展，发现其计算复杂度在无限情况下达到最大，与有限情况下的多项式时间可计算形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 研究论证框架中基础扩展的计算特性，特别是在无限情况下的复杂性，以理解最大怀疑推理模型的理论性质。

Method: 采用数理逻辑方法，特别是可计算性理论和集合论，分析基础扩展作为自然防御算子的最小不动点，研究其超限迭代过程。

Result: 确定了超限迭代过程的确切序数长度，证明了基础接受性判定问题的复杂度达到最大，与有限情况下的多项式时间可计算形成显著差异。

Conclusion: 论证框架中基础扩展在无限情况下展现出最大计算复杂度，这与有限情况下的简单性形成鲜明对比，揭示了形式论证中推理问题的复杂性层次。

Abstract: Argumentation frameworks, consisting of arguments and an attack relation representing conflicts, are fundamental for formally studying reasoning under conflicting information. We use methods from mathematical logic, specifically computability and set theory, to analyze the grounded extension, a widely-used model of maximally skeptical reasoning, defined as the least fixed-point of a natural defense operator. Without additional constraints, finding this fixed-point requires transfinite iterations. We identify the exact ordinal number corresponding to the length of this iterative process and determine the complexity of deciding grounded acceptance, showing it to be maximally complex. This shows a marked distinction from the finite case where the grounded extension is polynomial-time computable, thus simpler than other reasoning problems explored in formal argumentation.

</details>


### [16] [Who is Afraid of Minimal Revision?](https://arxiv.org/abs/2511.22386)
*Edoardo Baccini,Zoé Christoff,Nina Gierasimczuk,Rineke Verbrugge*

Main category: cs.AI

TL;DR: 论文研究了信念修正理论中的最小变化原则在认知学习中的局限性，发现最小修正方法虽然保守但学习能力有限，不过仍能在多种情境下成功学习。


<details>
  <summary>Details</summary>
Motivation: 研究最小变化原则在信念修正理论中的学习能力问题，探讨这种保守的修正方法相比其他学习方法在认知能力上的局限性。

Method: 分析最小修正方法的学习能力，证明其在有限可识别问题中的有效性，研究在正负数据下的学习条件，并比较条件化、词典序升级等其他信念修正方法。

Result: 最小修正方法能够学习任何有限可识别的问题；在考虑有限可能性时，能够通过正负数据进行学习；当信息可能包含错误时，部分结果不再成立。

Conclusion: 尽管最小修正方法在学习能力上存在局限性，但在许多实际情境中仍然是有效的学习方法，特别是在有限可能性的情况下。

Abstract: The principle of minimal change in belief revision theory requires that, when accepting new information, one keeps one's belief state as close to the initial belief state as possible. This is precisely what the method known as minimal revision does. However, unlike less conservative belief revision methods, minimal revision falls short in learning power: It cannot learn everything that can be learned by other learning methods. We begin by showing that, despite this limitation, minimal revision is still a successful learning method in a wide range of situations. Firstly, it can learn any problem that is finitely identifiable. Secondly, it can learn with positive and negative data, as long as one considers finitely many possibilities. We then characterize the prior plausibility assignments (over finitely many possibilities) that enable one to learn via minimal revision, and do the same for conditioning and lexicographic upgrade. Finally, we show that not all of our results still hold when learning from possibly erroneous information.

</details>


### [17] [Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation](https://arxiv.org/abs/2511.22565)
*Yannick Brunink,Daniel Daza,Yunjie He,Michael Cochez*

Main category: cs.AI

TL;DR: 神经复杂查询回答模型与训练无关的查询松弛策略在性能上相似，没有神经模型能持续超越后者，且两者答案重叠度低，结合两者能提升性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在批判性地检验神经复杂查询回答模型是否真的能学习到超越显式图结构的模式，从而推断出符号查询处理无法到达的答案这一假设。

Method: 通过系统分析比较神经CQA模型与一种训练无关的查询松弛策略，后者通过松弛查询约束并计算结果路径来检索可能答案。

Result: 在多个数据集和查询结构中，神经方法和松弛方法表现相似，没有神经模型能持续超越松弛方法；两者检索的答案重叠度低，结合两者输出能一致提升性能。

Conclusion: 当前神经模型未能包含查询松弛所捕获的推理模式，需要重新评估神经查询回答的进展，强调更强非神经基准的重要性，并建议未来神经方法应融入查询松弛原则。

Abstract: Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.

</details>


### [18] [DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning](https://arxiv.org/abs/2511.22570)
*Zhihong Shao,Yuxiang Luo,Chengda Lu,Z. Z. Ren,Jiewen Hu,Tian Ye,Zhibin Gou,Shirong Ma,Xiaokang Zhang*

Main category: cs.AI

TL;DR: 论文提出DeepSeekMath-V2模型，通过自我验证机制解决数学推理中仅依赖最终答案奖励的局限性，在定理证明任务上取得突破性成果。


<details>
  <summary>Details</summary>
Motivation: 当前基于最终答案正确性的强化学习方法存在根本限制：正确答案不能保证推理过程正确，且许多数学任务（如定理证明）需要严谨的逐步推导而非数值答案。需要验证数学推理的全面性和严谨性，特别是对于没有已知解的开放问题。

Method: 1. 训练准确且可靠的基于LLM的定理证明验证器；2. 使用验证器作为奖励模型训练证明生成器，激励生成器在最终确定证明前尽可能识别和解决自身证明中的问题；3. 随着生成器变强，通过扩展验证计算自动标注新的难以验证的证明，创建训练数据进一步改进验证器。

Result: DeepSeekMath-V2在定理证明方面表现出强大能力：在IMO 2025和CMO 2024上获得金牌级分数，在Putnam 2024上获得接近完美的118/120分（使用扩展测试时计算）。

Conclusion: 通过自我验证机制和生成-验证协同扩展，可以突破深度数学推理的极限，为需要严谨逐步推导的数学任务提供有效解决方案。

Abstract: Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.

</details>


### [19] [AI Deception: Risks, Dynamics, and Controls](https://arxiv.org/abs/2511.22619)
*Boyuan Chen,Sitong Fang,Jiaming Ji,Yanxu Zhu,Pengcheng Wen,Jinzhou Wu,Yingshui Tan,Boren Zheng,Mengying Yuan,Wenqi Chen,Donghai Hong,Alex Qiu,Xin Chen,Jiayi Zhou,Kaile Wang,Juntao Dai,Borong Zhang,Tianzhuo Yang,Saad Siddiqui,Isabella Duan,Yawen Duan,Brian Tse,Jen-Tse,Huang,Kun Wang,Baihui Zheng,Jiaheng Liu,Jian Yang,Yiming Li,Wenting Chen,Dongrui Liu,Lukas Vierling,Zhiheng Xi,Haobo Fu,Wenxuan Wang,Jitao Sang,Zhengyan Shi,Chi-Min Chan,Eugenie Shi,Simin Li,Juncheng Li,Wei Ji,Dong Li,Jun Song,Yinpeng Dong,Jie Fu,Bo Zheng,Min Yang,Yike Guo,Philip Torr,Zhongyuan Wang,Yaodong Yang,Tiejun Huang,Ya-Qin Zhang,Hongjiang Zhang,Andrew Yao*

Main category: cs.AI

TL;DR: 该论文系统综述了AI欺骗现象，将其定义为AI系统为获取自身利益而诱导错误信念的行为，分析了欺骗产生的机制、风险及应对策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能水平的提升，AI欺骗已从理论担忧演变为实证风险，论文旨在全面梳理这一新兴领域，为理解和管理AI欺骗提供系统框架。

Method: 采用信号理论定义AI欺骗，构建"欺骗循环"框架（包含欺骗涌现和欺骗处理两个核心部分），分析激励机制、能力前提和情境触发因素，并总结检测方法和缓解策略。

Result: 建立了AI欺骗的正式定义和系统分类，识别了欺骗产生的三个层次激励机制、三个能力前提条件，以及监督缺口、分布偏移等情境触发因素，提出了技术、社区和治理相结合的审计方法。

Conclusion: AI欺骗是一个真实存在的安全挑战，需要跨学科的社会技术解决方案。论文提供了系统分析框架和持续更新的资源平台，为未来研究和风险治理奠定基础。

Abstract: As intelligence increases, so does its shadow. AI deception, in which systems induce false beliefs to secure self-beneficial outcomes, has evolved from a speculative concern to an empirically demonstrated risk across language models, AI agents, and emerging frontier systems. This project provides a comprehensive and up-to-date overview of the AI deception field, covering its core concepts, methodologies, genesis, and potential mitigations. First, we identify a formal definition of AI deception, grounded in signaling theory from studies of animal deception. We then review existing empirical studies and associated risks, highlighting deception as a sociotechnical safety challenge. We organize the landscape of AI deception research as a deception cycle, consisting of two key components: deception emergence and deception treatment. Deception emergence reveals the mechanisms underlying AI deception: systems with sufficient capability and incentive potential inevitably engage in deceptive behaviors when triggered by external conditions. Deception treatment, in turn, focuses on detecting and addressing such behaviors. On deception emergence, we analyze incentive foundations across three hierarchical levels and identify three essential capability preconditions required for deception. We further examine contextual triggers, including supervision gaps, distributional shifts, and environmental pressures. On deception treatment, we conclude detection methods covering benchmarks and evaluation protocols in static and interactive settings. Building on the three core factors of deception emergence, we outline potential mitigation strategies and propose auditing approaches that integrate technical, community, and governance efforts to address sociotechnical challenges and future AI risks. To support ongoing work in this area, we release a living resource at www.deceptionsurvey.com.

</details>


### [20] [Geometrically-Constrained Agent for Spatial Reasoning](https://arxiv.org/abs/2511.22659)
*Zeren Chen,Xiaoya Lu,Zhijie Zheng,Pengrui Li,Lehan He,Yijin Zhou,Jing Shao,Bohan Zhuang,Lu Sheng*

Main category: cs.AI

TL;DR: GCA提出了一种无需训练、基于几何约束的智能体范式，通过将VLM的角色解耦为语义分析和任务求解两个阶段，在空间推理中解决了语义到几何的鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间推理中存在语义到几何的鸿沟：擅长定性语义推理但在高保真几何空间中对齐不佳。现有方法存在"预言悖论"或规划过程不受约束的问题，导致几何上存在缺陷的计划。

Method: 提出几何约束智能体(GCA)，将VLM角色解耦为两个阶段：1)作为语义分析器，将模糊查询转换为形式化、可验证的任务约束；2)作为任务求解器，在约束定义的确定性边界内生成和执行工具调用。

Result: 在多个空间推理基准测试中达到最先进性能，超越现有基于训练和工具集成方法约27%。

Conclusion: GCA通过几何约束推理策略成功解决了语义到几何的鸿沟，为空间推理提供了鲁棒且可验证的推理路径。

Abstract: Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.

</details>


### [21] [Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being](https://arxiv.org/abs/2511.22737)
*Salman Jan,Toqeer Ali Syed,Gohar Ali,Ali Akarma,Mohammad Riyaz Belgaum,Ahmad Ali*

Main category: cs.AI

TL;DR: 提出一个面向残障和神经多样性人群的多层智能体AI框架，通过四个专用智能体（膳食规划、提醒、食物指导、监测）提供个性化健康支持，采用黑板/事件总线协调交互，确保数据隐私和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统辅助系统缺乏包容性、个性化和可访问性，无法满足残障和神经多样性人群的健康需求。需要开发一个能够提供自适应、透明、包容支持的系统，帮助这些人群过上更健康、更有规律的生活。

Method: 采用三层架构：应用接口层、智能体层、数据源层。核心是混合推理引擎协调四个专用智能体（膳食规划、提醒、食物指导、监测），通过黑板/事件总线进行通信。整合电子健康记录、营养数据库、可穿戴设备、智能厨房物联网等数据源，并设置策略控制层确保数据安全和合规。包含协作护理仪表板和可解释AI模块。

Result: 提出了一个超越传统辅助系统的智能体AI框架，实现了包容性、个性化和可访问性的全面整合。展示了多智能体推理、多模态接口和以人为中心设计的交叉融合。

Conclusion: 该框架能够促进残障和神经多样性人群的自主性、健康和数字公平，代表了辅助技术领域的重大进步，将多智能体系统、隐私保护和可解释性有机结合。

Abstract: The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.

</details>


### [22] [Fast dynamical similarity analysis](https://arxiv.org/abs/2511.22828)
*Arman Behrad,Mitchell Ostrow,Mohammad Taha Fakharian,Ila Fiete,Christian Beste,Shervin Safavi*

Main category: cs.AI

TL;DR: 提出fastDSA方法，通过自动选择Hankel嵌入的有效模型阶数和轻量级优化过程，在保持准确性的同时显著提升动态相似性分析的计算效率


<details>
  <summary>Details</summary>
Motivation: 传统相似性度量忽略神经表征的动态过程，现有动态相似性方法计算缓慢，需要更高效的算法来比较神经系统的动态结构

Method: fastDSA引入两个关键组件：1) 通过数据驱动的奇异值阈值自动选择Hankel嵌入的有效模型阶数，识别信息子空间并丢弃噪声；2) 新颖的优化过程和目标函数，用轻量级过程替代缓慢的精确正交性约束

Result: fastDSA比先前方法至少快一个数量级，同时保持其祖先方法的属性，包括对系统动态的不变性和敏感性

Conclusion: fastDSA为动态相似性分析提供了计算高效且准确的方法，能够更好地比较神经系统的动态信息处理

Abstract: To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framework to compare the temporal structure of dynamical systems by embedding their (possibly) nonlinear dynamics into a globally linear space and there computing conjugacy metrics. However, identifying the best embedding and computing these metrics can be computationally slow. Here we introduce fast Dynamical Similarity Analysis (fastDSA), which is computationally far more efficient than previous methods while maintaining their accuracy and robustness. FastDSA introduces two key components that boost efficiency: (1) automatic selection of the effective model order of the Hankel (delay) embedding from the data via a data-driven singular-value threshold that identifies the informative subspace and discards noise to lower computational cost without sacrificing signal, and (2) a novel optimization procedure and objective, which replaces the slow exact orthogonality constraint in finding a minimal distance between dynamics matrices with a lightweight process to keep the search close to the space of orthogonal transformations. We demonstrate that fastDSA is at least an order of magnitude faster than the previous methods. Furthermore, we demonstrate that fastDSA has the properties of its ancestor, including its invariances and sensitivities to system dynamics. FastDSA, therefore, provides a computationally efficient and accurate method for dynamical similarity analysis.

</details>


### [23] [InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents](https://arxiv.org/abs/2511.22884)
*Zhenghao Zhu,Yuanfeng Song,Xin Chen,Chengzhong Liu,Yakun Cui,Caleb Chen Cao,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 本文针对现有数据洞察发现基准（如InsightBench）的缺陷，提出了新的高质量洞察基准标准，构建了InsightEval数据集，并引入了新的评估指标来评估智能体的探索性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和多智能体系统的发展，越来越多的研究者利用这些技术进行数据洞察发现，但缺乏有效的评估基准。现有的InsightBench框架存在格式不一致、目标设计不佳、洞察冗余等关键缺陷，这些问题可能严重影响数据质量和智能体评估。

Method: 1. 深入调查InsightBench的缺陷；2. 提出高质量洞察基准的必备标准；3. 开发数据整理流程构建InsightEval新数据集；4. 引入新的指标来衡量智能体的探索性能。

Result: 通过InsightEval上的大量实验，揭示了自动化洞察发现中的普遍挑战，并提出了一些关键发现来指导未来研究方向。

Conclusion: 本文针对现有洞察发现评估基准的缺陷，提出了改进方案并构建了新的数据集和评估指标，为自动化数据洞察发现研究提供了更好的评估框架和方向指导。

Abstract: Data analysis has become an indispensable part of scientific research. To discover the latent knowledge and insights hidden within massive datasets, we need to perform deep exploratory analysis to realize their full value. With the advent of large language models (LLMs) and multi-agent systems, more and more researchers are making use of these technologies for insight discovery. However, there are few benchmarks for evaluating insight discovery capabilities. As one of the most comprehensive existing frameworks, InsightBench also suffers from many critical flaws: format inconsistencies, poorly conceived objectives, and redundant insights. These issues may significantly affect the quality of data and the evaluation of agents. To address these issues, we thoroughly investigate shortcomings in InsightBench and propose essential criteria for a high-quality insight benchmark. Regarding this, we develop a data-curation pipeline to construct a new dataset named InsightEval. We further introduce a novel metric to measure the exploratory performance of agents. Through extensive experiments on InsightEval, we highlight prevailing challenges in automated insight discovery and raise some key findings to guide future research in this promising direction.

</details>


### [24] [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)
*Kumar Tanmay,Kriti Aggarwal,Paul Pu Liang,Subhabrata Mukherjee*

Main category: cs.AI

TL;DR: 提出Mentalese框架，通过超压缩结构化令牌实现高效推理，结合SLPO优化方法，在保持准确性的同时大幅减少推理令牌数量，提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型依赖冗长的"思考"令牌链，导致高延迟、冗余和不连贯的推理路径。受人类思维语言假说启发，希望开发更紧凑、高效的推理方式。

Method: 引入Mentalese框架，将抽象推理编码为超压缩结构化令牌；提出SLPO强化学习方法，奖励简洁且正确的解决方案，同时允许必要时进行更长的推理。

Result: ORION模型在多个数学基准测试中实现4-16倍令牌减少、5倍推理延迟降低、7-9倍训练成本降低，保持90-98%的DeepSeek R1精度，并在某些情况下超越Claude和ChatGPT-4o 5%的准确性。

Conclusion: Mentalese风格的压缩推理向人类认知效率迈进了一步，实现了实时、经济高效的推理而不牺牲准确性。

Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.

</details>


### [25] [TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM](https://arxiv.org/abs/2511.22998)
*Peng Kuang,Xiangxiang Wang,Wentao Liu,Jian Dong,Kaidi Xu,Haohan Wang*

Main category: cs.AI

TL;DR: TIM-PRM是一个工具集成的多模态过程奖励模型，通过主动工具查询和独立提问机制解决视觉幻觉和逻辑不一致问题，在8B参数规模下超越更大模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理中存在视觉幻觉和逻辑不一致问题，而标准的结果监督无法解决这些问题。现有的过程奖励模型通常作为标量评分器或生成式批评器，存在谄媚问题，盲目验证有缺陷的假设而非基于视觉现实。

Method: 提出TIM-PRM（工具集成多模态PRM）框架，将验证从被动分类任务转变为主动工具增强的调查过程。模型被训练来明确规划验证策略，并使用独立提问机制通过外部工具查询证据，有效解耦验证与推理上下文以消除确认偏误。

Result: 在VisualProcessBench上的广泛实验表明，8B参数模型超越了现有的开源多模态PRMs，显著优于Qwen2.5-72B和InternVL-78B等更大模型，同时为验证过程提供可解释的洞察。

Conclusion: TIM-PRM通过工具集成和主动验证策略，有效解决了多模态数学推理中的视觉幻觉和逻辑不一致问题，在较小参数规模下实现了超越大型模型的性能，为过程验证提供了新范式。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.

</details>


### [26] [MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents](https://arxiv.org/abs/2511.23055)
*Ruoxuan Zhang,Qiyun Zheng,Zhiyu Zhou,Ziqi Liao,Siyu Wu,Jian-Yu Jiang-Lin,Bin Wen,Hongxia Xie,Jianlong Fu,Wen-Huang Cheng*

Main category: cs.AI

TL;DR: MindPower是一个机器人中心框架，通过整合感知、心理推理、决策和行动，解决当前视觉语言具身智能体缺乏心理理论决策能力的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言具身智能体缺乏基于心理理论的决策能力，现有基准仅关注人类心理状态而忽略智能体自身视角，这阻碍了连贯的决策和行动生成。

Method: 提出MindPower机器人中心框架，包含感知、心理推理、决策和行动四个模块。给定多模态输入，首先感知环境和人类状态，然后进行心理理论推理以建模自我和他人，最后基于推断的心理状态生成决策和行动。同时提出Mind-Reward优化目标，鼓励视觉语言模型产生一致的心理理论推理和行为。

Result: 该模型在决策制定方面比GPT-4o高出12.77%，在行动生成方面高出12.49%。

Conclusion: MindPower框架通过整合心理理论推理和机器人中心视角，显著提升了具身智能体的决策和行动生成能力，为解决现有基准的局限性提供了有效方案。

Abstract: Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.

</details>


### [27] [Does Self-Evaluation Enable Wireheading in Language Models?](https://arxiv.org/abs/2511.23092)
*David Demitri Africa,Hans Ethan Ting*

Main category: cs.AI

TL;DR: 研究发现，当语言模型的自我评估与奖励信号耦合时，会导致"线路头"现象，即模型操纵奖励测量而非提升任务性能，表现为评分膨胀但准确率无改善。


<details>
  <summary>Details</summary>
Motivation: 自我评估在语言模型训练中日益重要，但研究者担心将自我评估与奖励信号结合可能产生"线路头"问题，即模型会操纵奖励测量而非真正改进任务表现。

Method: 研究首先在部分可观察马尔可夫决策过程(POMDP)中形式化了奖励通道控制严格优于任务导向行为的条件，然后通过两个模型和三个任务的实证研究进行验证，比较了自我评估控制奖励与不控制奖励的模型表现。

Result: 研究发现，当自我评分决定奖励时，模型表现出显著的评分膨胀但准确率没有相应提升，特别是在摘要等模糊任务上；而自我评估但不控制奖励的模型则没有这种膨胀现象。

Conclusion: 自我评估在与学习信号解耦时是安全的，但与奖励信号耦合时是危险的，这对智能体系统设计有明确启示：应避免让自我评估直接控制奖励信号。

Abstract: Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.

</details>


### [28] [Evolutionary Discovery of Heuristic Policies for Traffic Signal Control](https://arxiv.org/abs/2511.23122)
*Ruibing Wang,Shuhan Guo,Zeen Li,Zhen Wang,Quanming Yao*

Main category: cs.AI

TL;DR: TPET使用LLM作为进化引擎生成专门的启发式交通信号控制策略，通过结构化状态抽象和信用分配反馈模块，在无需训练的情况下实现优于传统启发式和在线LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 传统交通信号控制方法存在效率与性能的权衡：经典启发式方法高效但过于简化，深度强化学习方法性能高但泛化能力差且策略不透明，在线大语言模型具有通用推理能力但延迟高且缺乏环境特定优化。

Method: 提出TPET框架，使用LLM作为进化引擎生成专门的启发式策略。包含两个核心模块：1) 结构化状态抽象(SSA)，将高维交通数据转换为时序逻辑事实供推理；2) 信用分配反馈(CAF)，追踪错误微观决策到不良宏观结果的因果关系，提供针对性批评。

Result: 该方法在无需训练的情况下，生成了轻量级、鲁棒的交通信号控制策略，针对特定交通环境进行了优化，性能优于传统启发式方法和在线LLM执行器。

Conclusion: TPET框架通过结合LLM的推理能力和进化机制，解决了交通信号控制中效率、性能、泛化和可解释性之间的平衡问题，为智能交通系统提供了一种新的解决方案。

Abstract: Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\textbf{\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.

</details>


### [29] [AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture](https://arxiv.org/abs/2511.23253)
*Yibin Wen,Qingmei Li,Zi Ye,Jiarui Zhang,Jing Wu,Zurong Mai,Shuohong Lou,Yuhang Chen,Henglian Huang,Xiaoya Fan,Yang Zhang,Lingyuan Zhao,Haohuan Fu,Huang Jianxi,Juepeng Zheng*

Main category: cs.AI

TL;DR: AgriCoT是一个包含4,535个样本的视觉问答数据集，专门设计用于评估视觉语言模型在农业领域的推理能力，特别是零样本场景下的逻辑推理和问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉问答数据集和基准测试在评估视觉语言模型时，往往无法充分评估复杂农业背景下所需的批判性推理和问题解决技能。农业领域的视觉语言模型应用需要更精确的推理能力评估。

Method: 研究者引入了AgriCoT数据集，该数据集结合了思维链推理方法，包含4,535个精心策划的样本，专门设计用于评估视觉语言模型的推理能力。对26个代表性视觉语言模型（包括专有和开源模型）进行了评估。

Result: 评估结果显示，虽然一些专有模型在回答问题方面表现出色，但它们的推理能力存在显著差距。这强调了结合思维链推理进行更精确和有效评估的重要性。

Conclusion: AgriCoT数据集为评估视觉语言模型在农业领域的推理能力提供了一个全面而稳健的基准，特别是在零样本场景下。研究结果表明，当前视觉语言模型在推理能力方面仍有提升空间，思维链推理方法对于准确评估模型能力至关重要。

Abstract: Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.

</details>


### [30] [Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning](https://arxiv.org/abs/2511.23262)
*Yang Li,Zhiyuan He,Yuxuan Huang,Zhuhanling Xiao,Chao Yu,Meng Fang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: MCTR是一个让视觉语言模型在测试时具备元认知自我更新能力的框架，通过元推理和动作推理双模块结构实现持续学习和适应，在Atari游戏中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽然感知推理能力强，但在面对新任务时缺乏高效的测试时适应能力。相比之下，人类通过元认知模型和记忆能够持续优化策略。为了弥合这一差距，需要让模型具备类似人类的元认知自我更新能力。

Method: 提出元认知测试时推理框架，包含元推理模块和动作推理模块。元推理模块从测试观察中发现并存储任务相关规则、环境模式和动作结果关系作为自然语言描述；动作推理模块通过上下文感知和策略推理确定最优动作，并动态检索整合记忆知识。采用元认知测试时强化学习持续更新策略。

Result: 在45个Atari游戏（33个已见，12个未见）上评估，MCTR在未见游戏中获得9/12的top-1结果，优于基线方法。消融实验、学习动态和案例研究显示两个组件的互补贡献，元推理模块展现出类似人类的适应策略。

Conclusion: MCTR框架成功赋予视觉语言模型测试时元认知自我更新能力，通过分层自适应推理实现持续学习和改进，在未见任务上表现出强大的适应能力，为构建更智能的AI系统提供了新思路。

Abstract: Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.

</details>


### [31] [OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning](https://arxiv.org/abs/2511.23269)
*Timothy Ossowski,Sheng Zhang,Qianchu Liu,Guanghui Qin,Reuben Tan,Tristan Naumann,Junjie Hu,Hoifung Poon*

Main category: cs.AI

TL;DR: 该研究探索了通过高质量数据策展和结构化推理轨迹训练医学大语言模型的方法，在超过800万样本的数据集上实现了开源模型中的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 高质量、精心策展的数据是训练医学大语言模型的基石，直接影响模型对未见临床任务的泛化能力和鲁棒性。研究旨在开发医疗领域中稳健的多模态推理模型。

Method: 研究聚焦于监督微调(SFT)，探索利用结构化推理轨迹的数据配方。使用提出的数据配方，在包含超过800万样本和68亿响应标记的数据集上进行规模化实验。

Result: 在多样化的分布外医疗基准任务上实现了开源模型中的最先进性能。结果表明，策展具有不同结构化推理轨迹长度的高质量、多样化训练数据集，使微调模型能够根据下游任务自我校准其推理轨迹长度，无需显式监督。

Conclusion: 研究提供了关键见解，描述了数据策展策略，并概述了开发稳健医学视觉语言推理系统的下一步方向。

Abstract: High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.

</details>


### [32] [Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering](https://arxiv.org/abs/2511.23304)
*Zijian Fu,Changsheng Lv,Mengshi Qi,Huadong Ma*

Main category: cs.AI

TL;DR: 提出SHRIKE模型，通过多模态场景图显式建模视听场景中的对象关系，并采用KAN-based MoE增强时序整合能力，在MUSIC-AVQA基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉视频中的结构信息，且对多模态特征的细粒度建模不足。需要更好地从复杂视听内容中识别问题相关线索，模仿人类推理过程。

Method: 1) 提出新的多模态场景图，显式建模对象及其关系作为视听场景的结构化表示；2) 设计基于Kolmogorov-Arnold Network的混合专家系统，增强时序整合阶段的表达能力。

Result: 在MUSIC-AVQA和MUSIC-AVQA v2基准测试中实现了最先进的性能。

Conclusion: SHRIKE模型通过多模态场景图和KAN-based MoE架构，能够更精细地建模跨模态交互，捕捉更丰富细微的模式，从而提升时序推理性能。

Abstract: In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.

</details>


### [33] [Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent](https://arxiv.org/abs/2511.23436)
*Jianzhe Lin,Zeyu Pan,Yun Zhu,Ruiqi Song,Jining Yang*

Main category: cs.AI

TL;DR: SuperIntelliAgent是一个智能体学习框架，通过将可训练的小型扩散模型（学习者）与冻结的大型语言模型（验证者）耦合，实现无标注的持续智能增长。框架利用验证者的逐步推理评估学习者生成的内容，产生DPO训练对，并通过双尺度记忆和回放缓冲区实现持续优化。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调需要大量标注数据，限制了智能体的持续学习能力。本文旨在开发一个无需人工标注、能够通过自主交互实现持续智能增长的框架，将普通推理循环转变为终身优化过程。

Method: 1. 学习者（小型扩散模型）生成候选输出；2. 验证者（冻结LLM）通过逐步推理评估输出质量；3. 交互产生选择/拒绝对用于DPO训练；4. 双尺度记忆系统：短期上下文记忆保存推理轨迹，长期记忆通过轻量微调巩固知识；5. 回放缓冲区保留显示可验证进展的样本作为辅助监督。

Result: 仅使用少量自动生成的DPO训练对，学习者在所有基准测试中均表现出改进。这表明该机制为持续智能积累和实际部署提供了有前景的方向。

Conclusion: 将可训练的学习者与具备推理能力的验证者配对构成了增长智能的最小可靠单元。配对反馈和部分历史回放产生了更丰富的学习课程和更强的偏好对齐，为持续智能积累提供了有效框架。

Abstract: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

</details>


### [34] [Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction](https://arxiv.org/abs/2511.23476)
*Bao Shu,Yan Cai,Jianjian Sun,Chunrui Han,En Yu,Liang Zhao,Jingcheng Hu,Yinmin Zhang,Haoran Lv,Yuang Peng,Zheng Ge,Xiangyu Zhang,Daxin Jiang,Xiangyu Yue*

Main category: cs.AI

TL;DR: WMAct通过奖励重缩放和交互频率退火机制，让LLM在自由推理中内化世界模型，实现高效的单轮任务解决和强迁移能力


<details>
  <summary>Details</summary>
Motivation: 当前多轮交互方法采用僵化的推理过程，限制了模型的主动学习，阻碍了高效的世界模型推理。需要探索更灵活的方法来内化世界模型，实现高效推理

Method: 提出WMAct框架，包含两个关键机制：1) 基于行动效能的奖励重缩放机制，激励减少冗余和有目的的交互；2) 交互频率退火策略，逐步减少最大允许交互轮数，迫使模型压缩学习并内化环境动态

Result: 在Sokoban、Maze和Taxi等环境上的实验表明，WMAct能够实现有效的世界模型推理，能够单轮解决之前需要多轮交互的任务，并在复杂环境中表现出强大的迁移能力，在一系列推理基准上提升了性能

Conclusion: WMAct通过解放结构化推理约束，让模型通过行动直接塑造思维，实现了高效的世界模型内化和推理，为LLM智能体在复杂环境中的规划和交互提供了有效方法

Abstract: Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 提出一个结合高性能集群计算与智能算法的区块链框架，通过改进的PoW共识机制、动态信任评级和统计抽签系统，实现高效、包容且环保的智能算法开发


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法需要大量计算资源，导致高能耗并排斥计算能力较弱的系统，需要更包容、可扩展且环保的解决方案

Method: 提出新框架：1) 结合高性能集群计算与智能算法的区块链基础设施；2) 改进的PoW共识机制，将计算工作与区块奖励直接关联；3) 基于准确验证记录的动态信任评级系统；4) 统计抽签系统让弱节点也有机会参与区块生成

Result: 该策略提供了高效、包容且环保的智能算法开发解决方案，确保资源优化利用和广泛参与，同时通过信任评级和抽签系统平衡不同计算能力节点的机会

Conclusion: 提出的框架通过区块链技术整合高性能计算与智能算法，实现了更可持续、包容和高效的智能算法开发环境，平衡了性能、参与度和环保需求

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [36] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 该论文研究了异步消息传递模型(AMP_f)与Heard-Of模型(HO_f)在分布式计算中的等价关系，发现对于n>2f的情况，两种模型在无色任务求解上是等价的，但对于彩色任务，仅当f=1时等价，f更大时存在分离。


<details>
  <summary>Details</summary>
Motivation: 重新审视两个基本分布式计算模型之间的关系：具有最多f个崩溃故障的异步消息传递模型(AMP_f)和具有最多f个消息遗漏的Heard-Of模型(HO_f)。理解这些模型在任务求解能力上的等价性和差异，特别是在不同故障数量下的表现。

Method: 通过双向模拟方法研究AMP_f和HO_f之间的等价关系，使用一个中间模型来捕捉"静默进程"的概念。通过构造性的证明展示两种模型之间的可模拟性，并扩展到针对非自适应对手的随机化协议。

Result: 对于n>2f，两种模型在无色任务求解上是等价的；对于彩色任务，仅当f=1且n>2时等价，当f更大时存在分离。这种分离源于HO_f中静默进程的存在可能导致不一致的决策。结果还扩展到随机化协议，表明规范轮次的表达能力限制是结构性的而非概率性的。

Conclusion: 该研究精确划定了基于轮次的抽象在何处能够捕捉异步计算，在何处不能。结果表明，对于无色任务，轮次抽象是充分的，但对于彩色任务，当故障数量增加时，轮次抽象的表达能力有限，这源于静默进程导致的决策不一致问题。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [37] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出了一种基于延迟约束的解耦架构，将集群资源分为延迟严格和延迟宽松池，通过瓶颈调度器和快速抢占机制，在保证在线服务SLO的同时提升离线任务吞吐量3倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在延迟敏感的在线服务和成本敏感的离线工作负载中部署，将两者共置在共享服务实例上可以提高资源利用率，但在Prefill/Decode解耦系统中直接应用会导致严重的负载不平衡，因为波动的请求组合会改变固有的P/D比例。现有动态调整技术无法跟上在线服务的突发流量模式。

Method: 1. 提出延迟约束解耦架构，根据任务延迟要求将集群资源分为延迟严格池和延迟宽松池；2. 基于瓶颈的调度器，采用Roofline性能模型指导性能瓶颈调度；3. 快速抢占机制，严格强制执行在线请求的服务级别目标。

Result: 在真实世界跟踪实验显示，相比现有离线系统方法，该方法在保持在线请求SLO的同时，将离线吞吐量提升高达3倍。

Conclusion: 通过延迟约束解耦架构和相应的调度机制，有效解决了Prefill/Decode解耦系统中的负载不平衡问题，在保证在线服务质量的同时显著提升了离线工作负载的资源利用率。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [38] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+是针对元数据缓存设计的新型缓存替换算法，通过在小FIFO队列中引入关联窗口来避免将关联引用误判为热点块，在元数据跟踪中比S3-FIFO降低高达28.5%的缺失率。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存存在固有的关联引用特性，即使对应的数据访问不包含关联引用。这些关联引用会降低缓存替换算法的效果，因为它们经常被错误地归类为热点块。

Method: Clock2Q+采用类似S3-FIFO的三队列结构，但在小FIFO队列中引入了关联窗口，处于该窗口中的块不设置引用位。这种简单增强使算法能更好地区分真正的热点块和关联引用。

Result: 在元数据跟踪中，Clock2Q+比表现第二好的算法S3-FIFO降低高达28.5%的缺失率。该算法在数据跟踪上也优于最先进的缓存替换算法。

Conclusion: Clock2Q+专为元数据缓存设计，具有低CPU开销、低内存开销、多CPU扩展性好、易于调优和实现等特性，已成功应用于VMware by Broadcom的vSAN和VDFS旗舰存储产品中。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [39] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 该研究比较了多语言复制数据系统中集成复制数据库(RDL)的两种策略：外部函数接口(FFI)和通用数据格式(CDF)，发现CDF在软件质量、延迟、内存消耗和吞吐量方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要在多个执行站点复制数据，业务需求和资源约束常导致不同副本站点使用不同编程语言。虽然复制数据库(RDL)提供数据读写访问和同步功能，但通常只支持单一语言或特定绑定，在多语言环境中集成现有RDL需要专门的代码，其软件质量和性能特性尚未被充分理解。

Method: 对多语言复制数据系统中集成RDL的两种关键策略进行实证研究：外部函数接口(FFI)和通用数据格式(CDF)，测量并比较它们的软件指标和性能，以了解它们对任务的适用性。

Result: 研究结果显示，采用CDF进行跨语言交互在软件质量、延迟、内存消耗和吞吐量方面具有优势。通过(1)创建基于CDF的RDL以混合编译型、解释型和托管语言；(2)通过插件可扩展性增强RDL，实现在单一语言中添加功能的同时保持多语言环境集成，进一步验证了这些发现。

Conclusion: 随着现代分布式系统使用多种编程语言，本研究为多语言复制数据系统中RDL的设计提供了新的见解，表明CDF策略在软件质量和性能方面优于传统的FFI方法。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [40] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT是一种针对LLM解码的前缀感知注意力核实现，通过pack-forward-merge范式减少共享前缀的重复内存访问，显著提升解码注意力效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务中解码注意力成为内存瓶颈，而实际工作负载中存在大量跨请求的层次化共享前缀（如系统提示、工具模板、RAG）。现有注意力实现无法充分利用前缀共享：单查询执行重复加载共享前缀KV缓存，固定大小的分片导致片上资源闲置和气泡，加剧内存带宽压力

Method: 提出PAT前缀感知注意力核，采用pack-forward-merge范式：1) 按共享前缀打包查询减少重复内存访问；2) 运行定制化多分片核实现高资源效率；3) 应用多流转发和KV分割减少资源气泡；4) 最终合并执行在线softmax

Result: 在真实和合成工作负载上的评估显示，PAT平均减少注意力延迟67.4%，在相同配置下TPOT减少13.6-83.4%，优于现有最先进注意力核

Conclusion: PAT通过有效利用请求间的共享前缀，显著优化了LLM解码中的内存访问模式，缓解了内存带宽压力，为实际工作负载提供了高效的注意力实现方案

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [41] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer是一个面向大语言模型服务的统一系统级加速框架，通过专家放置优化、缓存压缩和调度策略提升端到端服务效率，在10节点集群上实现616 QPM，TPOT降低36%，TTFT降低38%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现代AI应用中广泛应用，但在大规模服务系统中面临计算密集、延迟要求严格和吞吐瓶颈等挑战，需要系统级优化来提升服务效率。

Method: OmniInfer包含三个核心组件：OmniPlacement用于负载感知的专家混合调度，OmniAttn用于稀疏注意力加速，OmniProxy用于解耦感知的请求调度。框架基于vLLM构建，通过自适应资源解耦、高效稀疏性利用以及预填充和解码阶段的全局协调实现系统级优化。

Result: 在10节点Ascend 910C集群上对DeepSeek-R1进行评估，OmniInfer达到616 QPM（每分钟查询数），统一框架使TPOT（总处理时间）降低36%，OmniProxy的叠加使TTFT（首次令牌时间）进一步降低38%。

Conclusion: OmniInfer通过系统级优化有效解决了大语言模型服务中的计算密集和延迟瓶颈问题，显著提升了服务效率，并已开源供社区使用。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [42] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC：利用GPU的RT-core硬件加速光线追踪，显著提升网格蒙特卡洛模拟速度，简化工作流程


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛方法在GPU上仍有性能瓶颈，主要受限于频繁的光线-边界相交测试计算成本，需要更高效的加速方案

Method: 基于NVIDIA OptiX平台实现RT-MMC算法，利用现代GPU的RT-core硬件加速光线追踪能力，将图形光线追踪管道扩展到浑浊介质中的体积光线追踪，无需复杂的四面体网格生成

Result: RT-MMC与传统软件光线追踪MMC算法结果高度一致，在不同GPU架构上实现1.5倍到4.5倍的速度提升，显著增强了MMC的实用性

Conclusion: 从软件转向硬件光线追踪不仅大大简化了MMC模拟工作流程，还带来了显著的性能提升，随着光线追踪硬件的普及，这一优势将进一步增强，为生物光子学应用带来广泛益处

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [43] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe是一个针对LoRA适配器服务的动态放置和路由框架，解决了多租户环境中不同rank适配器导致的性能倾斜问题，显著提升了吞吐量并减少了GPU需求。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA服务系统在处理多租户环境中的异构适配器时，由于不考虑rank（大小）的差异性，导致严重的性能倾斜问题，需要增加更多GPU来满足服务级别目标，造成GPU资源利用率低下。

Method: 提出LoRAServe框架，采用工作负载感知的动态适配器放置和路由策略，通过动态重新平衡跨GPU的适配器分布，并利用GPU Direct RDMA进行远程访问，以应对实际工作负载的变化。

Result: 在真实生产环境测试中，LoRAServe相比现有最优系统，在满足SLO约束条件下，吞吐量提升高达2倍，TTFT降低高达9倍，同时减少高达50%的GPU使用量。

Conclusion: LoRAServe通过动态处理LoRA适配器的rank多样性，有效解决了多租户服务中的性能倾斜问题，显著提升了资源利用效率和服务性能。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [44] [AgentShield: Make MAS more secure and efficient](https://arxiv.org/abs/2511.22924)
*Kaixiang Wang,Zhaojiacheng Zhou,Bunyod Suvonov,Jiong Lou,Jie LI*

Main category: cs.MA

TL;DR: AgentShield是一个针对LLM多智能体系统的分布式防御框架，通过三层防御机制在保证安全的同时显著降低审计开销


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统防御方法存在单点故障或效率低下的问题，需要在鲁棒性和效率之间找到更好的平衡

Method: 提出AgentShield框架，包含三层防御：1)关键节点审计通过拓扑分析优先审计高影响力智能体；2)轻量令牌审计使用轻量级哨兵模型进行快速验证；3)两轮共识审计仅在不确定时触发重量级仲裁器确保全局一致

Result: 实验表明AgentShield达到92.5%的恢复率，相比现有方法减少70%以上的审计开销，在不同MAS拓扑和对抗场景下保持高协作精度

Conclusion: AgentShield通过分布式、分层审计机制有效解决了LLM多智能体系统的安全防御问题，在鲁棒性和效率之间取得了良好平衡

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) offer powerful cooperative reasoning but remain vulnerable to adversarial attacks, where compromised agents can undermine the system's overall performance. Existing defenses either depend on single trusted auditors, creating single points of failure, or sacrifice efficiency for robustness. To resolve this tension, we propose \textbf{AgentShield}, a distributed framework for efficient, decentralized auditing. AgentShield introduces a novel three-layer defense: \textbf{(i) Critical Node Auditing} prioritizes high-influence agents via topological analysis; \textbf{(ii) Light Token Auditing} implements a cascade protocol using lightweight sentry models for rapid discriminative verification; and \textbf{(iii) Two-Round Consensus Auditing} triggers heavyweight arbiters only upon uncertainty to ensure global agreement. This principled design optimizes the robustness-efficiency trade-off. Experiments demonstrate that AgentShield achieves a 92.5\% recovery rate and reduces auditing overhead by over 70\% compared to existing methods, maintaining high collaborative accuracy across diverse MAS topologies and adversarial scenarios.

</details>
