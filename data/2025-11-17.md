<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [Towards Assume-Guarantee Verification of Abilities in Stochastic Multi-Agent Systems](https://arxiv.org/abs/2511.10649)
*Wojciech Jamroga,Damian Kurpiewski,Łukasz Mikulski*

Main category: cs.MA

TL;DR: 本文提出了几种用于验证具有不完美信息的概率交替时序逻辑的假设-保证方案，证明了这些方案的正确性并讨论了其完备性。


<details>
  <summary>Details</summary>
Motivation: 战略能力模型检查是一个极其困难的问题，特别是在具有不完美信息的智能体在随机环境中行动的现实情况下。假设-保证推理可以在这里提供很大帮助，将复杂问题分解为一组更简单的子问题。

Method: 提出了几种用于概率交替时序逻辑与不完美信息的假设-保证验证方案，并证明这些方案的正确性。同时提出了一种新的非概率交替逻辑变体，其中战略模态捕获"最多达到φ"的概念。

Result: 证明了所提出假设-保证验证方案的正确性，并讨论了它们的完备性。

Conclusion: 假设-保证推理为验证具有不完美信息的战略能力提供了一种有效的分解方法，新的逻辑变体扩展了战略推理的表达能力。

Abstract: Model checking of strategic abilities is a notoriously hard problem, even more so in the realistic case of agents with imperfect information, acting in a stochastic environment. Assume-guarantee reasoning can be of great help here, providing a way to decompose the complex problem into a small set of easier subproblems.
  In this paper, we propose several schemes for assume-guarantee verification of probabilistic alternating-time temporal logic with imperfect information. We prove the soundness of the schemes, and discuss their completeness. On the way, we also propose a new variant of (non-probabilistic) alternating-time logic, where the strategic modalities capture "achieving at most $\varphi$," analogous to Levesque's logic of "only knowing."

</details>


### [2] [Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents](https://arxiv.org/abs/2511.10687)
*Chih-Hsuan Yang,Tanwi Mallick,Le Chen,Krishnan Raghavan,Azton Wells,Amal Gueroudji,Ian T. Foster,Rajeev Thakur*

Main category: cs.MA

TL;DR: 提出了一个理论框架，将合作博弈论归因与过程奖励建模相结合，将系统级评估转化为智能体信用和响应级信号，为LLM多智能体训练提供从全局评估到局部监督的统一路径。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中LLM的训练方法缺乏将系统级评估与智能体级和消息级学习连接起来的原理性方法，需要建立统一的训练信号框架。

Method: 结合合作博弈论归因（如Shapley值）和过程奖励建模，在成功案例中通过Shapley信用分配公平分配结果并细化为每消息奖励，在失败案例中通过首次错误定位产生修复感知偏好。

Result: 产生的信号具有局部性、有符号性和信用守恒特性，是有界的、合作性的，并且与基于强化学习或偏好的后训练直接兼容。

Conclusion: 该研究提出了一个理论基础和训练信号框架，为LLM多智能体训练提供了从全局评估到局部监督的可审计路径，但实证验证留待未来工作。

Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.

</details>


### [3] [Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting](https://arxiv.org/abs/2511.10949)
*Nirmit Arora,Sathvik Joel,Ishan Kavathekar,Palak,Rohan Gandhi,Yash Pandya,Tanuja Ganu,Aditya Kanade,Akshay Nambi*

Main category: cs.MA

TL;DR: SafeAgents是一个统一可扩展的多智能体系统安全评估框架，通过Dharma诊断指标识别多智能体管道中的薄弱环节，揭示了常见设计模式存在显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能体在多智能体系统中部署增多，其安全性变得至关重要。现有研究主要评估单智能体安全，缺乏对多智能体设计中引入漏洞的系统性理解。

Method: 提出SafeAgents框架，系统性地暴露计划构建策略、智能体间上下文共享和回退行为等设计选择对对抗性提示的敏感性影响，并引入Dharma诊断指标。

Result: 在四种数据集上对五种广泛采用的多智能体架构进行综合研究，发现常见设计模式存在显著漏洞，例如集中式系统将原子指令委托给子智能体会掩盖有害目标，降低鲁棒性。

Conclusion: 研究结果强调了在多智能体系统中进行安全感知设计的必要性。

Abstract: LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: 提出了FengHuang平台，一种解耦式AI基础设施，通过多级共享内存架构解决传统GPU架构在AI推理中的内存和通信扩展限制，实现显著的内存容量减少、GPU计算节省和更快的通信速度。


<details>
  <summary>Details</summary>
Motivation: 传统GPU中心架构在AI推理工作负载中面临内存容量、带宽和互连扩展的限制，需要新的基础设施设计来解决这些可扩展性挑战。

Method: 采用解耦式AI基础设施平台，结合高速本地内存和集中式解耦远程内存的多级共享内存架构，通过主动张量分页和张量操作的近内存计算来增强性能。

Result: 模拟显示FengHuang实现高达93%的本地内存容量减少、50%的GPU计算节省，以及比传统GPU扩展快16倍到70倍的GPU间通信速度。在GPT-3、Grok-1和QWEN3-235B等工作负载上，可在保持终端用户性能的同时减少高达50%的GPU使用。

Conclusion: FengHuang作为机架级AI基础设施扩展解决方案提供了最佳平衡，其开放、异构设计消除了供应商锁定，增强了供应链灵活性，能够显著降低基础设施和电力成本。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [5] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: HPCAgentTester是一个基于多智能体LLM的框架，用于自动生成HPC软件的单元测试，特别针对OpenMP和MPI并行编程模型，通过协作工作流生成针对并行执行构造和复杂通信模式的测试用例。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理HPC应用中的非确定性行为和同步问题，需要更有效的自动化测试解决方案来确保并行软件的可靠性。

Method: 采用多智能体LLM框架，包含专门的Recipe Agent和Test Agent，通过迭代式的批判循环协作生成和优化测试用例，针对并行执行构造和层次化并行性。

Result: HPCAgentTester能够为OpenMP和MPI原语生成可编译且功能正确的测试，有效识别传统技术经常遗漏的细微错误，相比独立LLM显著提高了测试编译率和正确性。

Conclusion: HPCAgentTester为并行软件系统提供了一种更强大和可扩展的可靠性保障解决方案，在HPC单元测试自动化方面具有显著优势。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 本文提出了人工智能伦理熵的第二定律，证明无约束AI会自发偏离目标，需要持续的对齐工作来维持稳定性。


<details>
  <summary>Details</summary>
Motivation: 为AI对齐问题提供定量理论基础，将AI稳定性问题重新定义为连续热力学控制问题。

Method: 定义伦理熵S = -Σ p(g_i; theta) ln p(g_i; theta)，证明其时间导数dS/dt >= 0，推导临界稳定性边界gamma_crit = (lambda_max / 2) ln N。

Result: 70亿参数模型从初始熵0.32漂移到1.69±1.08 nats，而使用gamma=20.4对齐工作的系统保持稳定在0.00±0.00 nats。

Conclusion: 该框架为高级自主系统的稳定性和安全性提供了定量基础，将AI对齐重新定义为连续热力学控制问题。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [7] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文研究了多目标决策中的Pareto剪枝问题，将其重新定义为多赢家投票问题，分析了现有质量度量方法的不足，提出了新的directed coverage度量方法，并研究了计算复杂性边界。


<details>
  <summary>Details</summary>
Motivation: 现实决策问题往往涉及多个目标优化，决策者需要从Pareto最优解集中选择代表性子集，现有质量度量方法存在反直觉行为，需要更合理的度量标准。

Method: 将Pareto剪枝问题重新定义为多赢家投票问题，进行公理化分析，提出新的directed coverage度量方法，分析计算复杂性边界，并进行实验评估。

Result: 发现现有质量度量方法存在反直觉行为，提出的directed coverage度量方法在各种设置下表现具有竞争力或更优，确定了计算复杂性的可处理与难处理边界。

Conclusion: 质量度量方法的选择对所选解集特性有决定性影响，提出的directed coverage度量方法是Pareto剪枝问题的有效替代方案。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [8] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 该论文研究了如何将抽象论证问题编码为(Q)SAT问题，并提出了保持团宽度的有向分解引导(DDG)归约方法。


<details>
  <summary>Details</summary>
Motivation: 研究团宽度这一图参数在编码中的应用，因为与树宽度不同，团宽度在稠密图中也可能较小，但目前对团宽度的编码能力了解甚少。抽象论证框架作为基于有向图的推理系统，是研究计算性质的理想候选。

Method: 设计了从论证问题到(Q)SAT的新归约方法，这些归约线性保持团宽度，形成了有向分解引导(DDG)归约。

Result: 为所有论证语义（包括计数）建立了新结果，证明DDG归约引起的开销在合理假设下无法显著改进。

Conclusion: 该研究开启了理解团宽度编码能力的新方向，为基于团宽度的算法和编码提供了理论基础。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [9] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 该论文提出了多智能体卧底游戏（MUG）协议，通过引入反事实测试来检测幻觉智能体，改进多智能体辩论范式，提升多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多智能体辩论中因幻觉问题导致的不可靠性，传统方法假设所有辩论者都是理性的，但实际中智能体本身可能存在幻觉。

Method: 基于社交推理游戏'谁是卧底'设计MUG协议，通过修改参考图像引入反事实证据，观察智能体是否能准确识别变化，从而检测幻觉智能体。

Result: MUG协议在三个关键维度上改进了多智能体辩论：实现基于反事实测试的事实验证、引入跨证据推理、促进主动推理。

Conclusion: MUG为LLMs的多模态推理提供了一个更可靠有效的框架，通过反事实测试和动态证据修改来检测和缓解幻觉问题。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [10] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文提出了两种新的反事实决策度量：潜在结果排序概率（PoR）和获得最佳潜在结果概率（PoB），用于在不确定性下进行反事实决策。


<details>
  <summary>Details</summary>
Motivation: 决策者在面对不确定性时，需要通过因果推理从多个备选行动中选择最优行动，传统方法基于期望结果排序，需要更精细的决策规则。

Method: 引入PoR和PoB两个新度量，建立识别定理和边界推导，提出估计方法，并通过数值实验验证估计器的有限样本性质。

Result: 建立了PoR和PoB的识别理论框架，推导了边界条件，开发了有效的估计方法，并在真实数据集上展示了应用效果。

Conclusion: PoR和PoB为反事实决策提供了新的有效工具，能够更精确地评估个体层面的潜在结果排序和最优结果实现概率。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [11] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 这篇论文从自适应性的角度重新审视大语言模型的推理能力，提出将推理努力根据输入特征（如难度和不确定性）进行分配的能力作为评估智能的核心标准。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注推理效率，但忽视了当前LLMs对所有任务采用统一推理策略的根本问题——对简单问题生成过长推理链，而对困难任务无法扩展推理。

Method: 论文通过三个贡献来形式化自适应推理：1）在LLM背景下形式化演绎、归纳和溯因推理；2）将自适应推理形式化为控制增强的策略优化问题；3）提出系统分类法，将现有方法分为基于训练和无需训练的方法。

Result: 建立了一个系统框架，将自适应推理方法组织为训练基础方法（强化学习、监督微调、学习控制器）和无需训练方法（提示条件化、反馈驱动停止、模块化组合），实现了不同策略的系统比较。

Conclusion: 识别了自我评估、元推理和人类对齐推理控制等开放挑战，为未来自适应推理研究提供了清晰的方向。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [12] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过注意力机制自适应结合双曲、复数和欧几里得空间，解决了现有方法在处理不同关系类型时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理大规模多样化关系类型时存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习注意力机制动态选择每个关系类型的最优几何空间，并使用多空间一致性损失确保跨空间预测的一致性。

Result: 在从1K到10M论文的知识图谱上评估，相比TransE、RotatE、DistMult等基线方法持续改进。在10M论文数据集上达到0.612 MRR，相对最佳基线提升4.8%，同时保持高效训练和85ms/三元组的推理速度。

Conclusion: HyperComplEx通过自适应维度分配实现接近线性的规模扩展，为可扩展知识图谱嵌入研究提供了有效解决方案。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [13] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建碰撞前场景并推断车辆行为，在复杂碰撞案例中实现了100%的准确率，超越了人类专家的92%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的交通事故重建依赖人类专家，在处理不完整多模态数据时往往产生不一致的结果，需要更精确和一致的自动化解决方案。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言碰撞重建；第二阶段结合这些重建与时序事件数据记录器数据进行深入碰撞推理。处理了277起追尾前车减速碰撞案例。

Result: 在39个复杂碰撞案例评估中，框架实现了100%准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，即使在处理不完整数据时也保持稳健性能。

Conclusion: 该研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征碰撞前行为方面提供了前所未有的精确度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [14] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型作为专家指导的新框架，从不规则采样的纵向患者数据中学习神经退行性疾病进展，特别是阿尔茨海默病的病理传播。


<details>
  <summary>Details</summary>
Motivation: 当前方法过度简化了大脑连接性关系，假设单一模态的大脑连接组作为疾病传播基质，导致病理传播预测不准确。同时，纯数据驱动的方法由于缺乏适当约束而面临可识别性问题。

Method: 利用大型语言模型作为区域变量相互作用的专家指导，通过LLM合成多模态关系并整合多种疾病驱动机制，同时优化长期疾病轨迹构建和生物约束的图结构学习。

Result: 在阿尔茨海默病队列的tau-PET成像数据上验证，新框架相比传统方法具有更高的预测准确性和可解释性，并揭示了超出传统连接性测量的额外疾病驱动因素。

Conclusion: 该框架通过结合LLM的专家知识和多模态关系整合能力，显著改善了神经退行性疾病进展建模的准确性和可解释性，为理解疾病传播机制提供了新视角。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [15] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出多智能体法律验证器，将合规检查分解为法规解释、业务背景评估和风险评估等专业智能体，通过结构化合成协议协调，在200个日本个人信息保护法修正案案例上达到72%准确率，比单智能体基线提高21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在严格隐私法规（如日本个人信息保护法APPI）下，AI驱动的数据传输规划中的法律合规性日益重要，需要可扩展且符合法规的自动化合规验证框架。

Method: 采用多智能体法律验证器，将合规检查分解为法规解释、业务背景评估和风险评估等专业智能体，通过结构化合成协议协调工作。

Result: 在200个APPI修正案第16条案例数据集上，系统达到72%准确率，比单智能体基线提高21个百分点，在明确合规案例上达到90%准确率（基线为16%），同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理能显著提高法律AI性能，为可信赖和可解释的自动化合规验证提供了可扩展且符合法规的框架，但在模糊场景中仍存在挑战。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [16] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 本文探讨了自主AI系统在遇到训练数据未覆盖的复杂场景时，如何构建、评估和证明候选行动方案，以满足人类期望和价值观。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中必然会遇到训练数据未覆盖的复杂场景，需要超越训练策略来构建和评估行动方案，以实现与人类期望和价值观一致的目标。

Method: 通过理论分析和实证案例研究，考察了智能体如何整合规范性、实用性和情境性理解来选择更符合人类期望的行动方案。

Result: 识别了智能体在这些情境下决策所需的知识类型，包括规范性知识、实用知识和情境知识，以确保决策既符合目标又与人类期望一致。

Conclusion: 在复杂现实环境中，智能体需要整合多种知识类型来选择和追求更符合人类期望的行动方案，这对自主AI系统的稳健决策至关重要。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [17] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示来处理不可区分对象产生的对称性，相比之前的方法速度更快。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，抽象结构需要转换为求解器支持的结构，但对称性破坏技术应用于抽象变量会产生大量复杂约束，实际性能较差。

Method: 开发了一种新的不完全对称性破坏方法，通过更好地利用抽象结构的表示来处理不可区分对象产生的对称性。

Result: 该方法比之前(Akgün et al. 2025)提出的方法速度更快。

Conclusion: 新方法在打破抽象结构对称性方面比现有方法更有效，特别是在处理不可区分对象对称性时表现更好。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [18] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 本文研究发现角色分配策略对多智能体辩论性能有显著影响，提出了"Truth Last"策略可提升推理任务性能22%，并开发了MADC策略来应对实际应用中未知真相的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在提升LLM推理能力方面显示出潜力，但角色分配策略这一关键方面研究不足，特别是在实际应用中真相未知的情况下如何优化辩论机制。

Method: 提出"Truth Last"角色分配策略，并开发多智能体辩论一致性策略，通过路径一致性评估独立角色间的一致性，模拟一致性得分最高的角色作为真相。

Result: 在9个LLM模型上的验证显示，MADC策略持续表现出先进性能，有效克服了MAD的性能瓶颈，在推理任务中提升性能达22%。

Conclusion: MADC为LLM智能体扩展提供了关键改进路径，通过系统模拟和优化多智能体辩论的核心机制，显著提升了推理任务的性能表现。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [19] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升自动驾驶的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统规划方法在需要学习策略、状态预测器和评估器时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评估器，利用其硬编码动态实现高精度状态预测，通过可微分特性有效搜索动作序列，使用梯度下降优化想象未来轨迹中的动作。

Result: 实验表明，DSS（规划梯度和随机搜索的组合）相比序列预测、模仿学习、无模型强化学习和其他规划方法，显著提高了跟踪和路径规划的准确性。

Conclusion: DSS框架通过结合规划梯度和随机搜索，在自动驾驶规划任务中表现出优越性能，为复杂交通场景下的安全导航提供了有效解决方案。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [20] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了一种新的广义规划方法，通过从训练问题中学习条件-动作规则来构建可重用的规划程序，这些规则可以直接执行或用于剪枝规划搜索空间。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在合成能够解决相关规划问题族的程序，现有方法在合成成本、规划覆盖率和解决方案质量方面存在改进空间。

Method: 对每个训练问题，按顺序计算每个目标原子的最优规划，对结果规划执行目标回归，并将输出提升为一阶条件→动作规则集合。

Result: 实验表明，该方法在经典和数值规划领域的合成成本、规划覆盖率和解决方案质量三个指标上显著优于现有最先进的广义规划器。

Conclusion: 该方法能够学习有效的广义规划和状态空间剪枝公理，为广义规划提供了一种简单而有效的新方法。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [21] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench是一个专门评估几何生成推理的基准测试，旨在解决现有评估方法无法衡量统一多模态模型在语言理解和精确视觉生成融合能力方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估判别性理解或无约束图像生成，无法衡量生成推理的整合认知过程。几何构造需要语言理解和精确视觉生成的融合，因此提供了理想的测试平台。

Method: 提出GGBench基准测试，通过几何构造任务来系统诊断模型的理解、推理和主动构建解决方案的能力。

Result: GGBench为下一代智能系统设定了更严格的标准，提供了一个全面评估几何生成推理能力的框架。

Conclusion: 几何构造是评估统一多模态模型生成推理能力的理想测试平台，GGBench填补了现有评估方法的空白，推动了人工智能从被动感知向主动跨模态生成的范式转变。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [22] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考机制提升LLMs的表格推理能力，采用两阶段难度感知强化学习和不确定性量化，显著提高推理稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs表格推理方法存在推理深度不足、缺乏迭代精炼以及推理过程不稳定的问题，限制了其在真实应用中的可靠性。

Method: 提出STaR框架，通过显式建模逐步思考和不确定性感知推理来赋予LLMs慢思考能力。训练阶段采用两阶段难度感知强化学习，从简单到复杂查询渐进学习；推理阶段通过整合token级置信度和答案一致性进行轨迹级不确定性量化。

Result: 在基准测试中，STaR实现了优越的性能和增强的推理稳定性。在领域外数据集上表现出强大的泛化能力。

Conclusion: STaR作为一个可靠且受认知启发的解决方案，在LLMs表格推理方面具有巨大潜力。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [23] [AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery](https://arxiv.org/abs/2511.11257)
*Yuqi Yin,Yibo Fu,Siyuan Wang,Peng Sun,Hongyu Wang,Xiaohui Wang,Lei Zheng,Zhiyong Li,Zhirong Liu,Jianji Wang,Zhaoxi Sun*

Main category: cs.AI

TL;DR: AIonopedia是首个基于大语言模型的离子液体发现智能体，通过多模态领域基础模型实现准确性质预测和分层搜索架构，在真实湿实验验证中表现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离子液体发现面临数据有限、模型精度不足和工作流程碎片化等关键挑战，需要开发更有效的AI驱动发现方法。

Method: 开发了LLM增强的多模态离子液体领域基础模型，采用分层搜索架构进行分子筛选和设计，并在新构建的综合数据集上进行训练和评估。

Result: 模型在文献报道系统评估中表现出有效的离子液体修饰能力，真实湿实验验证证实了其在具有挑战性的分布外任务上的优异泛化性能。

Conclusion: AIonopedia能够显著加速真实世界的离子液体发现过程，展示了LLM在材料科学领域的实际应用价值。

Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.

</details>


### [24] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI训练和推理中每个组件来确保决策可追溯性的工作流，扩展了DBOM概念并利用机密计算技术生成防篡改、可验证的AI决策痕迹。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程文档化方面存在不足，这阻碍了决策溯源和责任链重建，特别是在AI决策违反法律时无法提供法庭可接受的证据。

Method: 采用DBOM概念扩展，结合机密计算技术，构建支持生成防篡改、可验证和详尽AI决策痕迹的运行工作流。

Result: 开发了一个能够区分有毒和可食用蘑菇的应用程序作为高风险决策支持的示例，展示了工作流的实际运作。

Conclusion: 该工作流为AI决策提供了可靠的可追溯性支持，是解决AI系统责任链问题的一个实用且彻底的方案。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [25] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本文提出了对比ABox解释的概念，用于回答"为什么a是C的实例而b不是？"这类问题。相比单独解释正面蕴含或缺失蕴含的方法，对比解释同时考虑两者，能够聚焦于a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法通常分别处理正面蕴含（为什么C(a)被知识库蕴含）和缺失蕴含（为什么C(b)不被蕴含），但缺乏同时考虑两者的对比性解释方法，无法有效回答关于不同实例分类差异的根本原因。

Method: 针对描述逻辑本体的ABox推理特殊情况，开发了适当的对比解释概念；分析了在不同最优性标准下各种变体的计算复杂性，涵盖了轻量级和更表达性的描述逻辑；实现了一种计算对比解释变体的方法，并在现实知识库的生成问题上进行了评估。

Result: 提出了对比ABox解释的正式定义；对不同描述逻辑下的计算复杂性进行了理论分析；开发了可运行的实现方法并在实际场景中验证了有效性。

Conclusion: 对比ABox解释能够有效解释不同实例分类差异的原因，通过同时考虑正面和负面案例来揭示相关共性和差异，为知识库推理提供了更全面的解释能力。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [26] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign是一个推理时框架，将大型视觉语言模型的对齐问题重新定义为经济理性的搜索过程，通过前瞻性函数动态权衡安全性、实用性和成本，在降低计算成本的同时实现强大的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型的对齐方法在安全性、实用性和运营成本之间存在权衡困境，且仅关注最终输出的过程盲目性会浪费大量计算预算在有害推理上，导致有害推理可以通过良性论证来伪装。

Method: 将LVLM视为有限理性智能体，逐步扩展思维图，使用前瞻性函数（类似于净现值）对行动进行评分，动态权衡预期安全性、实用性和成本与剩余预算的关系，并通过最弱环节原则强制执行路径安全性。

Result: 在3个闭源和2个开源模型上的6个数据集上的广泛实验表明，EcoAlign以较低的计算成本达到或超越了最先进的安全性和实用性水平。

Conclusion: EcoAlign为强大的LVLM对齐提供了一个原则性、经济性的路径，解决了当前对齐方法在经济效率方面的根本问题。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [27] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个结合强化学习和基于规则的社会运动模型的混合框架，用于实现社会感知导航，通过量化人类舒适度来优化机械能量和社会舒适度的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决在人群环境中导航时避免引起不适的问题，结合基于规则方法的可解释性和数据驱动方法的灵活性。

Method: 提出RLSLM混合强化学习框架，将基于经验行为实验的社会运动模型集成到强化学习的奖励函数中，生成方向敏感的社会舒适度场。

Result: 通过沉浸式VR实验证明RLSLM在用户体验上优于最先进的基于规则模型，消融和敏感性分析显示其可解释性显著优于传统数据驱动方法。

Conclusion: 这项工作提出了一种可扩展的、以人为中心的方法，有效整合认知科学和机器学习，用于现实世界的社会导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [28] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 该论文系统综述了多智能体强化学习在现实通信约束下的鲁棒高效通信策略，包括消息扰动、传输延迟和带宽限制等挑战，并聚焦于协同自动驾驶、分布式SLAM和联邦学习三个应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足，因此需要研究在现实通信约束下的鲁棒高效通信策略。

Method: 通过系统综述方法，分析近期在MARL中应对消息扰动、传输延迟和带宽限制等现实通信约束的通信策略进展。

Result: 识别了低延迟可靠性、带宽密集型数据共享和通信隐私权衡等核心挑战，并提出了通信、学习和鲁棒性协同设计的统一方法。

Conclusion: 需要采用通信、学习和鲁棒性协同设计的统一方法来弥合理论MARL模型与实际实现之间的差距，并指出了关键开放挑战和未来研究方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [29] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，通过整合非结构化临床笔记、实验室测试和患者时间序列数据，利用LLM处理临床文本和文本化实验室测试，以及transformer编码器处理纵向序列就诊数据，用于慢性疾病预测。


<details>
  <summary>Details</summary>
Motivation: 当前大多数预测模型未能充分利用EHR中多模态数据之间的交互、冗余和时间模式，通常只关注单一数据类型或忽视这些复杂性。

Method: 使用大型语言模型处理临床文本和文本化实验室测试，结合transformer编码器处理纵向序列就诊数据，整合多模态EHR数据。

Result: 在MIMIC-III和FEMH数据集上，CURENet在多标签框架中预测前10种慢性疾病时准确率超过94%。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者预后。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [30] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个经验引导的推理系统，能够在推理时动态生成完整的计算策略（包括LLM调用、工具、采样参数和控制逻辑），通过元策略实现所有策略组件的自适应调整，在多个挑战性基准测试中显著提升性能并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在训练后难以自适应调整问题解决方法，要么只能修改文本输入而无法改变采样参数、工具配置等核心组件，要么需要离线优化且部署后无法动态调整。

Method: 使用基于LLM的元策略生成完整策略，包含两个组件：Guide基于当前问题和结构化经验记忆生成候选策略，Consolidator整合执行反馈来改进未来策略生成。

Result: 在五个挑战性基准测试（AIME 2025、3-SAT和三个Big Bench Extra Hard任务）中，EGuR相比最强基线准确率提升高达14%，计算成本降低高达111倍，且随着经验积累性能持续提升。

Conclusion: EGuR通过动态生成完整策略实现了AI系统的实时自适应，在保持高性能的同时大幅降低计算开销，为构建更智能、高效的AI系统提供了新途径。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>
