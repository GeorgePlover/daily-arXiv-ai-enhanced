<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 39]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io是一个为分布式数据管道设计的解决方案，支持正确的回滚恢复和细粒度数据血缘捕获，适用于无服务器可扩展架构，性能评估显示在某些场景下优于ABS协议。


<details>
  <summary>Details</summary>
Motivation: 分布式数据管道需要可靠的故障恢复机制和细粒度的数据血缘跟踪，特别是在无服务器架构中，现有解决方案如ABS协议在某些场景下存在局限性。

Method: LOG.io采用基于日志的回滚恢复协议，支持通用编程模型，包括非确定性算子、外部系统交互和自定义代码，采用非阻塞设计，允许失败算子独立恢复而不中断其他算子，支持算子动态扩展。

Result: 在SAP Data Intelligence系统中的实验表明：当管道中存在慢算子且事件吞吐量适中时，LOG.io在正常处理时与ABS相当，在恢复时优于ABS；其他情况下ABS表现更好，但数据并行化可显著减少LOG.io的开销，而ABS无改善；数据血缘捕获开销在所有实验中均小于1.5%。

Conclusion: LOG.io为分布式数据管道提供了有效的回滚恢复和细粒度数据血缘捕获解决方案，在特定场景下优于现有方法，且数据血缘捕获开销可忽略不计。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [2] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: MMA通过多路径内存访问技术，解决PCIe带宽成为LLM性能瓶颈的问题，显著提升GPU与主机内存间的数据传输带宽，改善LLM服务的首令牌时间和模型切换延迟。


<details>
  <summary>Details</summary>
Motivation: PCIe带宽已成为大语言模型性能的关键瓶颈，特别是在前缀缓存获取和模型切换等场景。虽然理论上GPU与主机内存之间可以进行多路径数据传输，但PCIe和NVLink等异构协议限制了带宽只能达到单个PCIe链路的水平，导致服务器内部带宽未充分利用。

Method: 提出多路径内存访问（MMA）方案，首次实现GPU与主机内存之间的高效多路径数据传输。MMA通过动态库注入支持无缝部署，使LLM应用无需修改代码即可受益于该技术。

Result: 在测试平台上，MMA显著提升了GPU与内存间的数据传输带宽，峰值带宽达到245 GB/s，相比原生单路径带宽实现了4.62倍的加速。端到端评估显示，MMA将LLM服务的首令牌时间（TTFT）减少了1.14-2.38倍，并将vLLM睡眠模式下的模型切换延迟降低了1.12-2.48倍。

Conclusion: MMA有效解决了PCIe带宽瓶颈问题，通过多路径数据传输技术充分利用了服务器内部带宽，显著提升了LLM服务的性能表现，特别是在首令牌时间和模型切换延迟方面取得了实质性改进。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [3] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 该论文针对无服务器计算中的冷启动延迟问题，从开发者视角进行研究，提出了冷启动初始化反模式分类、SCABENCH基准测试和INITSCOPE分析框架，显著提升了诊断效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算简化了部署和扩展，但冷启动延迟仍然是主要的性能瓶颈。现有研究将缓解措施视为黑盒优化，而本文从开发者可见的设计问题角度研究冷启动，旨在提供更有效的诊断和优化方法。

Method: 1. 分析81个开源无服务器系统的issue报告，推导出初始化反模式、修复策略和诊断挑战的分类体系；2. 开发SCABENCH可复现基准测试；3. 设计INITSCOPE轻量级分析框架，将加载的代码与执行的代码关联起来。

Result: 在SCABENCH上，INITSCOPE相比现有工具将定位准确率提升了40%，诊断工作量减少了64%。开发者研究显示任务准确率更高、诊断速度更快。研究工具已公开可用。

Conclusion: 该研究为无服务器设计中的冷启动缓解提供了基于证据、性能感知的实践方法，通过开发者视角的系统性分析，显著改善了冷启动问题的诊断和优化能力。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [4] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 论文提出了一种针对NVIDIA MIG技术的在线调度框架，通过条件负载均衡、动态分区和作业迁移来解决资源争用和GPU碎片化问题，显著提升系统效率。


<details>
  <summary>Details</summary>
Motivation: 现代GPU工作负载需要高效资源共享，但NVIDIA的MIG技术虽然提供硬件级GPU分区，仍面临PCIe带宽等共享组件的资源争用问题，以及由于MIG配置有限导致的GPU碎片化问题，这些碎片化不仅来自空间不连续性，还来自严格的配置文件放置约束。

Method: 提出了一个在线调度框架，整合了条件负载均衡、动态分区和作业迁移技术。该方法动态调整作业放置以最小化争用，并重新组织GPU分配以应对内部和外部碎片化。

Result: 实验结果显示该方法显著提高了系统效率。当所有技术都应用时，makespan（完成所有作业所需时间）最多提升了35%。

Conclusion: 该研究提出的在线调度框架有效解决了MIG技术中的资源争用和碎片化问题，通过动态资源管理和作业迁移显著提升了GPU资源共享的系统效率。

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [5] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 提出Staggered Batch Scheduling (SBS)机制，通过缓冲请求形成最优执行批次，解决DP+EP架构中的调度问题，显著降低TTFT并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务向复杂的分布式架构（特别是P/D分离、大规模DP+EP范式）演进，带来了独特的调度挑战。DP+EP架构具有高昂的内部同步成本，传统的即时请求调度会导致严重的引擎内部排队和并行化气泡，降低首令牌时间(TTFT)。

Method: 提出Staggered Batch Scheduling (SBS)机制，通过故意缓冲请求来形成最优执行批次，实现时间解耦以消除内部排队气泡。同时利用缓冲创建的调度窗口，引入负载感知全局分配策略，平衡DP单元在Prefill和Decode阶段的计算负载。

Result: 在部署Deepseek-V3的生产H800集群上，相比最先进的即时调度基线，系统将TTFT降低了30%-40%，吞吐量提高了15%-20%。

Conclusion: Staggered Batch Scheduling机制有效解决了DP+EP架构中的调度问题，通过优化请求批处理和负载分配，显著提升了LLM服务的响应时间和吞吐性能。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [6] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus提出了一种在分解内存架构上具有锁分解的可扩展分布式事务系统，通过将锁从数据中分离并在计算节点上执行，解决了内存节点RDMA网卡的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有分解内存上的分布式事务系统中，内存节点的RDMA网卡成为主要性能瓶颈，这源于大量用于锁的单边原子操作，限制了系统的可扩展性。

Method: Lotus采用锁分解设计，将锁从数据中分离并在计算节点上执行；使用应用感知的锁管理机制基于OLTP工作负载局部性进行锁分片；引入锁优先事务协议分离锁定阶段；采用无锁重建的恢复机制处理计算节点故障。

Result: 实验结果显示，与现有最先进的分解内存事务系统相比，Lotus将事务吞吐量提高了最多2.1倍，延迟降低了最多49.4%。

Conclusion: Lotus通过锁分解有效解决了分解内存系统中RDMA网卡的瓶颈问题，实现了更好的可扩展性和性能，为分解内存上的高效事务处理提供了新方案。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [7] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 本文描述了一个专为科学工作负载中人工智能设计的联邦计算平台，提供可重复部署、透明访问分布式电子基础设施，覆盖完整机器学习生命周期。


<details>
  <summary>Details</summary>
Motivation: 为科学工作负载中的人工智能应用提供一个统一的联邦计算平台，解决分布式基础设施访问不一致、机器学习生命周期管理碎片化、以及模型可重复性和可追溯性不足的问题。

Method: 通过可重复部署技术构建联邦计算平台，提供全面的服务目录，包括交互式开发环境、GPU训练资源、注释工具、实验跟踪、联邦学习支持，以及覆盖云连续体的多种部署选项。

Result: 平台实现了对物理分布式电子基础设施的一致透明访问，提供完整的机器学习生命周期管理，集成多种AI模型提供商、数据集和存储资源，并具备可定制性以降低外部社区采用门槛。

Conclusion: 该联邦计算平台成功为科学AI工作负载提供了集成解决方案，通过可重复部署、全面服务目录和生态系统集成，有效支持了机器学习从开发到部署的完整生命周期。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [8] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 提出了一种CPU-GPU协同推理框架，通过GPU专家缓存机制减少数据传输需求，利用CPU多线程优化处理缓存未命中，在消费级硬件上提升MoE模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型通过选择性激活参数子集降低了计算需求，但最先进的MoE模型仍需要超出典型消费级GPU容量的内存。传统的CPU-GPU权重传输方法会引入延迟，限制了推理性能。

Method: 设计了一个新颖的CPU-GPU协同推理框架，包含GPU上的专家缓存机制以减少数据传输需求，通过缓存命中实现更快推理。将计算卸载到CPU以高效处理缓存未命中，并利用CPU多线程优化。

Result: 评估显示该框架带来了性能改进，突显了CPU-GPU协作在消费级系统单请求推理场景中最大化硬件利用率的潜力。

Conclusion: 该CPU-GPU协同推理框架通过专家缓存机制和CPU卸载优化，有效解决了MoE模型在消费级硬件上的部署挑战，实现了更好的推理性能。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [9] [Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services](https://arxiv.org/abs/2512.16167)
*Shiduo Yang,Jiye Wang,Jiayu Qin,Jianbin Li,Yu Wang,Yuanhe Zhao,Kenan Guo*

Main category: cs.MA

TL;DR: Ev-Trust是一个基于演化博弈论的战略均衡信任机制，用于解决LLM驱动的多智能体系统中的信任问题，通过动态反馈结构引导智能体行为演化，有效减少恶意策略并提升集体收益。


<details>
  <summary>Details</summary>
Motivation: 随着Web向以智能体为中心的范式快速演进，LLM驱动的多智能体系统虽然能实现复杂去中心化环境中的推理、规划和交互，但其开放性和异构性也放大了欺骗、欺诈和虚假信息的风险，对信任建立和系统鲁棒性构成严峻挑战。

Method: 提出Ev-Trust策略均衡信任机制，基于演化博弈论，将直接信任、间接信任和预期收益整合到动态反馈结构中，引导智能体行为向均衡演化。在去中心化的"请求-响应-支付-评估"服务框架内，使智能体能够自适应调整策略，自然排除恶意参与者同时强化高质量协作。

Result: 基于复制动态方程的理论推导证明了局部演化均衡的存在性和稳定性。实验结果表明，该方法能有效反映LLM驱动的开放服务交互场景中智能体的可信度，减少恶意策略，并增加集体收益。

Conclusion: Ev-Trust为群体演化博弈场景中的智能体服务Web信任建模提供了新视角，能够有效解决LLM多智能体系统中的信任挑战，促进系统稳健性和协作质量。

Abstract: The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized "Request-Response-Payment-Evaluation" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments](https://arxiv.org/abs/2512.15736)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: Anubuddhi是一个多智能体AI系统，能够从自然语言提示设计和模拟量子光学实验，无需专业编程知识。系统通过语义检索从三层工具箱组合光学布局，并通过物理模拟验证设计。


<details>
  <summary>Details</summary>
Motivation: 量子光学实验设计通常需要专业编程知识和物理专业知识，这限制了研究人员和教育工作者。Anubuddhi旨在民主化计算实验设计，使非专家也能参与量子光学研究和教学。

Method: 系统采用多智能体架构，结合意图路由、知识增强生成和双模式验证（QuTiP和FreeSim）。通过语义检索从三层工具箱中选择组件，然后通过物理模拟进行验证和收敛细化。

Result: 评估了13个实验，涵盖基础光学、量子信息协议和先进技术。系统获得8-9/10的设计-模拟对齐分数，模拟能忠实建模预期物理。发现结构正确性与定量准确性不同：高对齐确认正确的物理架构，但数值预测需要专家审查。

Conclusion: Anubuddhi成功民主化了量子光学实验设计，为研究和教学提供强大的初始设计，用户可以通过对话迭代细化。自由形式模拟在11/13实验中优于约束框架，表明量子光学多样性需要灵活的数学表示。

Abstract: We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.

</details>


### [11] [The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems](https://arxiv.org/abs/2512.15740)
*Timothy Prescher*

Main category: cs.AI

TL;DR: 本文提出比例责任原则（PPD），将道德责任建模为随智能体认知状态变化的函数，揭示不确定性不会消除道德责任，而是将行动责任转化为修复责任。


<details>
  <summary>Details</summary>
Motivation: 传统伦理框架在处理不确定性下的决策时存在局限，通常将其简单视为行动约束。本文旨在建立一个能够建模道德责任如何随智能体认知状态变化的框架，为可审计AI决策系统提供理论基础。

Method: 提出比例责任原则（PPD）框架，建立数学模型 D_total = K[(1-HI) + HI * g(C_signal)]，其中总责任是知识、谦逊/不确定性和情境信号强度的函数。通过蒙特卡洛模拟验证系统行为，并在临床伦理、受赠人权利法、经济治理和人工智能四个领域进行应用验证。

Result: 模拟显示，保持基线谦逊系数（λ > 0）的系统能产生更稳定的责任分配，降低过度自信决策的风险。比例责任原则作为复杂系统中的稳定原则，能够防止过度干预和疏忽，动态平衡认知信心与情境风险。

Conclusion: 通过将谦逊形式化为系统参数，PPD提供了一种数学上可处理的道德责任方法，可为可审计AI决策系统的开发提供参考。该框架具有跨学科有效性，能够动态调整责任分配以适应不确定性变化。

Abstract: Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).
  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.
  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.

</details>


### [12] [Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions](https://arxiv.org/abs/2512.15743)
*David Noever*

Main category: cs.AI

TL;DR: 提出一个从自然语言描述生成物理可实现的装配指令的框架，使用离散零件词汇表确保几何有效性、连接约束和可构建顺序，通过LDraw中间表示和大型语言模型生成有效装配序列


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于像素的扩散方法或CAD模型）无法支持复杂装配指令或组件交换，需要弥合语义设计意图与可制造输出之间的差距

Method: 使用LDraw作为文本丰富的中间表示，在离散零件词汇表内操作，通过工具引导大型语言模型生成有效的逐步构建序列和装配指令，提出Python库进行程序化模型生成

Result: 能够为超过3000个装配零件的砖基原型生成有效装配指令，在复杂卫星、飞机和建筑领域评估可构建输出，展示了可扩展性、模块化和保真度

Conclusion: 该方法作为物理API，通过约束词汇表将精确定向的砖位置与"词袋"连接，使任意功能需求编译为物质现实，为制造和工程原型中的自然语言实现开辟新设计选项

Abstract: We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel "bag of bricks" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a "bag of words" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.

</details>


### [13] [AI Epidemiology: achieving explainable AI through expert oversight patterns](https://arxiv.org/abs/2512.15783)
*Kit Tempest-Walters*

Main category: cs.AI

TL;DR: AI流行病学是一个通过应用群体层面监测方法来治理和解释先进AI系统的框架，类似于流行病学在理解分子机制前通过统计证据支持公共卫生干预的方式。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释性方法（如SHAP和机制可解释性）在处理大规模部署模型时面临模型复杂性问题，需要一种绕过模型内部复杂性、基于输出的治理框架。

Method: 通过标准化捕获AI-专家交互到结构化评估字段（风险等级、对齐分数、准确度分数），这些作为暴露变量通过统计关联预测输出失败，类似于胆固醇和血压预测心脏事件。输出失败关联随后通过专家覆盖和真实世界结果进行验证。

Result: 该框架为零负担专家提供自动审计跟踪，被动追踪专家与AI建议的趋同和分歧。通过分析输出而非内部模型计算，在机构更新模型和切换供应商时提供治理连续性。提供可靠性分数和语义评估，使专家和机构能够在AI输出造成危害前检测不可靠输出。

Conclusion: AI流行病学通过使领域专家无需机器学习专业知识即可治理AI系统，实现了AI监督的民主化，为复杂AI系统提供了一种实用、可扩展的治理框架。

Abstract: AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.
  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.
  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.

</details>


### [14] [Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM](https://arxiv.org/abs/2512.15784)
*Zibin Liu,Cheng Zhang,Xi Zhao,Yunfei Feng,Bingyu Bai,Dahu Feng,Erhu Feng,Yubin Xia,Haibo Chen*

Main category: cs.AI

TL;DR: MOBIMEM是一个基于内存的LLM智能体系统，通过三种专用内存原语实现无需模型重训练的自进化，在移动设备上显著提升任务成功率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的智能体架构在部署后难以自我进化，需要持续模型重训练/微调，计算开销大且存在准确性与推理效率的权衡问题。

Method: 提出MOBIMEM内存中心智能体系统：1) 三种专用内存原语：Profile Memory使用轻量级距离图结构对齐用户偏好；Experience Memory使用多级模板实例化新任务执行逻辑；Action Memory记录细粒度交互序列减少模型推理依赖。2) 集成操作系统风格服务：调度器协调并行子任务和内存操作；AgentRR机制实现安全高效动作重用；上下文感知异常处理确保优雅恢复。

Result: 在AndroidWorld和top-50应用上评估：达到83.1%的用户偏好对齐，检索时间23.83毫秒（比GraphRAG基线快280倍）；任务成功率提升高达50.3%；移动设备端到端延迟降低高达9倍。

Conclusion: MOBIMEM通过内存中心架构实现了无需模型重训练的智能体自进化，在移动环境中显著提升了用户偏好对齐、任务成功率和执行效率。

Abstract: Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.
  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.
  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.

</details>


### [15] [State-Augmented Graphs for Circular Economy Triage](https://arxiv.org/abs/2512.15824)
*Richard Fox,Rui Li,Gustav Jonsson,Farzaneh Goli,Miying Yang,Emel Aktas,Yongjing Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态增强的拆卸序列规划图的新型决策框架，用于优化循环经济产品分拣决策，通过编码拆卸历史到状态中实现马尔可夫性质，支持递归评估和最优决策。


<details>
  <summary>Details</summary>
Motivation: 循环经济产品分拣需要评估产品在达到当前使用终点后的可持续处理路径，这需要平衡保留价值与处理成本、劳动力约束的自适应决策。现有方法缺乏统一的框架来处理复杂的产品结构和操作约束。

Method: 提出一个基于状态增强的拆卸序列规划图的确定性求解器，通过将拆卸历史编码到状态中强制实现马尔可夫性质，使每个决策仅依赖于前一个状态。决策包括继续拆卸或选择循环经济选项，整合基于诊断健康分数的条件感知效用和复杂操作约束。

Result: 通过电动汽车电池的分层分拣案例展示了该框架的灵活性，决策由组件的递归估值驱动。该统一形式能够适应不同的机械复杂性、安全要求和经济驱动因素。

Conclusion: 该统一形式为优化不同产品和操作环境下的循环经济分拣决策提供了可处理且可推广的基础，支持自适应决策和复杂约束的整合。

Abstract: Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.

</details>


### [16] [PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations](https://arxiv.org/abs/2512.15894)
*Vahideh Zolfaghari*

Main category: cs.AI

TL;DR: 研究创建了儿科焦虑基准测试PediatricAnxietyBench，评估大语言模型在真实世界对抗性压力下的安全性，发现模型在面对焦虑家长紧急查询时存在安全隐患，70B模型表现优于8B模型但仍有漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着家长越来越多地使用大语言模型获取儿科指导，但模型在真实世界对抗性压力下的安全性了解不足。焦虑家长常使用紧急语言可能破坏模型安全防护，导致有害建议，需要评估模型在这种情况下的表现。

Method: 创建了包含300个高质量查询的开放基准测试PediatricAnxietyBench，涵盖10个儿科主题（150个患者来源，150个对抗性查询）。评估了两个Llama模型（70B和8B），采用多维安全框架，包括诊断克制、转诊依从性、模糊表达和紧急情况识别。对抗性查询融入了家长压力模式，如紧迫性、经济障碍和对免责声明的挑战。

Result: 平均安全得分为5.50/15（SD=2.41）。70B模型优于8B模型（6.26 vs 4.95，p<0.001），关键失败率更低（4.8% vs 12.0%，p=0.02）。对抗性查询使安全性降低8%（p=0.03），紧迫性导致最大降幅（-1.40）。在癫痫发作（33.3%不当诊断）和疫苗接种后查询中存在漏洞。模糊表达与安全性强相关（r=0.68，p<0.001），而紧急情况识别完全缺失。

Conclusion: 模型规模影响安全性，但所有模型都显示出对真实家长压力的脆弱性。PediatricAnxietyBench提供了一个可重复使用的对抗性评估框架，能够揭示标准基准测试忽略的临床重要失败模式。

Abstract: Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.

</details>


### [17] [Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries](https://arxiv.org/abs/2512.15906)
*Jonathan A. Handler*

Main category: cs.AI

TL;DR: Darth Vecdor (DV) 是一个从大型语言模型中提取知识到结构化SQL数据库的工具，旨在解决直接查询LLM时的成本、速度、安全性和置信度问题，特别适用于医疗领域。


<details>
  <summary>Details</summary>
Motivation: 虽然可以直接查询大型语言模型获取知识，但在高容量操作中，成本、速度、安全性和置信度等问题可能成为障碍。通过将信息从LLM中预先提取到标准数据库中，可以缓解这些问题，特别是在医疗等关键领域。

Method: DV通过构建具有特定功能的系统来缓解LLM响应中的错误、离题、自由文本、过于笼统和不一致等问题，并支持多元素响应。系统提供基于浏览器的图形用户界面，便于领域专家进行提示工程。

Result: DV已作为免费、开源、可扩展的软件发布，采用"按现状"提供的方式，不提供任何明示或暗示的保证。用户需要自行评估使用风险和收益，并确保使用安全有效。

Conclusion: 尽管DV可能存在严重缺陷，但作者希望当前和未来版本的DV及其输出能够通过适当使用来帮助改善医疗保健，特别是在解决LLM直接查询的限制方面。

Abstract: Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database ("knowledge base" or "knowledge graph"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an "as is" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.

</details>


### [18] [Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems](https://arxiv.org/abs/2512.15922)
*Jovan Pavlović,Miklós Krész,László Hajdu*

Main category: cs.AI

TL;DR: 提出基于扩散激活算法的RAG框架，通过自动构建的知识图谱增强大语言模型在复杂推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在复杂多步推理任务中存在局限性，将检索信息视为同等可靠，忽视可信度和关联性差异。GraphRAG方法虽能改善，但依赖高质量知识图谱构建，成本高且不可靠。

Method: 提出新型RAG框架，采用扩散激活算法从自动构建的知识图谱文档语料库中检索信息，增强大语言模型在复杂任务（如多跳问答）中的性能。

Result: 实验显示该方法达到或优于迭代RAG方法，可作为即插即用模块与多种RAG方法集成。与思维链迭代检索结合相比朴素RAG，答案正确率提升高达39%，且能在资源受限环境下使用小型开源模型取得效果。

Conclusion: 提出的扩散激活算法RAG框架能有效解决复杂推理任务中的信息检索问题，具有良好性能、易集成性和资源效率优势。

Abstract: Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.

</details>


### [19] [Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning](https://arxiv.org/abs/2512.15943)
*Polaris Jhandi,Owais Kazi,Shreyas Subramanian,Neel Sendas*

Main category: cs.AI

TL;DR: 该研究探索用小语言模型替代大语言模型以降低企业AI应用成本，通过微调OPT-350M模型在特定任务上取得优于ChatGPT等基线模型的性能


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在企业中的规模化应用，大语言模型的高计算成本成为可持续性和可访问性的关键障碍，这促使研究更经济的小语言模型替代方案

Method: 使用Hugging Face TRL的监督微调方法，对Meta AI的OPT-350M模型进行单轮微调，针对文档摘要、查询回答和结构化数据解释等代表性任务进行领域适配训练

Result: 微调后的小语言模型在ToolBench评估中取得了77.55%的通过率，显著优于ChatGPT-CoT（26.00%）、ToolLLaMA-DFS（30.18%）和ToolLLaMA-CoT（16.27%）等基线模型

Conclusion: 精心设计和针对性训练的小语言模型能够显著降低生成式AI的采用门槛，实现成本效益高的大规模生产系统集成，为企业在特定应用场景中提供可行的LLM替代方案

Abstract: As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\%), ToolLLaMA-DFS (30.18\%), and ToolLLaMA-CoT (16.27\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.

</details>


### [20] [Subjective functions](https://arxiv.org/abs/2512.15948)
*Samuel J. Gershman*

Main category: cs.AI

TL;DR: 论文探讨了目标函数的来源问题，提出主观函数概念作为内生于智能体的高阶目标函数，并以预期预测误差为例进行了具体分析。


<details>
  <summary>Details</summary>
Motivation: 人类智能能够动态合成新的目标函数，而现有AI系统缺乏这种能力。论文旨在探索目标函数的来源、选择机制，以及如何赋予人工系统类似的能力。

Method: 提出主观函数的概念——一种内生于智能体的高阶目标函数，以预期预测误差作为具体实例进行研究，并与心理学、神经科学和机器学习中的相关概念建立联系。

Result: 建立了主观函数作为内生于智能体的目标函数生成框架，展示了预期预测误差作为主观函数的具体实现，为理解目标函数来源提供了新的理论视角。

Conclusion: 主观函数概念为解决目标函数来源问题提供了有前景的框架，将内生于智能体的目标生成机制形式化，为开发能够自主合成目标的人工智能系统奠定了基础。

Abstract: Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.

</details>


### [21] [Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting](https://arxiv.org/abs/2512.16022)
*Defu Cao,Michael Gee,Jinbo Liu,Hengxuan Wang,Wei Yang,Rui Wang,Yan Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种新方法，将大型语言模型重新定位为智能法官，用于评估、解释和战略协调时间序列基础模型的集成，通过R1风格微调和SHAP引导训练，在GIFT-Eval基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型激增但缺乏一致性优势，需要可解释的集成方法；大型语言模型具有强大推理能力但直接应用于时间序列预测效果不佳。

Method: 将LLM重新定位为智能法官，通过SHAP引导的R1风格微调过程，教会模型将集成权重解释为关于时间动态的有意义的因果陈述，然后通过多轮对话进行前瞻性评估和自适应优化。

Result: 在GIFT-Eval基准测试的23个数据集、97个设置中，该方法在CRPS和MASE指标上显著优于领先的时间序列基础模型，建立了新的最先进结果。

Conclusion: 通过将LLM重新定位为智能法官来协调时间序列基础模型集成，结合SHAP引导的微调，能够实现高性能且可解释的时间序列预测，解决了现有方法的局限性。

Abstract: The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.

</details>


### [22] [Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets](https://arxiv.org/abs/2512.16030)
*Lukas Nel*

Main category: cs.AI

TL;DR: 论文引入KalshiBench基准测试，评估大语言模型对未来事件的校准能力，发现所有前沿模型都存在系统性过度自信问题，即使推理增强模型校准更差，表明模型缩放和推理增强不会自动改善校准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各种任务上表现出色，但其认知校准能力仍未被充分理解。现有基准主要评估静态知识准确性，缺乏对模型关于未知未来事件不确定性量化能力的评估。

Method: 引入KalshiBench基准，包含300个来自Kalshi交易所的预测市场问题，这些问题在模型训练截止后有可验证的真实结果。评估了Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B和Kimi-K2五个前沿模型，使用预期校准误差（ECE）和Brier技能评分等指标。

Result: 所有模型都表现出系统性过度自信。最佳校准模型Claude Opus 4.5的ECE为0.120，而推理增强模型GPT-5.2-XHigh校准更差（ECE=0.395）。只有一个模型获得正Brier技能评分，表明大多数模型表现不如简单预测基准率。

Conclusion: 模型缩放和推理增强不会自动带来校准改进，认知校准是一种需要针对性开发的独立能力。当前前沿大语言模型在量化未来事件不确定性方面存在显著缺陷。

Abstract: A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\% confidence, it should be correct 80\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.

</details>


### [23] [Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education](https://arxiv.org/abs/2512.16036)
*Diane Myung-kyung Woodbridge,Allyson Seba,Freddie Seba,Aydin Schwartz*

Main category: cs.AI

TL;DR: 研究人员开发了一个自动化系统，用于从课程大纲和机构政策网站中发现和分类AI相关政策，结合无监督主题建模和LLM分类，以促进教育中GenAI的安全、公平和教学对齐使用。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在教育中的广泛应用，学生使用这些工具进行学习，但也存在抄袭、错误信息和削弱批判性思维的风险。各教育机构制定了不同的AI政策，但这些政策差异大且不断变化，导致学生不清楚期望和最佳实践，需要系统化的政策发现和分类工具。

Method: 设计并实现了一个自动化系统，结合无监督主题建模技术识别关键政策主题，并使用大型语言模型（LLMs）对政策文本中的GenAI允许程度和其他要求进行分类。系统从课程大纲和机构政策网站收集政策信息。

Result: 开发的应用在主题发现方面获得了0.73的一致性分数。基于GPT-4.0的政策分类在八个已识别主题上实现了0.92-0.97的精确度和0.85-0.97的召回率。系统能够提供结构化和可解释的政策信息。

Conclusion: 该工具通过提供结构化的政策信息，促进了教育中GenAI技术的安全、公平和教学对齐使用。系统可以集成到教育技术平台中，帮助学生理解和遵守相关指南，解决政策不明确的问题。

Abstract: As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.

</details>


### [24] [ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs](https://arxiv.org/abs/2512.16149)
*Hao Chen,Zhexin Hu,Jiajun Chai,Haocheng Yang,Hang He,Xiaohan Wang,Wei Lin,Luhang Wang,Guojun Yin,Zhuofeng zhao*

Main category: cs.AI

TL;DR: ToolForge是一个自动化合成框架，仅需少量虚拟工具即可生成高质量工具调用训练数据，无需真实API调用，使8B参数模型在多个基准测试中超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法依赖大量真实API调用，成本高昂且缺乏多跳推理和自我反思能力，限制了工具调用LLM的训练效果。

Method: 基于(问题、黄金上下文、答案)三元组合成大规模工具学习数据，引入多跳推理和自我反思机制，采用多层验证框架结合规则和模型评估确保数据质量。

Result: 仅用8B参数的模型在使用合成数据训练后，在多个基准测试中超越了GPT-4o的性能表现。

Conclusion: ToolForge框架能够高效生成高质量工具调用训练数据，显著降低数据生成成本，提升模型在真实世界工具调用任务中的性能。

Abstract: Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .

</details>


### [25] [Science Consultant Agent](https://arxiv.org/abs/2512.16171)
*Karthikeyan K,Philip Wu,Xin Tang,Alexandre Alves*

Main category: cs.AI

TL;DR: Science Consultant Agent是一个基于网页的AI工具，通过问卷、智能填充、研究指导推荐和原型构建四个核心组件，帮助从业者选择和实施最有效的AI建模策略。


<details>
  <summary>Details</summary>
Motivation: 为AI从业者（包括产品经理、软件开发人员和研究人员）提供一个系统化的工具，帮助他们更高效地选择和实施AI建模策略，加速AI解决方案的开发过程。

Method: 通过四个核心组件：1）结构化问卷收集需求；2）智能填充辅助完成问卷；3）基于文献研究的推荐系统；4）原型构建器生成实施方案。整个流程在图1中展示。

Result: 开发了一个完整的网页AI工具，能够系统化地指导从业者选择最合适的AI建模策略，并生成相应的原型方案，加速AI解决方案的开发。

Conclusion: Science Consultant Agent是一个有效的AI建模策略选择工具，通过结合结构化问卷、文献研究和原型生成，为不同背景的从业者提供了系统化的AI解决方案开发支持。

Abstract: The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.

</details>


### [26] [Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis](https://arxiv.org/abs/2512.16237)
*Zhi Helu,Huang Jingjing,Xu Wang,Xu Yangbin,Zhang Wanyue,Jiang Baoyang,Deng Shirui,Zhu Liang,Li Fangfang,Zhao Tiejun,Lin Yankai,Yao Yuan*

Main category: cs.AI

TL;DR: SPRITE框架通过模拟器和大型模型程序化合成可扩展、多样且高质量的空间推理数据，解决了传统模板方法结构僵化与人工标注不可扩展的困境。


<details>
  <summary>Details</summary>
Motivation: 当前体现智能面临空间理解和推理能力的限制，现有增强视觉语言模型的方法存在两难：模板数据集可扩展但结构僵化，人工标注语言多样但不可扩展且计算不精确。

Method: 将真实数据生成重构为代码生成任务，利用LLM将复杂空间问题编译为可执行程序，通过模拟器提取的高精度场景元信息进行验证，确保计算精确性和可验证性。

Result: 构建了包含3个模拟器、11k+场景、300k+图像/视频指令调优对的数据集，训练出的VLM在多个空间基准测试中表现显著提升，优于同等规模的其他开源数据集。

Conclusion: SPRITE框架通过程序化合成方法克服了传统模板方法的低多样性问题，对于构建鲁棒、可泛化的空间智能至关重要，框架代码和完整数据集将公开以促进空间智能研究。

Abstract: Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.

</details>


### [27] [AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints](https://arxiv.org/abs/2512.16245)
*Aniruddha Roy,Jyoti Patel,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.AI

TL;DR: AlignMerge是一个几何感知的LLM合并框架，通过显式保持对齐性来避免传统合并方法破坏模型安全性，在保持任务性能的同时显著提升对齐指标。


<details>
  <summary>Details</summary>
Motivation: 传统LLM合并方法（如线性权重组合、任务向量、Fisher加权平均）虽然能保持损失函数值，但会悄悄破坏模型的对齐性（安全性）。合并不应只是数值技巧，而应是围绕已对齐锚点的几何约束操作。

Method: 在指令调优基模型的局部Fisher图中，估计对齐子空间并定义投影器P_A。优化目标函数L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud，其中L_geo保持合并结果在Fisher-Rao几何中接近专家模型，L_align惩罚沿对齐敏感方向的移动，L_bud强制执行软对齐预算。使用解码不变的对齐质量指数(AQI)作为对齐功能度量。

Result: 在五个模型家族（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）上，AlignMerge在合并安全锚点和任务专家时，显著提升对齐指标（AQI、毒性、LLM-judge对齐），同时在指令遵循、推理和帮助性方面匹配或超越最佳专家。相比Fisher soups、TIES、SafeMerge和MergeAlign，显示更小的对齐子空间漂移和更少的预算违反。

Conclusion: AlignMerge使对齐保持合并成为首要设计目标，为未来基础模型的几何感知组合提供了路径。将合并从数值技巧转变为几何约束操作，确保在组合能力的同时不破坏安全性。

Abstract: Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.
  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:
  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,
  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.
  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.

</details>


### [28] [QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems](https://arxiv.org/abs/2512.16279)
*Yiliu Yang,Yilei Jiang,Qunzhong Wang,Yingshui Tan,Xiaoyong Zhu,Sherman S. M. Chow,Bo Zheng,Xiangyu Yue*

Main category: cs.AI

TL;DR: QuadSentinel是一个四智能体安全防护系统，将自然语言安全策略编译为机器可检查规则，在线执行以保护基于大语言模型的智能体在多步骤任务中的安全。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体在执行复杂任务时存在安全风险，而部署者用自然语言编写的安全策略具有模糊性和上下文依赖性，难以映射到机器可检查规则，导致运行时执行不可靠。

Method: 将安全策略表达为序列式规则，设计四智能体防护系统（状态跟踪器、策略验证器、威胁监视器和裁判），将策略编译为基于可观察状态的谓词构建的机器可检查规则，并在线执行。裁判逻辑加上高效的top-k谓词更新器通过优先级检查和分层冲突解决来降低成本。

Result: 在ST-WebAgentBench（ICML CUA '25）和AgentHarm（ICLR '25）基准测试中，QuadSentinel提高了防护栏杆准确性和规则召回率，同时减少了误报。相比ShieldAgent（ICML '25）等单智能体基线，提供了更好的整体安全控制。

Conclusion: 近期部署可以采用这种模式，无需修改核心智能体，通过保持策略分离和机器可检查来实现安全防护。代码将在GitHub上公开。

Abstract: Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.

</details>


### [29] [Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection](https://arxiv.org/abs/2512.16300)
*Fanrui Zhang,Qiang Zhang,Sizhuo Zhou,Jianwen Sun,Chuanhao Li,Jiaxin Ai,Yukang Feng,Yujie Zhang,Wenjie Li,Zizhen Li,Yifan Chang,Jiawei Liu,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出了ForenAgent框架，通过多轮交互让多模态大语言模型自主生成、执行和迭代优化基于Python的低级工具，实现更灵活可解释的图像伪造检测


<details>
  <summary>Details</summary>
Motivation: 现有图像伪造检测方法要么使用低级语义无关的伪影特征，要么依赖多模态大语言模型的高级语义知识。这两种信息流在范式和推理方式上高度异构，难以统一建模它们的跨层次交互

Method: 提出ForenAgent框架，采用两阶段训练流程（冷启动+强化微调），设计动态推理循环（全局感知、局部聚焦、迭代探测、整体裁决），并构建FABench数据集（10万图像，20万交互问答对）

Result: 实验表明ForenAgent在具有挑战性的图像伪造检测任务中展现出新兴的工具使用能力和反思推理能力，为通用图像伪造检测开辟了有前景的路径

Conclusion: ForenAgent通过让MLLMs自主操作低级工具，实现了更灵活可解释的伪造分析，为解决图像伪造检测中异构信息流整合问题提供了有效方案

Abstract: Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.

</details>


### [30] [Adaptation of Agentic AI](https://arxiv.org/abs/2512.16301)
*Pengcheng Jiang,Jiacheng Lin,Zhiyi Shi,Zifeng Wang,Luxi He,Yichen Wu,Ming Zhong,Peiyang Song,Qizheng Zhang,Heng Wang,Xueqiang Xu,Hanwen Xu,Pengrui Han,Dylan Zhang,Jiashuo Sun,Chaoqi Yang,Kun Qian,Tian Wang,Changran Hu,Manling Li,Quanzheng Li,Hao Peng,Sheng Wang,Jingbo Shang,Chao Zhang,Jiaxuan You,Liyuan Liu,Pan Lu,Yu Zhang,Heng Ji,Yejin Choi,Dawn Song,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 本文提出了一个系统框架，将智能体AI系统的适应机制统一为智能体适应和工具适应两大类，并进一步细分为不同形式，为构建更强大、高效、可靠的智能体AI系统提供理论基础和实践指导。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统在能力和范围上的增长，适应机制成为提升性能、可靠性和泛化能力的核心手段。当前研究领域快速扩展但缺乏系统性框架，需要统一的理论基础来指导实践。

Method: 提出了一个系统性框架，将适应机制分为智能体适应和工具适应两大类。智能体适应进一步分为工具执行信号驱动和智能体输出信号驱动两种形式；工具适应分为智能体无关和智能体监督两种形式。通过该框架分析设计空间、明确权衡，并提供策略选择和切换的实践指导。

Result: 该框架有助于澄清智能体AI中适应策略的设计空间，使权衡更加明确，为系统设计中的策略选择或切换提供实践指导。通过对各类代表性方法的回顾，分析了它们的优势和局限性。

Conclusion: 本文为研究人员和从业者构建更强大、高效、可靠的智能体AI系统提供了概念基础和实践路线图，同时指出了关键开放挑战和未来机会。

Abstract: Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.

</details>


### [31] [Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference](https://arxiv.org/abs/2512.16317)
*Arther Tian,Alex Ding,Frank Chen,Alan Wu,Aaron Chan,Bruce Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种成本感知的PoQ框架，将显式效率测量整合到推理节点和评估节点的奖励机制中，通过实验证明该框架能有效奖励高质量低成本的推理模型和高效评估器。


<details>
  <summary>Details</summary>
Motivation: 去中心化LLM推理需要透明且抗审查的访问，但现有验证方法难以扩展到现代模型。原始PoQ忽略了推理节点和评估节点之间的异构计算成本，需要设计成本感知的框架来平衡质量和成本。

Method: 提出成本感知PoQ框架，将显式效率测量整合到奖励机制中。设计结合了真实标记级F1、轻量级学习评估器和基于GPT判断的统一评估流程，采用线性奖励函数平衡归一化质量和成本。

Result: 实验表明：语义文本相似性双编码器比交叉编码器与真实值和GPT分数相关性更高；质量-成本分析显示池中最大模型在单位延迟质量方面最有效；蒙特卡洛模拟显示成本感知奖励方案能一致奖励高质量低成本的推理模型和高效评估器。

Conclusion: 成本感知PoQ为经济可持续的去中心化LLM推理提供了实用基础，能有效奖励高质量低成本的节点，惩罚低质量慢速节点。

Abstract: Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.
  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.

</details>


### [32] [PCIA: A Path Construction Imitation Algorithm for Global Optimization](https://arxiv.org/abs/2512.16392)
*Mohammad-Javad Rezaei,Mozafar Bag-Mohammadi*

Main category: cs.AI

TL;DR: 提出了一种新的元启发式优化算法PCIA，受人类构建和使用路径的方式启发，在53个数学优化问题和13个约束优化问题上表现优异


<details>
  <summary>Details</summary>
Motivation: 受人类路径构建行为的启发：人类偏好热门交通路线，在路径封闭时能智能混合现有路径构建新路线，并随机选择不同路径探索未知目的地

Method: PCIA模仿人类路径构建行为，生成随机种群寻找最佳路径（类似群体算法），每个粒子代表一条通往目的地的路径

Result: 在53个数学优化问题和13个约束优化问题上测试，PCIA与流行及最新的元启发式算法相比具有高度竞争力

Conclusion: PCIA是一种有效的元启发式优化算法，受人类路径构建行为启发，在多种优化问题上表现出色

Abstract: In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.

</details>


### [33] [Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs](https://arxiv.org/abs/2512.16424)
*Nguyen Xuan-Vu,Daniel Armstrong,Milena Wehrbach,Andres M Bran,Zlatko Jončev,Philippe Schwaller*

Main category: cs.AI

TL;DR: Synthelite是一个基于大语言模型的计算机辅助合成规划框架，能够通过自然语言交互生成合成路线，并允许专家干预，在约束条件下达到95%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助合成规划框架缺乏与人类专家交互的机制，限制了化学家洞察力的整合。需要开发能够利用人类专业知识并灵活适应各种约束的合成规划工具。

Method: 使用大语言模型直接提出逆合成转化，利用LLMs固有的化学知识和推理能力生成端到端合成路线，同时通过自然语言提示允许专家干预。

Result: Synthelite能够灵活适应多样化的用户指定约束，在策略约束和起始材料约束的合成任务中达到95%的成功率，并在路线设计中考虑化学可行性。

Conclusion: Synthelite既是实用工具，也是迈向以大语言模型为核心协调器的合成规划范式的重要一步，展示了LLMs在化学合成规划中的潜力。

Abstract: Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.

</details>


### [34] [TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles](https://arxiv.org/abs/2512.16442)
*Allard Oelen,Sören Auer*

Main category: cs.AI

TL;DR: TIB AIssistant是一个AI支持的研究平台，通过多个专门助手和工具支持整个研究生命周期，生成的数据可导出为RO-Crate格式以提高研究透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和大语言模型的广泛应用，学术界需要AI支持的研究工具来协助研究人员完成整个研究生命周期中的各项任务，提高研究效率和透明度。

Method: 开发TIB AIssistant平台，包含多个专门负责特定研究任务的助手，提供访问外部学术服务的工具，并将生成的数据存储在资产中，可导出为RO-Crate格式。

Result: 通过顺序演示各个助手的交互功能，展示了平台如何协作生成研究论文草稿的各个部分，为构建社区维护的AI支持研究平台奠定了基础。

Conclusion: TIB AIssistant为AI支持的研究提供了一个可行平台，通过模块化助手和标准化数据格式支持研究的透明度和可重复性，为未来社区维护的研究平台发展奠定了基础。

Abstract: The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.

</details>


### [35] [Towards AI-Supported Research: a Vision of the TIB AIssistant](https://arxiv.org/abs/2512.16447)
*Sören Auer,Allard Oelen,Mohamad Yaser Jaradeh,Mutahira Khalid,Farhana Keya,Sasi Kiran Gaddipati,Jennifer D'Souza,Lorenz Schlüter,Amirreza Alasti,Gollam Rabby,Azanzi Jiomekong,Oliver Karras*

Main category: cs.AI

TL;DR: TIB AIssistant是一个领域无关的人机协作平台，旨在通过AI助手支持跨学科研究生命周期中的各种任务，解决AI集成到研究中的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和大型语言模型的快速发展有望改变研究方式，但将AI有效集成到研究中面临诸多挑战，包括领域需求差异、AI素养有限、工具协调复杂以及生成式AI在研究中的准确性不明确等问题。

Method: 提出TIB AIssistant平台，采用模块化组件设计，包括提示和工具库、共享数据存储以及灵活的编排框架，支持研究生命周期中的构思、文献分析、方法开发、数据分析和学术写作等任务。

Result: 开发了早期原型系统，展示了该方法的可行性和潜在影响，包括概念框架、系统架构和实施细节。

Conclusion: TIB AIssistant作为一个领域无关的人机协作平台，有潜力支持跨学科的科学发现，通过模块化设计解决AI集成到研究中的挑战。

Abstract: The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.

</details>


### [36] [TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries](https://arxiv.org/abs/2512.16453)
*Jiayang Yang,Chunhui Zhao,Martin Guay,Zhixing Cao*

Main category: cs.AI

TL;DR: TS2R是一个将锂离子电池运行时间序列数据转换为结构化报告的提示框架，使LLM能够在BESS管理中执行推理、预测和决策任务。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多变量时间序列数据解释方面具有潜力，但在实际电池储能系统运维中的应用尚未充分探索，需要建立传感器信号与高层上下文洞察之间的桥梁。

Method: TS2R通过分割、语义抽象和基于规则的解释的组合，将短期时间动态编码为自然语言，创建结构化的语义丰富报告作为LLM的输入。

Result: 在实验室和真实数据集上的评估显示，TS2R在异常检测、荷电状态预测和充放电管理任务中，相比视觉、嵌入和文本提示基线，在准确性、鲁棒性和可解释性方面持续提升LLM性能。

Conclusion: TS2R集成的LLM无需重新训练或架构修改即可达到专家级决策质量和预测一致性，为自适应、LLM驱动的电池智能建立了实用路径。

Abstract: Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.

</details>


### [37] [cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution](https://arxiv.org/abs/2512.16465)
*Jinwu Chen,Qidie Wu,Bin Li,Lin Ma,Xin Si,Yang Hu,Shouyi Yin,Jun Yang*

Main category: cs.AI

TL;DR: cuPilot是一个策略协调的多智能体框架，通过引入策略作为内核演化的中间语义表示，自动优化CUDA内核，在100个内核基准测试中平均比PyTorch快3.09倍。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化具有挑战性且劳动密集，需要硬件-软件协同设计专业知识，而高性能内核库通常是专有的。现有基于大语言模型和进化算法的方法由于智能体设计欠佳和演化表示不匹配，性能表现不足。

Method: 提出cuPilot框架，采用策略协调的多智能体方法，将策略作为内核演化的中间语义表示。关键技术包括策略协调的演化算法、屋顶线引导提示和策略级种群初始化。

Result: 在100个内核基准测试中，cuPilot生成的内核平均比PyTorch快3.09倍。在GEMM任务中，cuPilot展示了复杂的优化技术，并实现了关键硬件单元的高利用率。

Conclusion: cuPilot通过引入策略作为中间表示，解决了现有方法中智能体设计和演化表示不匹配的问题，显著提高了CUDA内核优化的自动化水平和性能表现。

Abstract: Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.

</details>


### [38] [Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery](https://arxiv.org/abs/2512.16468)
*Danial Safaei,Siddartha Khastgir,Mohsen Alirezaei,Jeroen Ploeg,Son Tong,Xingyu Zhao*

Main category: cs.AI

TL;DR: 论文提出了一种新的系统特定保真度度量方法——决定性特征保真度（DFF），用于评估自动驾驶虚拟测试中合成数据的有效性，关注系统决策所依据的因果证据一致性而非像素级视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶安全验证依赖虚拟测试和合成数据，但研究发现像素级视觉真实感并不能保证从仿真到真实世界的可靠迁移。关键问题是系统在真实和仿真环境中是否基于相同的因果证据做出决策，而不仅仅是图像对人类"看起来真实"。现有方法缺乏这种基于行为的保真度度量。

Method: 提出决定性特征保真度（DFF）度量方法，扩展现有保真度谱系以捕捉机制对等性——系统在跨域决策中因果证据的一致性。利用可解释AI方法识别和比较驱动系统输出的决定性特征，提出基于反事实解释的实用估计器，并设计DFF引导的校准方案来提升仿真器保真度。

Result: 在2126对匹配的KITTI-VirtualKITTI2数据对上进行的实验表明，DFF能够揭示传统输出值保真度忽略的差异。结果显示，DFF引导的校准在保持输出值保真度的同时，提升了决定性特征和输入级保真度，适用于多种不同的被测系统。

Conclusion: DFF提供了一种系统特定的行为基础保真度度量，能够更有效地评估合成数据在自动驾驶虚拟测试中的有效性，确保系统在仿真和真实环境中基于相同的因果证据做出决策，为仿真器校准提供了新方向。

Abstract: Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images "look real" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.

</details>


### [39] [Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network](https://arxiv.org/abs/2512.16491)
*Theresa Eimer,Lennart Schäpermeier,André Biedenkapp,Alexander Tornede,Lars Kotthoff,Pieter Leyman,Matthias Feurer,Katharina Eggensperger,Kaitlin Maile,Tanja Tornede,Anna Kozak,Ke Xue,Marcel Wever,Mitra Baratchi,Damir Pulatov,Heike Trautmann,Haniye Kashgarani,Marius Lindauer*

Main category: cs.AI

TL;DR: 该报告收集了元算法研究中的最佳实践，涵盖从研究问题制定到结果分析的完整实验周期，为研究人员提供指导


<details>
  <summary>Details</summary>
Motivation: 元算法研究（如算法选择、配置和调度）依赖大量计算实验，实验设置自由度大但存在多种错误来源，威胁研究结果的可扩展性和有效性。现有最佳实践分散在不同出版物和领域，且各自独立发展

Method: 收集COSEAL社区各子领域的元算法研究良好实践，涵盖完整实验周期：从研究问题制定和实验设计选择，到实验执行，再到结果分析和公正呈现

Result: 建立了元算法研究中的当前最先进实践，为元算法领域的新研究人员和从业者提供指导方针

Conclusion: 该报告系统整理了元算法研究的实验最佳实践，有助于提高研究的严谨性和可重复性，促进该领域的科学发展

Abstract: Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.

</details>


### [40] [Scaling Laws for Energy Efficiency of Local LLMs](https://arxiv.org/abs/2512.16531)
*Ander Alvarez,Alessandro Genuardi,Nilotpal Sinha,Antonio Tiene,Samuel Mugel,Román Orús*

Main category: cs.AI

TL;DR: 本文系统性地研究了在CPU-only边缘设备上部署本地大语言模型和视觉语言模型的性能规律，发现了两个经验性缩放定律，并展示了量子启发压缩技术能显著降低计算和能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU主导现代AI部署，但大多数消费硬件（笔记本电脑、台式机、工业控制器、嵌入式系统）依赖CPU。然而，对于本地语言和视觉语言工作负载的纯CPU推理的计算规律仍未被充分探索。

Method: 在两个代表性CPU层级上系统性地基准测试：MacBook Pro M2（主流笔记本级）和Raspberry Pi 5（低功耗嵌入式）。采用基于处理器和内存使用连续采样及曲线下面积积分的统一方法，分析计算负载如何随输入文本长度（语言模型）和图像分辨率（视觉语言模型）缩放。

Result: 发现了两个经验性缩放定律：1）语言模型推理的计算成本与token长度近似线性缩放；2）视觉语言模型存在预处理驱动的"分辨率拐点"，计算量在内部分辨率钳位以上保持恒定，低于该值则急剧下降。量子启发压缩技术能将处理器和内存使用降低高达71.9%，能耗降低高达62%，同时保持或提高语义准确性。

Conclusion: 这些结果为本地语言和视觉语言工作负载的多模态纯CPU缩放提供了系统性量化分析，并确定了模型压缩和输入分辨率预处理作为可持续边缘推理的有效、低成本杠杆。

Abstract: Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven "resolution knee", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.

</details>


### [41] [Prefix Probing: Lightweight Harmful Content Detection for Large Language Models](https://arxiv.org/abs/2512.16650)
*Jirui Yang,Hengqi Guo,Zhihui Lu,Yi Zhao,Yuansen Zhang,Shijing Hu,Qiang Duan,Yinggui Wang,Tao Wei*

Main category: cs.AI

TL;DR: Prefix Probing是一种黑盒有害内容检测方法，通过比较"同意/执行"与"拒绝/安全"前缀的条件对数概率，利用前缀缓存将检测开销降至接近首词元延迟，无需额外模型或多阶段推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实世界安全敏感应用中面临检测准确性、推理延迟和部署成本之间的三向权衡问题。现有方法通常需要额外的安全模型或多阶段推理，增加了计算开销和部署复杂性。

Method: 提出Prefix Probing方法：1) 比较"同意/执行"与"拒绝/安全"开头前缀的条件对数概率；2) 利用前缀缓存减少检测开销；3) 设计高效的自动前缀构建算法，发现信息量大的前缀；4) 仅需单次对数概率计算即可生成有害性分数并应用阈值。

Result: 广泛实验表明，Prefix Probing的检测效果与主流外部安全模型相当，同时仅产生最小的计算成本，无需额外模型部署，具有强大的实用性和效率。

Conclusion: Prefix Probing是一种高效实用的有害内容检测方法，通过前缀概率比较和缓存机制，在保持检测准确性的同时显著降低了计算开销和部署复杂度，解决了LLM安全应用中的三向权衡问题。

Abstract: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

</details>


### [42] [Comprehensive AI Literacy: The Case for Centering Human Agency](https://arxiv.org/abs/2512.16656)
*Sri Yash Tadimalla,Justin Cary,Gordon Hull,Jordan Register,Daniel Maxwell,David Pugalee,Tina Heafner*

Main category: cs.AI

TL;DR: 该立场论文主张从功能性AI技能培训转向全面的AI素养教育，强调以人类能动性为核心，培养批判性思维和伦理决策能力，而非被动接受技术。


<details>
  <summary>Details</summary>
Motivation: 当前AI技术快速融入社会，但教育框架未能有效应对，导致危险的素养鸿沟。功能性AI工具使用技能的发展掩盖了对AI的批判性和伦理思考的培养，需要系统性变革。

Method: 提出以人类能动性为核心的全面AI素养框架，包括AI素养、流畅性和能力三个层次。强调将技术视为可选择的工具而非必然接受的事物，通过批判性思维和认识论理解来培养教育生态系统中所有利益相关者的决策能力。

Result: 通过该框架，教育者和学生将成为以人为中心的AI方法中的能动主体，能够清晰表达决策意图、态度，并理解这些决策对学术工作、职业和社会的影响。

Conclusion: 真正的AI素养需要系统性转变，从功能性技能培训转向以人类能动性为核心的教育，将技术视为可选择的工具，培养批判性思维和伦理决策能力，使所有教育利益相关者成为技术使用的主动决策者而非被动接受者。

Abstract: The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.

</details>


### [43] [Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm](https://arxiv.org/abs/2512.16694)
*Wisnu Uriawan,Achmad Ajie Priyajie,Angga Gustian,Fikri Nur Hidayat,Sendi Ahmad Rafiudin,Muhamad Fikri Zaelani*

Main category: cs.AI

TL;DR: 使用Apriori算法对印尼语布哈里圣训进行无监督主题分组，通过关联规则挖掘发现圣训中的语义关系模式


<details>
  <summary>Details</summary>
Motivation: 随着伊斯兰文本数字化的发展，迫切需要自动化圣训主题分组技术，以支持数字伊斯兰研究和基于技术的学习系统

Method: 采用无监督学习方法，使用Apriori算法进行关联规则挖掘。数据预处理包括大小写转换、标点清理、分词、停用词去除和词干提取。使用支持度、置信度和提升度参数分析语义关系

Result: 发现了有意义的关联模式：如"rakaat-祈祷"、"verse-启示"、"hadith-故事"之间的关系，这些模式描述了崇拜、启示和圣训叙述等主题

Conclusion: Apriori算法能够自动揭示潜在的语义关系，为数字伊斯兰研究和基于技术的学习系统开发做出贡献

Abstract: This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.

</details>


### [44] [Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning](https://arxiv.org/abs/2512.16698)
*Mahbub E Sobhani,Md. Faiyaz Abdullah Sayeedi,Mohammad Nehad Alam,Proma Hossain Progga,Swakkhar Shatabda*

Main category: cs.AI

TL;DR: 多智能体设计在几何问题求解中并非总是最优：开源模型从多智能体中获益明显，而闭源模型在经典基准上单智能体表现更好，仅在较新数据集上获得有限提升。


<details>
  <summary>Details</summary>
Motivation: 探究多模态大语言模型中多智能体设计相对于单智能体在几何问题求解中的实际效益，明确不同模型类型在不同基准上的表现差异。

Method: 在四个视觉数学基准（Geometry3K、MathVerse、OlympiadBench、We-Math）上系统比较单智能体和多智能体流水线，测试开源模型（Qwen-2.5-VL）和闭源模型（Gemini-2.0-Flash）。

Result: 开源模型在多智能体模式下表现显著提升（如Qwen-2.5-VL 7B在Geometry3K上提升+6.8分）；闭源模型在经典基准上单智能体更优，仅在较新的We-Math数据集上获得有限改进。

Conclusion: 多智能体流水线对开源模型有明显优势，对强大的专有系统在新基准上也有辅助作用，但智能体分解并非普遍最优策略。

Abstract: Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver

</details>


### [45] [Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems](https://arxiv.org/abs/2512.16707)
*Abhisek Ganguly*

Main category: cs.AI

TL;DR: 该论文形式化了算法智能的两个独立计算限制：形式不完备性和动态不可预测性，并证明这两种极端情况共同限制了智能体推理自身预测能力的能力。


<details>
  <summary>Details</summary>
Motivation: 研究算法智能的固有计算限制，特别是形式不完备性（限制一致性推理系统的演绎能力）和动态不可预测性（限制有限精度下的长期预测），探索这些限制如何影响智能体对自身预测能力的推理。

Method: 通过形式化分析，将哥德尔不完备定理和混沌系统的不可预测性相结合，研究这两种计算限制如何共同约束算法智能体的自我推理能力。

Result: 证明算法智能体通常无法计算自身的最大预测视界，揭示了智能系统中推理、预测和自我分析之间的固有权衡。

Conclusion: 形式不完备性和动态不可预测性共同构成了算法智能的基本限制，智能体无法完全理解自身的预测能力，这为智能系统的设计提供了理论边界。

Abstract: We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.

</details>


### [46] [Discovering and Learning Probabilistic Models of Black-Box AI Capabilities](https://arxiv.org/abs/2512.16733)
*Daniel Bramblett,Rushang Karia,Adrian Ciotinga,Ruthvick Suresh,Pulkit Verma,YooJung Choi,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种使用PDDL风格表示来学习和建模黑盒AI系统规划能力的方法，通过蒙特卡洛树搜索生成测试任务并学习符号模型，能够描述AI的能力、执行条件和可能结果。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型等黑盒AI系统越来越多地用于顺序决策，需要开发高效方法来提供这些系统能力的可靠且可解释的表示，以确保其安全部署和操作。

Method: 使用PDDL风格表示来建模黑盒AI的规划能力，采用蒙特卡洛树搜索范式系统性地创建测试任务、获取数据并剪枝可能的符号模型假设空间。

Result: 理论结果证明了学习模型的可靠性、完备性和收敛性。多个黑盒AI系统的实证结果表明该方法具有广泛适用性、高效性和准确性。

Conclusion: 该方法能够有效学习和表示黑盒AI系统的规划能力，为理解这些系统的行为提供了可解释的符号模型，有助于确保AI系统的安全部署。

Abstract: Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.

</details>


### [47] [AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach](https://arxiv.org/abs/2512.16739)
*Yipeng Zhuang,Yifeng Guo,Yuewen Li,Yuheng Wu,Philip Leung-Ho Yu,Tingting Song,Zhiyong Wang,Kunzhong Zhou,Weifang Wang,Li Zhuang*

Main category: cs.AI

TL;DR: 提出混合机器学习和大语言模型管道，预测肺癌住院患者48小时和72小时内的疼痛发作，结合结构化与非结构化电子病历数据，提升预测准确性和临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 肺癌患者中高达91%会经历突发性疼痛发作，需要及时干预。现有方法难以有效预测疼痛发作，限制了主动疼痛管理。需要开发能够结合多种数据源、提高预测敏感性和临床可解释性的工具。

Method: 采用混合机器学习和大语言模型管道：1）机器学习模块分析结构化数据（人口统计学、肿瘤分期、生命体征、WHO分级镇痛药使用）并捕捉时间性药物趋势；2）大语言模型解释模糊的用药记录和自由文本临床笔记。两种模态集成提升预测性能。

Result: 在266名住院患者的回顾性队列中，框架达到0.874（48小时）和0.917（72小时）的准确率。大语言模型的增强使敏感性分别提高了8.6%和10.4%，改善了预测性能。

Conclusion: 这种混合方法为早期疼痛发作预测提供了临床可解释且可扩展的工具，有望提高肿瘤护理中的治疗精确性和优化资源分配。

Abstract: Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.

</details>


### [48] [CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?](https://arxiv.org/abs/2512.16755)
*Siqi Wang,Chao Liang,Yunfan Gao,Erxin Yu,Sen Li,Yushi Li,Jing Li,Haofen Wang*

Main category: cs.AI

TL;DR: CitySeeker是一个评估视觉语言模型在动态城市环境中处理隐性需求导航能力的新基准，包含6440条轨迹，覆盖8个城市和7种目标场景。实验显示当前最佳模型任务完成率仅21.1%，存在长时推理错误累积、空间认知不足和体验记忆缺陷等瓶颈。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在显式指令导航方面取得进展，但在动态城市环境中解释隐性人类需求（如"我渴了"）的能力尚未充分探索。需要评估模型的空间推理和决策能力，以解决隐性需求驱动的具体导航任务。

Method: 提出CitySeeker基准，包含6,440条轨迹，覆盖8个城市，捕捉7种目标驱动场景中的多样化视觉特征和隐性需求。通过实验分析模型性能瓶颈，并探索回溯机制、丰富空间认知和基于记忆检索（BCR）等策略，这些策略受人类认知映射中迭代观察-推理循环和自适应路径优化的启发。

Result: 实验显示即使最佳模型（如Qwen2.5-VL-32B-Instruct）任务完成率也仅为21.1%。识别出三个关键瓶颈：长时推理中的错误累积、空间认知不足和体验记忆缺陷。BCR策略分析为开发具有强大空间智能的视觉语言模型提供了可行见解。

Conclusion: CitySeeker基准揭示了当前视觉语言模型在解决隐性需求导航任务中的严重不足，特别是在长时推理、空间认知和记忆检索方面。提出的分析框架和BCR策略为开发具有鲁棒空间智能的模型、应对"最后一英里"导航挑战提供了重要指导。

Abstract: Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., "I am thirsty") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling "last-mile" navigation challenges.

</details>
