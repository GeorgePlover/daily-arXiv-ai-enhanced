{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS\u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u57fa\u4e8e\u4eba\u7c7b\u5bf9\u9f50\u7684\u952e\u76d8\u9f20\u6807\u8f93\u5165\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e4b\u524d\u6700\u4f73\u6a21\u578b2\u500d\u7684\u6210\u529f\u7387\uff0c\u5728\u672a\u89c1\u8fc7\u76843D\u7f51\u9875\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u7b49\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u8de8\u5f02\u6784\u9886\u57df\uff08\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u9875\u3001\u6a21\u62df\u6e38\u620f\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u514b\u670dAPI\u6216GUI\u65b9\u6cd5\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u7a7a\u95f4\u951a\u5b9a\u5230\u4eba\u7c7b\u5bf9\u9f50\u7684\u952e\u76d8\u9f20\u6807\u8f93\u5165\uff1b\u5728500B tokens\u7684\u591a\u6837\u5316\u8f68\u8ff9\u548c\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b\u91c7\u7528\u8870\u51cf\u6301\u7eed\u635f\u5931\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff1b\u4f7f\u7528\u9ad8\u6548\u7684\u7a00\u758f\u601d\u8003\u7b56\u7565\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u662f\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u76842\u500d\uff1b\u5728\u672a\u89c1\u8fc7\u76843D\u7f51\u9875\u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff1b\u5728FPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aGPT-5\u3001Gemini-2.5-Pro\u548cClaude-4-Sonnet\uff1b\u8bad\u7ec3\u65f6\u95f4\u548c\u6d4b\u8bd5\u65f6\u95f4\u7684\u6269\u5c55\u7ed3\u679c\u8bc1\u5b9e\u7edf\u4e00\u52a8\u4f5c\u7a7a\u95f4\u5728\u8de8\u6e38\u620f\u548c\u591a\u6a21\u6001\u6570\u636e\u6269\u5c55\u65f6\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u7b80\u5355\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u4e0e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5f15\u5165\u5b66\u79d1\u521b\u9020\u529b\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u6848\u4f8b\u8bf4\u660eAI\u53ef\u80fd\u53d6\u4ee3\u800c\u975e\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4ece\u800c\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89d2\u8272\uff0c\u7279\u522b\u662f\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u65e8\u5728\u7406\u89e3AI\u6280\u672f\u5982\u4f55\u6539\u53d8\u79d1\u5b66\u5b9e\u8df5\u7684\u4ef7\u503c\u548c\u610f\u4e49\u3002", "method": "\u57fa\u4e8e\u521b\u9020\u529b\u54f2\u5b66\u7406\u8bba\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u63d0\u51fa\u5b66\u79d1\u521b\u9020\u529b\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9bAI\u65b9\u6cd5\u53ef\u80fd\u53d6\u4ee3\u5b66\u79d1\u521b\u9020\u529b\uff0c\u8fd9\u79cd\u53d6\u4ee3\u53ef\u80fd\u6539\u53d8\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "conclusion": "AI\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u5e94\u7528\u9700\u8981\u8c28\u614e\u8003\u8651\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u907f\u514d\u56e0\u6280\u672f\u5e94\u7528\u800c\u524a\u5f31\u79d1\u5b66\u5b9e\u8df5\u7684\u5185\u5728\u4ef7\u503c\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u73af\u5883POMDP\uff08ME-POMDP\uff09\u6a21\u578b\uff0c\u5904\u7406\u79bb\u6563\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5c06ME-POMDP\u8f6c\u5316\u4e3a\u5bf9\u6297\u4fe1\u5ff5POMDP\uff08AB-POMDP\uff09\uff0c\u5f00\u53d1\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002", "motivation": "\u5f53\u591a\u4e2a\u9886\u57df\u4e13\u5bb6\u5bf9\u95ee\u9898\u5efa\u6a21\u5b58\u5728\u5206\u6b67\u65f6\uff0c\u9700\u8981\u627e\u5230\u80fd\u5728\u6240\u6709\u53ef\u80fdPOMDP\u6a21\u578b\u4e0b\u90fd\u8868\u73b0\u826f\u597d\u7684\u5355\u4e00\u9c81\u68d2\u7b56\u7565\uff0c\u6700\u5927\u5316\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u5956\u52b1\u3002", "method": "\u5c06ME-POMDP\u63a8\u5e7f\u4e3a\u5177\u6709\u521d\u59cb\u4fe1\u5ff5\u96c6\u5408\u7684AB-POMDP\uff0c\u8bc1\u660eME-POMDP\u53ef\u7b80\u5316\u4e3a\u4ec5\u53d8\u5316\u8f6c\u79fb/\u5956\u52b1\u51fd\u6570\u6216\u89c2\u5bdf/\u5956\u52b1\u51fd\u6570\u7684\u7b49\u4ef7\u5f62\u5f0f\uff0c\u5f00\u53d1\u7cbe\u786e\u548c\u57fa\u4e8e\u70b9\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u80fd\u591f\u5728\u6807\u51c6POMDP\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u73af\u5883\u6269\u5c55\u4e2d\u8ba1\u7b97\u9c81\u68d2\u7b56\u7565\u3002", "conclusion": "ME-POMDP\u6846\u67b6\u6709\u6548\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7AB-POMDP\u8f6c\u5316\u548c\u76f8\u5e94\u7b97\u6cd5\u80fd\u591f\u8ba1\u7b97\u591a\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u7b56\u7565\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u65f6\u8c03\u4f18\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u9884\u8bad\u7ec3transformer\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u4ece\u5934\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u65e0\u9700\u6570\u636e\u5e93\u5339\u914d\u6216\u4e2d\u95f4\u6b65\u9aa4\u3002", "motivation": "\u5f53\u524d\u4e32\u8054\u8d28\u8c31\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5e93\u5339\u914d\u6216\u9700\u8981\u4e2d\u95f4\u7247\u6bb5\u9884\u6d4b\u7684\u591a\u6b65\u9aa4\u6d41\u7a0b\uff0c\u96be\u4ee5\u8bc6\u522b\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u4e0d\u5b58\u5728\u7684\u5316\u5408\u7269\u3002", "method": "\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u4f18\u589e\u5f3a\u9884\u8bad\u7ec3transformer\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\u548c\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5NPLIB1\u548cMassSpecGym\u4e0a\u5206\u522b\u8d85\u8fc7\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5DiffMS 100%\u548c20%\uff0c\u6d4b\u8bd5\u65f6\u8c03\u4f18\u5728MassSpecGym\u4e0a\u6bd4\u4f20\u7edf\u5fae\u8c03\u6027\u80fd\u63d0\u534762%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u52a8\u6001\u9002\u5e94\u65b0\u8d28\u8c31\u6570\u636e\uff0c\u5373\u4f7f\u9884\u6d4b\u4e0e\u771f\u5b9e\u503c\u6709\u504f\u5dee\uff0c\u751f\u6210\u7684\u5206\u5b50\u5019\u9009\u7ed3\u6784\u4ecd\u7136\u51c6\u786e\uff0c\u4e3a\u4eba\u5de5\u89e3\u91ca\u548c\u66f4\u53ef\u9760\u7684\u8bc6\u522b\u63d0\u4f9b\u6709\u4ef7\u503c\u6307\u5bfc\u3002"}}
{"id": "2510.23911", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.23911", "abs": "https://arxiv.org/abs/2510.23911", "authors": ["Arno Uhlig", "Iris Braun", "Matthias W\u00e4hlisch"], "title": "The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing", "comment": "15 pages", "summary": "Allocating resources in a distributed environment is a fundamental challenge.\nIn this paper, we analyze the scheduling and placement of virtual machines\n(VMs) in the cloud platform of SAP, the world's largest enterprise resource\nplanning software vendor. Based on data from roughly 1,800 hypervisors and\n48,000 VMs within a 30-day observation period, we highlight potential\nimprovements for workload management. The data was measured through\nobservability tooling that tracks resource usage and performance metrics across\nthe entire infrastructure. In contrast to existing datasets, ours uniquely\noffers fine-grained time-series telemetry data of fully virtualized\nenterprise-level workloads from both long-running and memory-intensive SAP\nS/4HANA and diverse, general-purpose applications. Our key findings include\nseveral suboptimal scheduling situations, such as CPU resource contention\nexceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced\ncompute hosts with a maximum CPU~utilization on intra-building block hosts of\nup to 99%, and overprovisioned CPU and memory resources resulting into over 80%\nof VMs using less than 70% of the provided resources. Bolstered by these\nfindings, we derive requirements for the design and implementation of novel\nplacement and scheduling algorithms and provide guidance to optimize resource\nallocations. We make the full dataset used in this study publicly available to\nenable data-driven evaluations of scheduling approaches for large-scale cloud\ninfrastructures in future research.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86SAP\u4e91\u5e73\u53f0\u4e2d\u865a\u62df\u673a\u8c03\u5ea6\u548c\u653e\u7f6e\u95ee\u9898\uff0c\u57fa\u4e8e1800\u53f0\u865a\u62df\u5316\u4e3b\u673a\u548c48000\u4e2aVM\u768430\u5929\u6570\u636e\uff0c\u63ed\u793a\u4e86\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u591a\u4e2a\u4f4e\u6548\u95ee\u9898\uff0c\u5305\u62ecCPU\u8d44\u6e90\u4e89\u7528\u8d85\u8fc740%\u3001CPU\u5c31\u7eea\u65f6\u95f4\u8fbe220\u79d2\u3001\u4e3b\u673a\u8d1f\u8f7d\u4e25\u91cd\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f01\u4e1a\u7ea7\u4e91\u5e73\u53f0\u4e2d\u4f18\u5316\u865a\u62df\u673a\u8c03\u5ea6\u548c\u8d44\u6e90\u7ba1\u7406\uff0c\u4ee5\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "method": "\u901a\u8fc7\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u6536\u96c630\u5929\u51851800\u53f0\u865a\u62df\u5316\u4e3b\u673a\u548c48000\u4e2a\u865a\u62df\u673a\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5e8f\u5217\u9065\u6d4b\u6570\u636e\uff0c\u5206\u6790SAP S/4HANA\u548c\u5176\u4ed6\u901a\u7528\u5e94\u7528\u7684\u8d44\u6e90\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u591a\u4e2a\u6b21\u4f18\u8c03\u5ea6\u60c5\u51b5\uff1aCPU\u8d44\u6e90\u4e89\u7528\u8d85\u8fc740%\uff0cCPU\u5c31\u7eea\u65f6\u95f4\u9ad8\u8fbe220\u79d2\uff0c\u4e3b\u673a\u95f4CPU\u5229\u7528\u7387\u4e25\u91cd\u4e0d\u5e73\u8861\uff08\u6700\u9ad8\u8fbe99%\uff09\uff0c\u8d85\u8fc780%\u7684VM\u4f7f\u7528\u4e0d\u523070%\u7684\u5206\u914d\u8d44\u6e90\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\u63d0\u51fa\u4e86\u65b0\u578b\u8c03\u5ea6\u7b97\u6cd5\u7684\u8bbe\u8ba1\u9700\u6c42\uff0c\u4e3a\u4f18\u5316\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u5c06\u5b8c\u6574\u6570\u636e\u96c6\u516c\u5f00\u4ee5\u652f\u6301\u672a\u6765\u5927\u89c4\u6a21\u4e91\u57fa\u7840\u8bbe\u65bd\u8c03\u5ea6\u65b9\u6cd5\u7684\u6570\u636e\u9a71\u52a8\u8bc4\u4f30\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u8c61\u68cb\u8c1c\u9898\u521b\u4f5c\u4e2d\u7684\u521b\u9020\u529b\uff0c\u5f00\u53d1\u4e86\u80fd\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u3001\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u6cd5\u7684\u8c61\u68cb\u8c1c\u9898AI\u7cfb\u7edf\uff0c\u5e76\u7531\u4e09\u4f4d\u4e16\u754c\u7ea7\u8c61\u68cb\u4e13\u5bb6\u8bc4\u4f30\u5176\u521b\u9020\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u662f\u5426\u80fd\u591f\u4ea7\u751f\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u548c\u65b0\u9896\u6027\u7684\u8f93\u51fa\uff0c\u7279\u522b\u662f\u5728\u8c61\u68cb\u8c1c\u9898\u521b\u4f5c\u8fd9\u4e00\u9700\u8981\u9ad8\u5ea6\u521b\u9020\u529b\u7684\u9886\u57df\u3002", "method": "\u5f00\u53d1\u4e13\u95e8\u7684AI\u7cfb\u7edf\u751f\u6210\u8c61\u68cb\u8c1c\u9898\uff0c\u7136\u540e\u9080\u8bf7\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u7684\u8c61\u68cb\u4e13\u5bb6\uff08\u56fd\u9645\u8c61\u68cb\u6392\u5c40\u5927\u5e08Amatzia Avni\u3001\u7279\u7ea7\u5927\u5e08Jonathan Levitt\u548cMatthew Sadler\uff09\u5bf9AI\u751f\u6210\u7684\u8c1c\u9898\u8fdb\u884c\u8bc4\u5ba1\uff0c\u8bc4\u4f30\u5176\u521b\u9020\u6027\u3001\u6311\u6218\u6027\u548c\u7f8e\u5b66\u8bbe\u8ba1\u3002", "result": "\u4e09\u4f4d\u8c61\u68cb\u4e13\u5bb6\u5bf9AI\u751f\u6210\u7684\u8c1c\u9898\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u9009\u51fa\u4e86\u4ed6\u4eec\u6700\u559c\u6b22\u7684\u8c1c\u9898\uff0c\u5e76\u89e3\u91ca\u4e86\u8fd9\u4e9b\u8c1c\u9898\u5728\u521b\u9020\u6027\u3001\u6311\u6218\u6c34\u5e73\u6216\u7f8e\u5b66\u8bbe\u8ba1\u65b9\u9762\u7684\u5438\u5f15\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u751f\u6210\u5f0fAI\u5728\u8c61\u68cb\u8c1c\u9898\u521b\u4f5c\u9886\u57df\u80fd\u591f\u4ea7\u751f\u5177\u6709\u521b\u9020\u6027\u548c\u7f8e\u5b66\u4ef7\u503c\u7684\u8f93\u51fa\uff0c\u5f97\u5230\u4e86\u4e13\u4e1a\u8c61\u68cb\u4e13\u5bb6\u7684\u8ba4\u53ef\u3002"}}
{"id": "2510.23993", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23993", "abs": "https://arxiv.org/abs/2510.23993", "authors": ["Anthony Carreon", "Jagmohan Singh", "Shivank Sharma", "Shuzhi Zhang", "Venkat Raman"], "title": "A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales", "comment": "32 pages, 12 figures", "summary": "High-speed chemically active flows present significant computational\nchallenges due to their disparate space and time scales, where stiff chemistry\noften dominates simulation time. While modern supercomputing scientific codes\nachieve exascale performance by leveraging graphics processing units (GPUs),\nexisting GPU-based compressible combustion solvers face critical limitations in\nmemory management, load balancing, and handling the highly localized nature of\nchemical reactions. To this end, we present a high-performance compressible\nreacting flow solver built on the AMReX framework and optimized for multi-GPU\nsettings. Our approach addresses three GPU performance bottlenecks: memory\naccess patterns through column-major storage optimization, computational\nworkload variability via a bulk-sparse integration strategy for chemical\nkinetics, and multi-GPU load distribution for adaptive mesh refinement\napplications. The solver adapts existing matrix-based chemical kinetics\nformulations to multigrid contexts. Using representative combustion\napplications including hydrogen-air detonations and jet in supersonic crossflow\nconfigurations, we demonstrate $2-5\\times$ performance improvements over\ninitial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA\nH100 GPUs. Roofline analysis reveals substantial improvements in arithmetic\nintensity for both convection ($\\sim 10 \\times$) and chemistry ($\\sim 4\n\\times$) routines, confirming efficient utilization of GPU memory bandwidth and\ncomputational resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAMReX\u6846\u67b6\u7684\u9ad8\u6027\u80fd\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\uff0c\u9488\u5bf9\u591aGPU\u73af\u5883\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u8ba1\u7b97\u8d1f\u8f7d\u53d8\u5316\u548c\u591aGPU\u8d1f\u8f7d\u5206\u914d\u7b49\u6027\u80fd\u74f6\u9888\uff0c\u5728\u71c3\u70e7\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e862-5\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9ad8\u901f\u5316\u5b66\u53cd\u5e94\u6d41\u7531\u4e8e\u7a7a\u95f4\u548c\u65f6\u95f4\u5c3a\u5ea6\u7684\u5de8\u5927\u5dee\u5f02\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u6311\u6218\uff0c\u5176\u4e2d\u521a\u6027\u5316\u5b66\u52a8\u529b\u5b66\u901a\u5e38\u4e3b\u5bfc\u6a21\u62df\u65f6\u95f4\u3002\u73b0\u6709\u7684\u57fa\u4e8eGPU\u7684\u53ef\u538b\u7f29\u71c3\u70e7\u6c42\u89e3\u5668\u5728\u5185\u5b58\u7ba1\u7406\u3001\u8d1f\u8f7d\u5e73\u8861\u548c\u5904\u7406\u5316\u5b66\u53cd\u5e94\u9ad8\u5ea6\u5c40\u90e8\u5316\u7279\u6027\u65b9\u9762\u5b58\u5728\u5173\u952e\u9650\u5236\u3002", "method": "\u57fa\u4e8eAMReX\u6846\u67b6\u6784\u5efa\u591aGPU\u4f18\u5316\u7684\u53cd\u5e94\u6d41\u6c42\u89e3\u5668\uff0c\u91c7\u7528\u5217\u4f18\u5148\u5b58\u50a8\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u4f7f\u7528\u6279\u91cf\u7a00\u758f\u79ef\u5206\u7b56\u7565\u5904\u7406\u5316\u5b66\u52a8\u529b\u5b66\u8ba1\u7b97\u8d1f\u8f7d\u53d8\u5316\uff0c\u5e76\u9488\u5bf9\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u5e94\u7528\u4f18\u5316\u591aGPU\u8d1f\u8f7d\u5206\u914d\uff0c\u5c06\u73b0\u6709\u57fa\u4e8e\u77e9\u9635\u7684\u5316\u5b66\u52a8\u529b\u5b66\u516c\u5f0f\u9002\u914d\u5230\u591a\u91cd\u7f51\u683c\u73af\u5883\u3002", "result": "\u5728\u6c22-\u7a7a\u6c14\u7206\u8f70\u548c\u8d85\u58f0\u901f\u6a2a\u6d41\u4e2d\u5c04\u6d41\u7b49\u4ee3\u8868\u6027\u71c3\u70e7\u5e94\u7528\u4e2d\uff0c\u76f8\u6bd4\u521d\u59cbGPU\u5b9e\u73b0\u5b9e\u73b0\u4e862-5\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u57281-96\u4e2aNVIDIA H100 GPU\u4e0a\u8868\u73b0\u51fa\u63a5\u8fd1\u7406\u60f3\u7684\u5f31\u6269\u5c55\u6027\u3002\u5c4b\u9876\u7ebf\u5206\u6790\u663e\u793a\u5bf9\u6d41\u548c\u5316\u5b66\u52a8\u529b\u5b66\u4f8b\u7a0b\u7684\u7b97\u672f\u5f3a\u5ea6\u5206\u522b\u63d0\u9ad8\u4e86\u7ea610\u500d\u548c4\u500d\u3002", "conclusion": "\u8be5\u6c42\u89e3\u5668\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u8ba1\u7b97\u8d1f\u8f7d\u5e73\u8861\u548c\u591aGPU\u8d1f\u8f7d\u5206\u914d\uff0c\u6709\u6548\u5229\u7528\u4e86GPU\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u538b\u7f29\u53cd\u5e94\u6d41\u6a21\u62df\u7684\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2510.23899", "categories": ["cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23899", "abs": "https://arxiv.org/abs/2510.23899", "authors": ["Maria G. Mendoza", "Addison Kalanther", "Daniel Bostwick", "Emma Stephan", "Chinmay Maheshwari", "Shankar Sastry"], "title": "Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments", "comment": "Accepted to IEEE Global Humanitarian Technology Conference (GHTC\n  2025). 8 pages, 4 figures", "summary": "Autonomous drone technology holds significant promise for enhancing search\nand rescue operations during evacuations by guiding humans toward safety and\nsupporting broader emergency response efforts. However, their application in\ndynamic, real-time evacuation support remains limited. Existing models often\noverlook the psychological and emotional complexity of human behavior under\nextreme stress. In real-world fire scenarios, evacuees frequently deviate from\ndesignated safe routes due to panic and uncertainty. To address these\nchallenges, this paper presents a multi-agent coordination framework in which\nautonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time\nby locating, intercepting, and guiding them to safety under uncertain\nconditions. We model the problem as a Partially Observable Markov Decision\nProcess (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)\nand a low-level rescuer (LLR), coordinate through shared observations and\ncomplementary capabilities. Human behavior is captured using an agent-based\nmodel grounded in empirical psychology, where panic dynamically affects\ndecision-making and movement in response to environmental stimuli. The\nenvironment features stochastic fire spread, unknown evacuee locations, and\nlimited visibility, requiring UAVs to plan over long horizons to search for\nhumans and adapt in real-time. Our framework employs the Proximal Policy\nOptimization (PPO) algorithm with recurrent policies to enable robust\ndecision-making in partially observable settings. Simulation results\ndemonstrate that the UAV team can rapidly locate and intercept evacuees,\nsignificantly reducing the time required for them to reach safety compared to\nscenarios without UAV assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\uff0c\u4f7f\u7528\u5f02\u6784\u65e0\u4eba\u673a\u5728\u706b\u707e\u758f\u6563\u4e2d\u5b9e\u65f6\u5b9a\u4f4d\u3001\u62e6\u622a\u548c\u5f15\u5bfc\u758f\u6563\u4eba\u5458\u5230\u5b89\u5168\u533a\u57df\uff0c\u57fa\u4e8ePOMDP\u5efa\u6a21\u548cPPO\u7b97\u6cd5\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11\u758f\u6563\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u5ffd\u7565\u6781\u7aef\u538b\u529b\u4e0b\u4eba\u7c7b\u884c\u4e3a\u7684\u5fc3\u7406\u548c\u60c5\u611f\u590d\u6742\u6027\uff0c\u771f\u5b9e\u706b\u707e\u573a\u666f\u4e2d\u758f\u6563\u4eba\u5458\u5e38\u56e0\u6050\u614c\u548c\u4e0d\u786e\u5b9a\u6027\u504f\u79bb\u5b89\u5168\u8def\u7ebf\uff0c\u9700\u8981\u65e0\u4eba\u673a\u5b9e\u65f6\u534f\u52a9\u758f\u6563\u3002", "method": "\u91c7\u7528\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\u5efa\u6a21\uff0c\u4e24\u4e2a\u5f02\u6784\u65e0\u4eba\u673a\u667a\u80fd\u4f53(\u9ad8\u5c42\u6551\u63f4\u8005\u548c\u4f4e\u5c42\u6551\u63f4\u8005)\u901a\u8fc7\u5171\u4eab\u89c2\u5bdf\u548c\u4e92\u8865\u80fd\u529b\u534f\u8c03\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u5fc3\u7406\u5b66\u7684\u667a\u80fd\u4f53\u6a21\u578b\u6355\u6349\u6050\u614c\u5bf9\u51b3\u7b56\u548c\u79fb\u52a8\u7684\u5f71\u54cd\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u548c\u5faa\u73af\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u4eba\u673a\u56e2\u961f\u80fd\u591f\u5feb\u901f\u5b9a\u4f4d\u548c\u62e6\u622a\u758f\u6563\u4eba\u5458\uff0c\u76f8\u6bd4\u65e0\u65e0\u4eba\u673a\u534f\u52a9\u7684\u60c5\u51b5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u758f\u6563\u4eba\u5458\u5230\u8fbe\u5b89\u5168\u533a\u57df\u6240\u9700\u7684\u65f6\u95f4\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53\u534f\u8c03\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u5b9e\u65f6\u758f\u6563\u652f\u6301\u4e2d\u7684\u6311\u6218\uff0c\u65e0\u4eba\u673a\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u6210\u529f\u534f\u52a9\u4eba\u7c7b\u758f\u6563\uff0c\u4e3a\u7d27\u6025\u54cd\u5e94\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u764c\u75c7\u8bca\u65ad\u548c\u9884\u540e\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5f31\u70b9\uff0c\u5305\u62ec\u8bca\u65ad\u51c6\u786e\u6027\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u3001\u51e0\u4f55\u4e0d\u7a33\u5b9a\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u6e90\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\u5047\u8bbe\u4e0e\u4eba\u4f53\u7ec4\u7ec7\u5185\u5728\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6982\u5ff5\u4e0d\u5339\u914d\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u5728\u975e\u533b\u5b66\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5feb\u901f\u5e94\u7528\u672a\u80fd\u5b9e\u73b0\u9884\u671f\u7684\u7a81\u7834\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u6839\u672c\u5f31\u70b9\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u539f\u56e0\u6765\u8bc6\u522b\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u7f3a\u9677\uff1a\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u3001\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u67b6\u6784\u590d\u6742\u6027\u3001\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u521b\u65b0\u3001\u6570\u636e\u4e0d\u8db3\u4ee5\u53ca\u4e0e\u7ec4\u7ec7\u5207\u7247\u5927\u5c0f\u76f8\u5173\u7684\u57fa\u672c\u8bbe\u8ba1\u7f3a\u9677\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u8bca\u65ad\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u51e0\u4f55\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5b89\u5168\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u672c\u8d28\u5b58\u5728\u6982\u5ff5\u6027\u4e0d\u5339\u914d\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u6982\u5ff5\u6846\u67b6\u9700\u8981\u6839\u672c\u6027\u91cd\u65b0\u601d\u8003\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u4eba\u4f53\u7ec4\u7ec7\u7684\u590d\u6742\u7279\u6027\u3002"}}
{"id": "2510.24175", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24175", "abs": "https://arxiv.org/abs/2510.24175", "authors": ["Nitin Shukla", "Alessandro Romeo", "Caterina Caravita", "Michael Redenti", "Radim Vavrik", "Lubomir Riha", "Andrea Mignone", "Marco Rossazza", "Stefano Truzzi", "Luca Tornatore", "Antonio Ragagnin", "Tiago Castro", "Geray S. Karademir", "Klaus Dolag", "Pranab J. Deka", "Fabio Bacchini", "Rostislav-Paul Wilhelm", "Daniele Gregori", "Elisabetta Boella"], "title": "Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System", "comment": null, "summary": "Developing and redesigning astrophysical, cosmological, and space plasma\nnumerical codes for existing and next-generation accelerators is critical for\nenabling large-scale simulations. To address these challenges, the SPACE Center\nof Excellence (SPACE-CoE) fosters collaboration between scientists, code\ndevelopers, and high-performance computing experts to optimize applications for\nthe exascale era. This paper presents our strategy and initial results on the\nLeonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3\nand iPIC3D, using profiling tools to analyze performance on single and multiple\nnodes. Preliminary tests show all three codes scale efficiently, reaching 80%\nscalability up to 1,024 GPUs.", "AI": {"tldr": "SPACE\u4e2d\u5fc3\u901a\u8fc7\u4f18\u5316gPLUTO\u3001OpenGadget3\u548ciPIC3D\u4e09\u4e2a\u65d7\u8230\u4ee3\u7801\uff0c\u5728Leonardo\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1024\u4e2aGPU\u768480%\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5929\u4f53\u7269\u7406\u548c\u5b87\u5b99\u5b66\u7684\u5927\u89c4\u6a21\u6a21\u62df\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u4e3a\u73b0\u6709\u548c\u4e0b\u4e00\u4ee3\u52a0\u901f\u5668\u5f00\u53d1\u548c\u91cd\u65b0\u8bbe\u8ba1\u5929\u4f53\u7269\u7406\u3001\u5b87\u5b99\u5b66\u548c\u7a7a\u95f4\u7b49\u79bb\u5b50\u4f53\u6570\u503c\u4ee3\u7801\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u5e94\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6027\u80fd\u5206\u6790\u5de5\u5177\u5206\u6790\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u4e0a\u7684\u6027\u80fd\uff0c\u4f18\u5316\u4e09\u4e2a\u65d7\u8230\u4ee3\u7801\uff08gPLUTO\u3001OpenGadget3\u548ciPIC3D\uff09\u5728CINECA\u7684Leonardo\u7cfb\u7edf\u4e0a\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u663e\u793a\u6240\u6709\u4e09\u4e2a\u4ee3\u7801\u90fd\u80fd\u9ad8\u6548\u6269\u5c55\uff0c\u57281024\u4e2aGPU\u4e0a\u8fbe\u523080%\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SPACE\u4e2d\u5fc3\u901a\u8fc7\u79d1\u5b66\u5bb6\u3001\u4ee3\u7801\u5f00\u53d1\u8005\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e13\u5bb6\u4e4b\u95f4\u7684\u5408\u4f5c\uff0c\u6210\u529f\u4f18\u5316\u4e86\u5173\u952e\u5e94\u7528\uff0c\u4e3a\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u65f6\u4ee3\u505a\u597d\u4e86\u51c6\u5907\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9012\u5f52\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u524d\u77bb\u5206\u89e3\u3001\u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\u548c\u5185\u5b58\u9ad8\u6548\u6267\u884c\u4e09\u79cd\u673a\u5236\uff0c\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u6f02\u79fb\u548c\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u52a8\u6001\u91cd\u89c4\u5212\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u987a\u5e8f\u63d0\u793a\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u548c\u91cd\u590d\u5931\u8d25\u5faa\u73af\uff0c\u4ee5\u53ca\u5206\u5c42\u63d0\u793a\u65b9\u6cd5\u524a\u5f31\u8de8\u7ea7\u8fde\u7eed\u6027\u6216\u4ea7\u751f\u5927\u91cf\u8fd0\u884c\u65f6\u5f00\u9500\u7684\u95ee\u9898\u3002", "method": "ReCAP\u6846\u67b6\u7ed3\u5408\u4e09\u79cd\u5173\u952e\u673a\u5236\uff1a(1)\u8ba1\u5212\u524d\u77bb\u5206\u89e3\uff1a\u6a21\u578b\u751f\u6210\u5b8c\u6574\u5b50\u4efb\u52a1\u5217\u8868\uff0c\u6267\u884c\u7b2c\u4e00\u9879\u5e76\u4f18\u5316\u5269\u4f59\u4efb\u52a1\uff1b(2)\u7ed3\u6784\u5316\u7236\u8ba1\u5212\u91cd\u6ce8\u5165\uff1a\u5728\u9012\u5f52\u8fd4\u56de\u65f6\u4fdd\u6301\u4e00\u81f4\u7684\u8de8\u7ea7\u4e0a\u4e0b\u6587\uff1b(3)\u5185\u5b58\u9ad8\u6548\u6267\u884c\uff1a\u9650\u5236\u6d3b\u52a8\u63d0\u793a\u4f7f\u6210\u672c\u968f\u4efb\u52a1\u6df1\u5ea6\u7ebf\u6027\u6269\u5c55\u3002", "result": "\u5728\u5404\u79cd\u957f\u65f6\u7a0b\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReCAP\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u76ee\u6807\u5bf9\u9f50\u548c\u6210\u529f\u7387\uff0c\u5728\u4e25\u683cpass@1\u534f\u8bae\u4e0b\uff0c\u540c\u6b65Robotouille\u4efb\u52a1\u63d0\u5347\u4e8632%\uff0c\u5f02\u6b65Robotouille\u4efb\u52a1\u63d0\u5347\u4e8629%\u3002", "conclusion": "ReCAP\u901a\u8fc7\u5c06\u9ad8\u5c42\u76ee\u6807\u4e0e\u4f4e\u5c42\u52a8\u4f5c\u5bf9\u9f50\u3001\u51cf\u5c11\u5197\u4f59\u63d0\u793a\u548c\u4fdd\u6301\u8de8\u9012\u5f52\u7684\u8fde\u8d2f\u4e0a\u4e0b\u6587\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24205", "abs": "https://arxiv.org/abs/2510.24205", "authors": ["Telmo Ribeiro", "Jos\u00e9 Proen\u00e7a", "M\u00e1rio Florido"], "title": "CoMPSeT: A Framework for Comparing Multiparty Session Types", "comment": "In Proceedings EXPRESS/SOS 2025, arXiv:2510.23211", "summary": "Concurrent systems are often complex and difficult to design. Choreographic\nlanguages, such as Multiparty Session Types (MPST), allow the description of\nglobal protocols of interactions by capturing valid patterns of interactions\nbetween participants. Many variations of MPST exist, each one with its rather\nspecific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that\nprovides clearer insights over different features in existing MPST. We select a\nrepresentative set of MPST examples and provide mechanisms to combine different\nfeatures and to animate and compare the semantics of concrete examples. CoMPSeT\nis open-source, compiled into JavaScript, and can be directly executed from any\nbrowser, becoming useful both for researchers who want to better understand the\nlandscape of MPST and for teachers who want to explain global choreographies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCoMPSeT\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u591a\u515a\u4f1a\u8bdd\u7c7b\u578b\uff08MPST\uff09\u7684\u7279\u6027\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u6559\u5e08\u66f4\u597d\u5730\u7406\u89e3\u5168\u5c40\u7f16\u6392\u534f\u8bae\u3002", "motivation": "\u5e76\u53d1\u7cfb\u7edf\u8bbe\u8ba1\u590d\u6742\uff0c\u73b0\u6709\u7684MPST\u53d8\u4f53\u5404\u6709\u7279\u5b9a\u529f\u80fd\u548c\u7279\u6027\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6bd4\u8f83\u5de5\u5177\u6765\u6e05\u6670\u5c55\u793a\u4e0d\u540c\u7279\u6027\u3002", "method": "\u9009\u62e9\u4ee3\u8868\u6027MPST\u793a\u4f8b\uff0c\u63d0\u4f9b\u673a\u5236\u7ec4\u5408\u4e0d\u540c\u7279\u6027\uff0c\u901a\u8fc7\u52a8\u753b\u548c\u6bd4\u8f83\u5177\u4f53\u793a\u4f8b\u7684\u8bed\u4e49\u6765\u5c55\u793a\u5dee\u5f02\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90\u7684CoMPSeT\u5de5\u5177\uff0c\u7f16\u8bd1\u4e3aJavaScript\uff0c\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u76f4\u63a5\u8fd0\u884c\u3002", "conclusion": "CoMPSeT\u4e3a\u7814\u7a76\u4eba\u5458\u7406\u89e3MPST\u9886\u57df\u548c\u6559\u5e08\u8bb2\u89e3\u5168\u5c40\u7f16\u6392\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u6bd4\u8f83\u8d2a\u5a6a\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0LLM\u667a\u80fd\u4f53\u5728\u9002\u5f53\u63d0\u793a\u4e0b\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\u534f\u8c03\u591a\u4e2a\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u957f\u671f\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u76ee\u6807\u5206\u914d\u95ee\u9898\u3002", "method": "\u667a\u80fd\u4f53\u57fa\u4e8e\u73af\u5883\u7ed3\u6784\u5316\u8868\u793a\u72ec\u7acb\u751f\u6210\u76ee\u6807\u504f\u597d\u6392\u5e8f\uff0c\u7136\u540e\u4ea4\u6362\u6392\u5e8f\u4fe1\u606f\uff0c\u901a\u8fc7\u56fa\u5b9a\u7684\u786e\u5b9a\u6027\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\u8fdb\u884c\u76ee\u6807\u5206\u914d\uff0c\u65e0\u9700\u534f\u5546\u6216\u8fed\u4ee3\u534f\u8c03\u3002\u7cfb\u7edf\u6bd4\u8f83\u4e86\u8d2a\u5a6a\u542f\u53d1\u5f0f\u3001\u6700\u4f18\u5206\u914d\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u3002", "result": "LLM\u667a\u80fd\u4f53\u5728\u63d0\u4f9b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u76f8\u5173\u5b9a\u91cf\u4fe1\u606f\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u7a81\u51fa\u4e86\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u4fe1\u606f\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24442", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24442", "abs": "https://arxiv.org/abs/2510.24442", "authors": ["Yiding Wang", "Yuxuan Chen", "Fanxu Meng", "Xifan Chen", "Xiaolei Yang", "Muhan Zhang"], "title": "Law in Silico: Simulating Legal Society with LLM-Based Agents", "comment": null, "summary": "Since real-world legal experiments are often costly or infeasible, simulating\nlegal societies with Artificial Intelligence (AI) systems provides an effective\nalternative for verifying and developing legal theory, as well as supporting\nlegal administration. Large Language Models (LLMs), with their world knowledge\nand role-playing capabilities, are strong candidates to serve as the foundation\nfor legal society simulation. However, the application of LLMs to simulate\nlegal systems remains underexplored. In this work, we introduce Law in Silico,\nan LLM-based agent framework for simulating legal scenarios with individual\ndecision-making and institutional mechanisms of legislation, adjudication, and\nenforcement. Our experiments, which compare simulated crime rates with\nreal-world data, demonstrate that LLM-based agents can largely reproduce\nmacro-level crime trends and provide insights that align with real-world\nobservations. At the same time, micro-level simulations reveal that a\nwell-functioning, transparent, and adaptive legal system offers better\nprotection of the rights of vulnerable individuals.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u6846\u67b6Law in Silico\uff0c\u901a\u8fc7\u6a21\u62df\u7acb\u6cd5\u3001\u88c1\u51b3\u548c\u6267\u884c\u7b49\u5236\u5ea6\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86LLM\u80fd\u591f\u91cd\u73b0\u5b8f\u89c2\u72af\u7f6a\u8d8b\u52bf\u5e76\u4e3a\u6cd5\u5f8b\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6cd5\u5f8b\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u6216\u96be\u4ee5\u5b9e\u65bd\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6a21\u62df\u6cd5\u5f8b\u793e\u4f1a\u6210\u4e3a\u9a8c\u8bc1\u548c\u53d1\u5c55\u6cd5\u5f8b\u7406\u8bba\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002LLM\u51ed\u501f\u5176\u4e16\u754c\u77e5\u8bc6\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u662f\u6784\u5efa\u6cd5\u5f8b\u793e\u4f1a\u6a21\u62df\u7684\u7406\u60f3\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86Law in Silico\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u6a21\u62df\u5305\u542b\u4e2a\u4f53\u51b3\u7b56\u4ee5\u53ca\u7acb\u6cd5\u3001\u88c1\u51b3\u548c\u6267\u884c\u7b49\u5236\u5ea6\u673a\u5236\u7684\u6cd5\u5f8b\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u80fd\u591f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u91cd\u73b0\u5b8f\u89c2\u5c42\u9762\u7684\u72af\u7f6a\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e0e\u73b0\u5b9e\u4e16\u754c\u89c2\u5bdf\u4e00\u81f4\u7684\u89c1\u89e3\u3002\u5fae\u89c2\u5c42\u9762\u7684\u6a21\u62df\u63ed\u793a\u4e86\u4e00\u4e2a\u8fd0\u4f5c\u826f\u597d\u3001\u900f\u660e\u4e14\u9002\u5e94\u6027\u7684\u6cd5\u5f8b\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u4fdd\u62a4\u5f31\u52bf\u4e2a\u4f53\u7684\u6743\u5229\u3002", "conclusion": "LLM\u5728\u6a21\u62df\u6cd5\u5f8b\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u6cd5\u5f8b\u7406\u8bba\u9a8c\u8bc1\u548c\u884c\u653f\u51b3\u7b56\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5efa\u7acb\u5b8c\u5584\u6cd5\u5f8b\u4f53\u7cfb\u5bf9\u4fdd\u62a4\u5f31\u52bf\u7fa4\u4f53\u6743\u5229\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u578b\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u53cd\u76f4\u89c9\u3001\u521b\u9020\u6027\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u7684\u80fd\u529b\uff0c\u751f\u6210\u7684\u53cd\u76f4\u89c9\u8c1c\u9898\u6bd4\u4f8b\u4ece0.22%\u63d0\u5347\u52302.5%\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u6700\u4f73\u6a21\u578b\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u751f\u6210\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u5b66\u4ef7\u503c\u548c\u53cd\u76f4\u89c9\u6027\u7684\u8f93\u51fa\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u9886\u57df\u3002", "method": "\u9996\u5148\u5bf9\u751f\u6210\u5f0fAI\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5956\u52b1\u673a\u5236\u6765\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\u3001\u53cd\u76f4\u89c9\u6027\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u53cd\u76f4\u89c9\u8c1c\u9898\u751f\u6210\u6bd4\u4f8b\u4ece0.22%\uff08\u76d1\u7763\u5b66\u4e60\uff09\u63d0\u9ad8\u52302.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6570\u636e\u96c6\uff082.1%\uff09\u548c\u6700\u4f73Lichess\u8bad\u7ec3\u6a21\u578b\uff080.4%\uff09\u3002\u751f\u6210\u7684\u8c1c\u9898\u6ee1\u8db3\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\u57fa\u51c6\uff0c\u4fdd\u7559\u4e86\u7f8e\u5b66\u4e3b\u9898\uff0c\u5e76\u88ab\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4e3a\u6bd4\u4e66\u7c4d\u8c1c\u9898\u66f4\u5177\u521b\u9020\u6027\u3001\u8da3\u5473\u6027\u548c\u53cd\u76f4\u89c9\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684AI\u751f\u6210\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u4e13\u5bb6\u8ba4\u53ef\u5176\u521b\u9020\u6027\uff0c\u6700\u7ec8\u6210\u679c\u662f\u4e00\u672c\u7cbe\u5fc3\u7b56\u5212\u7684AI\u751f\u6210\u8c1c\u9898\u624b\u518c\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001\u7269\u7406\u5efa\u6a21\u3001LSTM\u3001\u6df7\u5408\u5efa\u6a21\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\uff09\u5728\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff0c\u800cMPC\u63a7\u5236\u5668\u6027\u80fd\u6700\u7a33\u5065\u3002", "motivation": "\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u6574\u5408\u7269\u7406\u57fa\u7840\u3001\u6570\u636e\u9a71\u52a8\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4f20\u7edf\u4e0eAI\u9a71\u52a8\u63a7\u5236\u5668\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u5fae\u578b\u6e29\u5ba4\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u3001PBM\u3001LSTM\u3001HAM\uff09\u548c\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08MPC\u3001RL\u3001LLM\uff09\uff0c\u5728\u63d2\u503c\u548c\u5916\u63a8\u573a\u666f\u4e0b\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "HAM\u6a21\u578b\u5728\u7cbe\u5ea6\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u5747\u8861\uff1bLSTM\u7cbe\u5ea6\u9ad8\u4f46\u8d44\u6e90\u6d88\u8017\u5927\uff1bMPC\u63a7\u5236\u5668\u6027\u80fd\u7a33\u5065\u53ef\u9884\u6d4b\uff1bRL\u63a7\u5236\u5668\u9002\u5e94\u6027\u5f3a\uff1bLLM\u63a7\u5236\u5668\u7ed3\u5408\u9884\u6d4b\u5de5\u5177\u53ef\u5b9e\u73b0\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "HAM\u63d0\u4f9b\u4e86\u5efa\u6a21\u4e2d\u6700\u5e73\u8861\u7684\u6027\u80fd\uff0cMPC\u5728\u63a7\u5236\u4e2d\u8868\u73b0\u6700\u7a33\u5065\uff0c\u4e0d\u540c\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u9002\u5e94\u6027\u548c\u5b9e\u73b0\u590d\u6742\u5ea6\u65b9\u9762\u5b58\u5728\u6743\u8861\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fdAI\u7cfb\u7edf\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u65e8\u5728\u652f\u6301\u5b89\u5168\u8bbe\u8ba1\u7684\u667a\u80fd\u7cfb\u7edf\u5f00\u53d1\u3002", "motivation": "\u968f\u7740\u5177\u5907\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8bb0\u5fc6\u548c\u81ea\u4e3b\u80fd\u529b\u7684\u667a\u80fdAI\u7cfb\u7edf\u5728web\u3001\u8f6f\u4ef6\u548c\u7269\u7406\u73af\u5883\u4e2d\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\uff0c\u5b83\u4eec\u521b\u9020\u4e86\u4e0e\u4f20\u7edfAI\u5b89\u5168\u548c\u8f6f\u4ef6\u5b89\u5168\u4e0d\u540c\u7684\u65b0\u578b\u653e\u5927\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u667a\u80fdAI\u7279\u6709\u7684\u5a01\u80c1\u5206\u7c7b\u6cd5\uff0c\u56de\u987e\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u89d2\u5ea6\u8ba8\u8bba\u9632\u5fa1\u7b56\u7565\uff0c\u7efc\u5408\u5f53\u524d\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u667a\u80fdAI\u7cfb\u7edf\u7684\u5a01\u80c1\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u72ec\u7279\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u9632\u5fa1\u63aa\u65bd\u3002", "conclusion": "\u667a\u80fdAI\u7cfb\u7edf\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6311\u6218\uff0c\u9700\u8981\u5b89\u5168\u8bbe\u8ba1\u7684\u65b9\u6cd5\u6765\u5f00\u53d1\uff0c\u5f53\u524d\u7814\u7a76\u4e3a\u6784\u5efa\u5b89\u5168\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f46\u4ecd\u5b58\u5728\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\uff0c\u901a\u8fc7\u591a\u6837\u6027\u5bfb\u6c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u6269\u5c55\u7b56\u7565\uff0c\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8bad\u7ec3\u7b97\u6cd5\uff08\u5982SFT\u3001PPO\u3001GRPO\uff09\u5728\u672a\u89c1\u63a8\u7406\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u4e25\u91cd\u4f9d\u8d56\u6709\u504f\u7684\u5956\u52b1\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5c06LVLMs\u4e2d\u7684\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u540e\u9a8c\u63a8\u65ad\uff0c\u63d0\u51fa\u57fa\u4e8e\u644a\u9500\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff1b\u5229\u7528\u591a\u6837\u6027\u5bfb\u6c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5f15\u5165\u7a00\u758f\u5956\u52b1\u51fd\u6570\uff0c\u9f13\u52b1\u591a\u6837\u5316\u7684\u9ad8\u4f3c\u7136\u6f5c\u5728CoT\uff1b\u91c7\u7528\u8d1d\u53f6\u65af\u63a8\u7406\u6269\u5c55\u7b56\u7565\uff0c\u7528\u8fb9\u9645\u4f3c\u7136\u66ff\u4ee3\u6602\u8d35\u7684Best-of-N\u548cBeam Search\u6765\u9ad8\u6548\u6392\u5e8f\u6700\u4f18\u63a8\u7406\u8def\u5f84\u548c\u7b54\u6848\u3002", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u7684LVLMs\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u6846\u67b6\u548c\u591a\u6837\u6027\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u63d0\u51fa\u76f4\u89c9\u4e3b\u4e49\u53bb\u4e2d\u5fc3\u5316\u56e0\u679c\u53d1\u73b0\u6846\u67b6judo calculus\uff0c\u4f7f\u7528j-stable\u56e0\u679c\u63a8\u65ad\u548cj-do-calculus\u5728\u5c42\u62d3\u6251\u4e2d\u5f62\u5f0f\u5316\u5904\u7406\u73b0\u5b9e\u5e94\u7528\u4e2d\u56e0\u679c\u6548\u5e94\u7684\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u56e0\u679c\u6548\u5e94\u4f9d\u8d56\u4e8e\u60c5\u5883\uff08\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u7b49\uff09\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u5904\u7406\u8fd9\u79cd\u60c5\u5883\u4f9d\u8d56\u6027\u3002", "method": "\u4f7f\u7528judo calculus\u7ed3\u5408\u5c42\u62d3\u6251\u7406\u8bba\uff0c\u901a\u8fc7Lawvere-Tierney\u6a21\u6001\u7b97\u5b50j\u9009\u62e9\u76f8\u5173\u60c5\u5883\uff0c\u5b9e\u73b0j-stable\u56e0\u679c\u63a8\u65ad\uff0c\u5e76\u4e0e\u6807\u51c6\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u7ecf\u5178\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u5f97\u76ca\u4e8e\u5c42\u62d3\u6251\u56e0\u679c\u53d1\u73b0\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u3002", "conclusion": "judo calculus\u4e3a\u5904\u7406\u60c5\u5883\u4f9d\u8d56\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7b26\u53f7\u4f30\u8ba1\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ea4\u53c9\u71b5\u635f\u5931\u66ff\u6362\u4e3a\u4e8c\u5143\u5206\u7c7b\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u4e0b\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u8bc1\u660e\u7684\u4e00\u81f4\u6027\u548c\u591a\u9879\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u8fb9\u754c\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5f02\u8d28\u6027\u5f88\u8106\u5f31\uff0c\u62df\u5408\u7b80\u5355\u7684\u6982\u7387\u6a21\u578b\u5230\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u7fa4\u4f53\u5e73\u5747\u6548\u7528\u4f30\u8ba1\uff0c\u8fd9\u662f\u793e\u4f1a\u798f\u5229\u7684\u89c4\u8303\u5ea6\u91cf\u3002", "method": "\u63d0\u51fa\u7b26\u53f7\u4f30\u8ba1\u5668\u65b9\u6cd5\uff0c\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u6362\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u6062\u590d\u4e00\u81f4\u7684\u6709\u5e8f\u5bf9\u9f50\uff0c\u5e76\u5b9e\u73b0\u8be5\u8bbe\u7f6e\u4e0b\u7684\u9996\u4e2a\u591a\u9879\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u8fb9\u754c\u3002", "result": "\u5728\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684LLM\u5bf9\u9f50\u73b0\u5b9e\u6a21\u62df\u4e2d\uff0c\u7b26\u53f7\u4f30\u8ba1\u5668\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u62df\u4eba\u7269\u9762\u677f\u4e0a\u7684\u504f\u597d\u5931\u771f\uff0c\u5c06\uff08\u89d2\u5ea6\uff09\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u8fd135%\uff0c\u4e0e\u771f\u5b9e\u7fa4\u4f53\u504f\u597d\u7684\u4e0d\u4e00\u81f4\u6027\u4ece12%\u964d\u81f38%\uff0c\u4f18\u4e8e\u6807\u51c6RLHF\u3002", "conclusion": "\u7b26\u53f7\u4f30\u8ba1\u5668\u5728\u4fdd\u6301\u73b0\u6709LLM\u5bf9\u9f50\u7ba1\u9053\u5b9e\u73b0\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u9700\u8981\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u548c\u8ddf\u8e2a\u4e2a\u4f53\u7ea7\u504f\u597d\u6570\u636e\u7684\u9762\u677f\u6570\u636e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u6765\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u3002", "motivation": "\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u524d\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53\u5f02\u8d28\u6027\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u7684\u6307\u6807\uff0c\u590d\u6742\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u4ea4\u4e92\u5173\u7cfb\u672a\u88ab\u5145\u5206\u6355\u6349\uff0c\u4ee5\u53ca\u4e2a\u4f53\u7ea7\u79fb\u52a8\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u9002\u5408\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6574\u5408\u4e2a\u4f53\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\uff0c\u5229\u7528\u5927\u89c4\u6a21\u7a00\u758f\u4e2a\u4f53\u7ea7\u6570\u636e\u6355\u6349\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6574\u5408\u4e2a\u4f53SIR\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u6761\u4ef6\u6a21\u578b\u80fd\u591f\u6355\u6349\u5230\u5177\u6709\u76f8\u4f3c\u4e8b\u4ef6\u524d\u79fb\u52a8\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u4e4b\u95f4\u79fb\u52a8\u6a21\u5f0f\u7684\u5dee\u5f02\u53d8\u5316\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u76f8\u4f3c\u4e8b\u4ef6\u524d\u6a21\u5f0f\u4f46\u97e7\u6027\u4e0d\u540c\u7684\u4e2a\u4f53\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u7ec4\u4ef6\uff0c\u5206\u522b\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u548c\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u7684token\u5316\u673a\u5236\u8fdb\u884c\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u9762\u4e34\u7684\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u5468\u671f\u6027\u6a21\u5f0f\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u672a\u5206\u5316\u7684\u5e8f\u5217\uff0c\u6ca1\u6709\u663e\u5f0f\u89e3\u8026\u5176\u56fa\u6709\u7ed3\u6784\u7ec4\u4ef6\u3002", "method": "\u63d0\u51faOneCast\u6846\u67b6\uff1a1\uff09\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u7ec4\u4ef6\uff1b2\uff09\u5b63\u8282\u6027\u7ec4\u4ef6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u6a21\u5757\u4f7f\u7528\u53ef\u89e3\u91ca\u57fa\u51fd\u6570\u91cd\u5efa\u5468\u671f\u6027\u6a21\u5f0f\uff1b3\uff09\u8d8b\u52bf\u7ec4\u4ef6\u901a\u8fc7\u8bed\u4e49\u611f\u77e5tokenizer\u7f16\u7801\u4e3a\u79bb\u6563token\uff0c\u4f7f\u7528\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u8fdb\u884c\u63a8\u65ad\uff1b4\uff09\u4e24\u5206\u652f\u8f93\u51fa\u7ed3\u5408\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cOneCast\u5927\u591a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u548c\u6a21\u5757\u5316\u5efa\u6a21\u65b9\u6cd5\uff0cOneCast\u80fd\u591f\u6709\u6548\u6355\u6349\u5b63\u8282\u6027\u6a21\u5f0f\u540c\u65f6\u8ddf\u8e2a\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\uff0c\u5728\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u9a70\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u4f18\u4e8e\u7269\u7406\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8ddf\u8f66\u95f4\u8ddd\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u666e\u53ca\uff0c\u9700\u8981\u7406\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u4ee5\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u6a21\u578b\uff08IDM\u3001OVM\u3001OVRV\u3001\u7b80\u5316CACC\uff09\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff09\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7535\u52a8\u6c7d\u8f66\u8ddf\u968f\u5185\u71c3\u673a\u8f66\u8f86\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u6821\u51c6\u548c\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5728\u4e0d\u540c\u8ddf\u8f66\u95f4\u8ddd\u6761\u4ef6\u4e0b\u7684RMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u8ddd\uff09\u30010.0016\uff08\u957f\u95f4\u8ddd\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u8ddd\uff09\uff1b\u5728\u7269\u7406\u6a21\u578b\u4e2d\uff0cCACC\u6a21\u578b\u8868\u73b0\u6700\u597d\uff0c\u957f\u95f4\u8dddRMSE\u4e3a2.67\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u9a70\u884c\u4e3a\u5efa\u6a21\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u5bf9\u4e8e\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u4e3b\u4ea4\u901a\u52a8\u6001\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens\u662f\u4e00\u4e2a\u900f\u660e\u7684AI\u52a9\u624b\uff0c\u5141\u8bb8\u75c5\u7406\u5b66\u5bb6\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\u7ec4\u7ec7\u5207\u7247\u95ee\u9898\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u62a5\u544a\u548c\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u5e2e\u52a9\u533b\u751f\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u5730\u505a\u51fa\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u533b\u751f\u771f\u6b63\u4fe1\u4efb\u4eba\u5de5\u667a\u80fd\uff0cAI\u4e0d\u80fd\u662f\u9ed1\u76d2\uff0c\u9700\u8981\u50cf\u54a8\u8be2\u540c\u4e8b\u4e00\u6837\u7406\u89e3\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u521b\u5efaHistoLens\u7cfb\u7edf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3aAI\u5f15\u64ce\u7684\u7cbe\u786e\u67e5\u8be2\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u62a5\u544a\u548c\u70ed\u56fe\u53ef\u89c6\u5316\u8bc1\u636e\uff0c\u5e76\u8bad\u7ec3AI\u4e13\u6ce8\u4e8e\u60a3\u8005\u7ec4\u7ec7\u800c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\u3002", "result": "\u5b9e\u73b0\u4e86\u75c5\u7406\u5b66\u5bb6\u4e3b\u5bfc\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u533b\u751f\u53ef\u4ee5\u4f7f\u7528\u53ef\u4fe1\u8d56\u7684AI\u52a9\u624b\u9a8c\u8bc1\u89c1\u89e3\uff0c\u505a\u51fa\u66f4\u5feb\u3001\u66f4\u81ea\u4fe1\u7684\u8bca\u65ad\u3002", "conclusion": "HistoLens\u4f5c\u4e3a\u900f\u660e\u7684\u534f\u4f5c\u4f19\u4f34\uff0c\u8ba9\u75c5\u7406\u5b66\u5bb6\u4fdd\u6301\u4e13\u5bb6\u4e3b\u5bfc\u5730\u4f4d\uff0c\u540c\u65f6\u5229\u7528AI\u52a9\u624b\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u4fe1\u5fc3\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "OpsAgent\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u81ea\u6f14\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e91\u7cfb\u7edf\u4e8b\u4ef6\u7ba1\u7406\uff0c\u901a\u8fc7\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u900f\u660e\u8bca\u65ad\u63a8\u7406\uff0c\u5177\u6709\u53cc\u81ea\u6f14\u8fdb\u673a\u5236\u652f\u6301\u6301\u7eed\u80fd\u529b\u589e\u957f\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u4e8b\u4ef6\u7ba1\u7406\u5728\u9762\u4e34\u6d77\u91cf\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u65f6\u52b3\u52a8\u5bc6\u96c6\u4e14\u6613\u51fa\u9519\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u8de8\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3001\u90e8\u7f72\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u514d\u8bad\u7ec3\u6570\u636e\u5904\u7406\u5c06\u5f02\u6784\u53ef\u89c2\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u5b9e\u73b0\u900f\u660e\u8bca\u65ad\u63a8\u7406\uff0c\u5f15\u5165\u53cc\u81ea\u6f14\u8fdb\u673a\u5236\uff08\u5185\u90e8\u6a21\u578b\u66f4\u65b0\u548c\u5916\u90e8\u7ecf\u9a8c\u79ef\u7d2f\uff09\u652f\u6301\u6301\u7eed\u80fd\u529b\u589e\u957f\u3002", "result": "\u5728OPENRCA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660eOpsAgent\u5177\u6709\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u81ea\u6f14\u8fdb\u80fd\u529b\u3002", "conclusion": "OpsAgent\u662f\u4e00\u4e2a\u5b9e\u9645\u53ef\u90e8\u7f72\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e91\u7cfb\u7edf\u7684\u957f\u671f\u8fd0\u7ef4\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "BLM\u2081\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u8fc1\u79fb\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u6cdb\u5316\uff0c\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5bb6\u65cf\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u5177\u8eab\u5316\u4e4b\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0cVLAs\u7f3a\u4e4f\u9c81\u68d2\u7684\u9ad8\u5c42\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u591a\u6570ELLMs\u5c40\u9650\u4e8e\u6570\u5b57\u7a7a\u95f4\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u7269\u7406\u4e16\u754c\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u65e0\u7f1d\u8de8\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\u64cd\u4f5c\u3001\u540c\u65f6\u8de8\u5177\u8eab\u5316\u548c\u4efb\u52a1\u6cdb\u5316\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7cbe\u9009\u6570\u5b57\u8bed\u6599\u5411MLLM\u6ce8\u5165\u5177\u8eab\u77e5\u8bc6\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u4eceMLLM\u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u6765\u6307\u5bfc\u63a7\u5236\uff0c\u65e0\u9700\u5fae\u8c03MLLM\u4e3b\u5e72\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u81ea\u6536\u96c6\u7684\u8de8\u5177\u8eab\u6f14\u793a\u5957\u4ef6\uff0c\u6db5\u76d6\u56db\u79cd\u673a\u5668\u4eba\u5177\u8eab\u548c\u516d\u4e2a\u6e10\u8fdb\u6311\u6218\u6027\u4efb\u52a1\u3002", "result": "\u5728\u6570\u5b57\u548c\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5355\u4e2aBLM\u2081\u5b9e\u4f8b\u4f18\u4e8e\u56db\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08MLLMs\u3001ELLMs\u3001VLAs\u548cGMLMs\uff09\uff0c\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea66%\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u7ea63%\u3002", "conclusion": "BLM\u2081\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u5177\u8eab\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u5728\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\u4e2d\u65e0\u7f1d\u64cd\u4f5c\u7684\u591a\u6a21\u6001\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9MCTS\u4e2d\u62bd\u8c61\u65b9\u6cd5\u5b58\u5728\u7684\u540c\u5c42\u8282\u70b9UCB\u503c\u76f8\u540c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u6765\u66ff\u4ee3\u968f\u673a\u65ad\u70b9\u89c4\u5219\uff0c\u5e76\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7b56\u7565\u7684\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "MCTS\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u72b6\u6001\u548c\u52a8\u4f5c\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982pruned OGA\u5728\u591a\u4e2a\u52a8\u4f5c\u5c5e\u4e8e\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u65f6\uff0c\u5b83\u4eec\u7684UCB\u503c\u4f1a\u76f8\u540c\uff0c\u53ea\u80fd\u4f9d\u8d56\u968f\u673a\u65ad\u70b9\u89c4\u5219\uff0c\u8fd9\u5f71\u54cd\u4e86\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u591a\u79cd\u66ff\u4ee3\u968f\u673a\u7b56\u7565\u7684\u5185\u90e8\u62bd\u8c61\u7b56\u7565\uff0c\u7528\u4e8e\u5904\u7406\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u4e2d\u591a\u4e2a\u52a8\u4f5c\u7684UCB\u503c\u76f8\u540c\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u591a\u4e2a\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u90fd\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u8bbe\u8ba1\u66f4\u667a\u80fd\u7684\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u89e3\u51b3MCTS\u62bd\u8c61\u65b9\u6cd5\u4e2d\u7684UCB\u503c\u76f8\u540c\u95ee\u9898\uff0c\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5185\u90e8\u884c\u4e3a\u7684\u76f8\u5173\u6027\u77e9\u9635\u79e9\u4f5c\u4e3a\u63a8\u7406\u8def\u5f84\u53ef\u4fe1\u5ea6\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u6709\u6548\u68c0\u6d4bLLM\u8f93\u51fa\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u548c\u5e7b\u89c9\u3002\u73b0\u6709\u68c0\u67e5\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u77e9\u9635\u79e9\u662f\u63a8\u7406\u6b63\u786e\u6027\u7684\u7a33\u5065\u6307\u6807\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86Self-Indicator\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u52a0\u6743\u5019\u9009\u63a8\u7406\u8def\u5f84\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u533a\u5206\u6b63\u786e\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\u65b9\u9762\u8fbe\u5230\u8d85\u8fc775%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e868%\u4ee5\u4e0a\u3002", "conclusion": "LLM\u5185\u90e8\u884c\u4e3a\u672c\u8eab\u5df2\u5305\u542b\u5176\u63a8\u7406\u8def\u5f84\u53ef\u4fe1\u5ea6\u7684\u4fe1\u606f\uff0cSelf-Indicator\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u7c7b\u578b\u7684LLM\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u58f0\u660e\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4e0d\u540c\u667a\u80fd\u4f53\u5bf9\u58f0\u660e\u771f\u5b9e\u6027\u4ea7\u751f\u5206\u6b67\u5e76\u5206\u522b\u63d0\u4f9b\u652f\u6301\u548c\u53cd\u5bf9\u8bc1\u636e\uff0c\u6784\u5efa\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6(QBAFs)\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u591a\u79cd\u667a\u80fd\u4f53\u7c7b\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\u591a\u667a\u80fd\u4f53\u7ec4\u5408\u8bc1\u636e\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "motivation": "\u5c06\u5224\u65ad\u9884\u6d4b\u89c6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u672a\u6765\u4e8b\u4ef6\u7684\u5408\u7406\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bc1\u636e\u6536\u96c6\u548c\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u53ef\u89e3\u91ca\u7684\u9a8c\u8bc1\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u58f0\u660e\u9a8c\u8bc1\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u79cd\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\uff1aArgLLM\u667a\u80fd\u4f53\uff08\u751f\u6210\u548c\u8bc4\u4f30QBAFs\uff09\u3001RbAM\u667a\u80fd\u4f53\uff08\u4ece\u5916\u90e8\u6e90\u8fdb\u884c\u5173\u7cfb\u578b\u8bba\u8bc1\u6316\u6398\u751f\u6210QBAFs\uff09\u3001RAG-ArgLLM\u667a\u80fd\u4f53\uff08\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4ece\u5916\u90e8\u6e90\u83b7\u53d6\u8bba\u8bc1\uff09\u3002\u5728\u6807\u51c6\u5224\u65ad\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c2-3\u4e2a\u667a\u80fd\u4f53\u7684\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ec4\u5408\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u53ef\u4ee5\u6539\u5584\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u89c6\u89d2\u7684\u8bc1\u636e\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5224\u65ad\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u63d0\u4f9b\u4e86\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u4e3a\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u6307\u51fagLLMs\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4eba\u5de5\u7f16\u7801\u5458\u4e14\u6210\u672c\u66f4\u4f4e\uff0c\u4f46\u5b58\u5728\u4e03\u5927\u6311\u6218\u9700\u8981\u89e3\u51b3\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6700\u4f73\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u64ad\u7814\u7a76\u5185\u5bb9\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u89e3\u7801\u9690\u542b\u610f\u4e49\u3001\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e14\u6210\u672c\u4f4e\u5ec9\uff0c\u4f46\u5176\u5728\u65b9\u6cd5\u8bba\u5de5\u5177\u5305\u4e2d\u7684\u6574\u5408\u4ecd\u4e0d\u6210\u719f\uff0c\u9700\u8981\u89e3\u51b3\u5f71\u54cd\u7ed3\u679c\u8d28\u91cf\u7684\u4e03\u5927\u5173\u952e\u6311\u6218\u3002", "method": "\u7efc\u5408\u65b0\u5174\u7814\u7a76\uff0c\u63d0\u51fa\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u6db5\u76d6\u4ee3\u7801\u672c\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u4f18\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u6027\u80fd\u63d0\u5347\u7b49\u4e03\u4e2a\u5173\u952e\u73af\u8282\u3002", "result": "gLLMs\u5728\u4f20\u64ad\u79d1\u5b66\u76f8\u5173\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f17\u5305\u5de5\u4f5c\u8005\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u7f16\u7801\u5458\uff0c\u80fd\u4ee5\u66f4\u5c11\u7684\u65f6\u95f4\u548c\u6210\u672c\u5b8c\u6210\u5de5\u4f5c\uff0c\u5e76\u80fd\u89e3\u7801\u9690\u542b\u610f\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4f7f\u57fa\u4e8egLLM\u7684\u5185\u5bb9\u5206\u6790\u5bf9\u66f4\u5e7f\u6cdb\u7684\u4f20\u64ad\u7814\u7a76\u8005\u66f4\u52a0\u53ef\u53ca\uff0c\u5e76\u786e\u4fdd\u9075\u5b88\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u7814\u7a76\u4f26\u7406\u7b49\u5b66\u79d1\u8d28\u91cf\u6807\u51c6\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027(PCS)\u539f\u5219\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u8fdbLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u6307\u5bfc\uff0c\u9650\u5236\u4e86\u5728\u5608\u6742\u590d\u6742\u73b0\u5b9e\u6570\u636e\u96c6\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8ePCS\u539f\u5219\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\u5904\u7406\u6570\u636e\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u8d1f\u8d23\uff0c\u7ed3\u5408\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\u3002", "result": "\u57289\u4e2a\u4e0d\u540c\u7279\u5f81\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cVDSAgents\u6301\u7eed\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "conclusion": "\u5c06PCS\u539f\u5219\u5d4c\u5165LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u662f\u53ef\u884c\u7684\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u533b\u7597AI\u9762\u5411\u5e73\u5747\u60a3\u8005\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u4e3a\u4e2a\u4f53\u60a3\u8005\u63d0\u4f9b\u66f4\u7cbe\u51c6\u3001\u900f\u660e\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u4f20\u7edf\u533b\u7597AI\u7cfb\u7edf\u901a\u8fc7\u6700\u5c0f\u5316\u5927\u6570\u636e\u96c6\u4e0a\u7684\u9519\u8bef\u6765\u83b7\u5f97\u5f3a\u805a\u5408\u51c6\u786e\u6027\uff0c\u4f46\u5728\u8fb9\u7f18\u75c5\u4f8b\uff08\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u5171\u5b58\u3001\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u79cd\u5e73\u5747\u60a3\u8005\u8c2c\u8bef\u635f\u5bb3\u4e86\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u667a\u80fd\u4f53\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u5f0f\u805a\u7c7b\uff0c\u5171\u4eab\u6a21\u578b\u5e93\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\uff0c\u901a\u8fc7\u534f\u8c03\u5c42\u6743\u8861\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\uff0c\u751f\u6210\u5305\u542b\u98ce\u9669\u4f30\u8ba1\u3001\u7f6e\u4fe1\u533a\u95f4\u3001\u5f02\u5e38\u6807\u5fd7\u548c\u76f8\u5173\u8bc1\u636e\u7684\u51b3\u7b56\u652f\u6301\u5305\u3002", "result": "\u9a8c\u8bc1\u65b9\u6cd5\u4ece\u7fa4\u4f53\u5e73\u5747\u8f6c\u5411\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u4f4e\u5bc6\u5ea6\u533a\u57df\u9519\u8bef\u3001\u5c0f\u6837\u672c\u6821\u51c6\u548c\u98ce\u9669-\u8986\u76d6\u6743\u8861\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u4f7f\u533b\u7597AI\u4e0e\u533b\u5b66\u9996\u8981\u539f\u5219\u4fdd\u6301\u4e00\u81f4\uff1a\u63d0\u4f9b\u900f\u660e\u3001\u516c\u5e73\u4e14\u4ee5\u4e2a\u4f53\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u7684\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3Web\u5e73\u53f0\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u63a8\u7406\u8d28\u91cf\u7684\u53cc\u91cd\u8981\u6c42\u3002", "method": "\u5c06\u5355\u67e5\u8be2\u63a8\u7406\u5206\u89e3\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a\u5173\u952e\u70b9\u751f\u6210\uff08\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684few-shot\u63d0\u793a\u63d0\u53d6\u903b\u8f91\u7ed3\u6784\u5173\u952e\u70b9\uff09\u548c\u5185\u5bb9\u5e76\u884c\u6269\u5c55\uff08\u57fa\u4e8e\u4f9d\u8d56\u56fe\u5e76\u53d1\u6269\u5c55\u5185\u5bb9\uff09\u3002\u5f15\u5165\u6d41\u6c34\u7ebf\u8c03\u5ea6\u673a\u5236\uff0c\u5229\u7528\u4e24\u4e2a\u9636\u6bb5\u7684\u8ba1\u7b97\u7279\u6027\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOrion\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad84.33\u500d\u7684token\u751f\u6210\u901f\u5ea6\u63d0\u5347\u30013.42\u500d\u7684\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e\uff0c\u4ee5\u53ca18.75%\u7684\u63a8\u7406\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "Orion\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5173\u952e\u70b9\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7684\u6548\u7387\u548c\u54c1\u8d28\uff0c\u4e3a\u5b9e\u65f6Web\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT\u3001Claude\u3001DeepSeek\u7b49\uff09\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u75288\u4e2a\u5b9a\u5236\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\uff0c\u6d89\u53ca\u7406\u89e3\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\u3001\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u903b\u8f91\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba\u3002", "method": "\u4f7f\u75288\u4e2a\u5b9a\u5236\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u591a\u4e2aLLMs\uff08GPT\u3001Claude\u3001DeepSeek\u3001Gemini\u7b49\uff09\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u6280\u80fd\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "LLMs\u7684\u7ed3\u679c\u4e0e\u4eba\u7c7b\u8868\u73b0\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660eLLMs\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u6f14\u7ece\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e24\u9636\u6bb5\u7ba1\u9053\uff0c\u901a\u8fc7\u4ea4\u53c9\u4efb\u52a1\u793a\u4f8b\u4f2a\u6807\u8bb0\u5c11\u91cf\u76ee\u6807\u5b9e\u4f8b\uff0c\u7136\u540e\u4f7f\u7528\u56fe\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u6269\u5c55\u6807\u7b7e\uff0c\u51cf\u5c11\u5bf9LLM\u7684\u4f9d\u8d56\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u6536\u96c6\u9ad8\u8d28\u91cf\u793a\u4f8b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6210\u672c\u9ad8\u6602\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u9700\u8981\u51cf\u5c11\u5bf9LLM\u6570\u636e\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u9996\u5148\u5229\u7528\u4ea4\u53c9\u4efb\u52a1\u793a\u4f8b\u63d0\u793aLLM\u4f2a\u6807\u8bb0\u5c11\u91cf\u76ee\u6807\u5b9e\u4f8b\uff0c\u7136\u540e\u5f15\u5165\u56fe\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\u5c06\u6807\u7b7e\u4fe1\u606f\u4f20\u64ad\u5230\u5269\u4f59\u76ee\u6807\u793a\u4f8b\u4e2d\uff0c\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u3002", "result": "\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u4ea4\u53c9\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u65e0LLM\u4f20\u64ad\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u90e8\u7f72\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u6570\u636e\u751f\u547d\u5468\u671f\uff0c\u5c06\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u751f\u6210\u5f0f\u533b\u7597\u7cfb\u7edf\u7684\u57fa\u77f3\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6df1\u5165\u7406\u89e3\u533b\u7597\u4efb\u52a1\u53ca\u5176\u53ef\u5b9e\u73b0\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u6574\u5408\u3002", "method": "\u6784\u5efa\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u4f5c\u4e3a\u57fa\u7840\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6837\u533b\u7597\u6570\u636e\u548c\u77e5\u8bc6\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\uff0c\u91c7\u7528\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u9ad8\u6548\u6570\u636e\u5904\u7406\u7ba1\u9053\u3002", "result": "\u8be5\u751f\u6001\u7cfb\u7edf\u80fd\u591f\u4e3a\u4e0a\u6e38\u6a21\u578b\u7ec4\u4ef6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u540c\u65f6\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u63a8\u7406\u3002", "conclusion": "\u8fd9\u79cd\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u6709\u6548\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u533b\u7597\u4fdd\u5065\u670d\u52a1\u90e8\u7f72\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "FunReason-MT\u662f\u4e00\u4e2a\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883-API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u8fed\u4ee3\u94fe\u6765\u89e3\u51b3\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u6570\u636e\u7684\u590d\u6742\u6027\u6311\u6218\uff0c\u5728BFCL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff08\u5982\u968f\u673a\u73af\u5883\u91c7\u6837\u6216\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\uff09\u4e0d\u8db3\u4ee5\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5b58\u5728\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u3001\u5de5\u5177\u67b6\u6784\u9694\u79bb\u548c\u591a\u8f6e\u903b\u8f91\u4f9d\u8d56\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "FunReason-MT\u91c7\u7528\u4e09\u79cd\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u73af\u5883-API\u56fe\u4ea4\u4e92\u6536\u96c6\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff1b2\uff09\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u7b80\u5316\u56f0\u96be\u67e5\u8be2\u6784\u5efa\uff1b3\uff09\u5f15\u5bfc\u8fed\u4ee3\u94fe\u751f\u6210\u590d\u6742\u601d\u7ef4\u94fe\u3002", "result": "\u5728Berkeley\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\uff08BFCLv3\uff09\u4e0a\uff0c\u57fa\u4e8eFunReason-MT\u751f\u6210\u6570\u636e\u6784\u5efa\u76844B\u6a21\u578b\u5728\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u5927\u591a\u6570\u95ed\u6e90\u6a21\u578b\u3002\u5728BFCLv4\u4e0a\u7684\u8fdb\u4e00\u6b65\u6027\u80fd\u6539\u8fdb\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "FunReason-MT\u4e3a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u6570\u636e\u6e90\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u7684\u6570\u636e\u5408\u6210\u6311\u6218\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5206\u6790\u4e86\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u75c5\u5bb3\u7cbe\u51c6\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u6587\u732e\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f5c\u7269\u75c5\u5bb3\u7cbe\u51c6\u7ba1\u7406\u4ece\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u53d1\u5c55\u5230\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u7279\u5f81\u5b66\u4e60\u3002\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff0c\u4e3a\u75c5\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u5904\u7406\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7b5b\u9009\u7ea640\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u5206\u6790\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662fLLMs\u548cVLMs\uff09\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a\u57fa\u7840\u6a21\u578b\u5e94\u7528\u57282023-24\u5e74\u5feb\u901f\u589e\u957f\uff1bVLMs\u53d1\u5c55\u901f\u5ea6\u8fdc\u8d85LLMs\uff1bRL\u548cAL\u5728\u667a\u80fd\u55b7\u6d12\u4e2d\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff1b\u6570\u5b57\u5b6a\u751f\u7ed3\u5408RL\u53ef\u6a21\u62df\u9776\u5411\u55b7\u6d12\uff1b\u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u662f\u5173\u952e\u6311\u6218\uff1b\u4eba\u673a\u534f\u4f5c\u4ecd\u6709\u5c40\u9650\u3002", "conclusion": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3\u4f5c\u7269\u75c5\u5bb3\u7cbe\u51c6\u7ba1\u7406\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5173\u6ce8\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u548c\u4eba\u673a\u534f\u4f5c\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "OrchDAG\u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u5177\u6709\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u7528\u4e8e\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u5ffd\u7565\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u8bad\u7ec3\u667a\u80fd\u4f53\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "method": "\u5f15\u5165OrchDAG\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff1b\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u589e\u5f3aRLVR\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u4f46\u53ef\u89e3\u51b3\u7684\u57fa\u51c6\uff0c\u6240\u63d0\u51fa\u7684\u5956\u52b1\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u65f6\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u56fe\u7ed3\u6784\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u793a\u4f8b\u5de5\u4ef6\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u5bf9\u9f50\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u548c\u7a0b\u5e8f\u77e5\u8bc6\u3002", "motivation": "\u4e3a\u4e86\u63ed\u793a\u548c\u5229\u7528\u5de5\u5177\u4e0e\u6587\u6863\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u6539\u8fdb\u793a\u4f8b\u5de5\u4ef6\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u4ece\u5de5\u5177\u6a21\u5f0f\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ece\u5185\u90e8\u6587\u6863\u548cSOP\u6784\u5efa\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u5c06\u4e24\u8005\u878d\u5408\uff0c\u91c7\u7528\u6df1\u5ea6-\u7a00\u758f\u96c6\u6210\u7b56\u7565\u5bf9\u9f50\u7ed3\u6784\u5de5\u5177\u4f9d\u8d56\u4e0e\u7a0b\u5e8f\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u6539\u8fdb\u8ba1\u5212\u751f\u6210\u3002", "conclusion": "\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u8fde\u63a5\u5bf9\u4e8e\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u548c\u89c4\u5212\u5177\u6709\u663e\u8457\u76ca\u5904\u3002"}}
