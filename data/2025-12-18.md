<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ是一个针对中等规模语言模型（3B-32B参数）在消费级GPU上训练的端到端CUDA/C++实现，通过优化内存和通信瓶颈，能在单张16GB显卡上训练7B模型，或在4张RTX 4090上训练32B模型。


<details>
  <summary>Details</summary>
Motivation: 针对消费级GPU内存有限、通信速度慢的特点，为中等规模语言模型训练提供经济高效的解决方案，使普通研究者和开发者能够在相对廉价的硬件上进行模型训练。

Method: 采用激活检查点、卸载技术和基于复制引擎的集合通信等优化技术，实现标准的8位训练流程，不依赖额外的算法近似，保持约50%的FLOP利用率。

Result: 能够在单张16GB中端游戏显卡上训练或微调7B模型，在配备4张RTX 4090的工作站上训练32B模型，效率可与在更昂贵的云级GPU上运行的生产级系统相媲美。

Conclusion: LLMQ展示了在消费级GPU上高效训练中等规模语言模型的可行性，为资源受限的研究者和开发者提供了经济实惠的解决方案，降低了语言模型训练的门槛。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [2] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 该研究探索了GPU上Bloom过滤器的优化设计空间，通过向量化、线程协作和计算延迟三个维度的优化，在保持高精度的同时实现了高吞吐量，性能显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: Bloom过滤器是近似成员查询的基础数据结构，GPU因其大规模线程级并行性和高带宽内存，是加速Bloom过滤器的理想平台。尽管CPU优化实现已有深入研究，但GPU设计仍未被充分探索，需要填补这一空白。

Method: 研究从三个维度探索GPU上的设计空间：向量化、线程协作和计算延迟。通过分析硬件对不同参数配置的响应，并将这些观察与性能趋势关联，开发了优化的GPU实现。

Result: 优化设计突破了传统速度与精度之间的权衡，在保持高精度配置优越准确性的同时，实现了通常仅限于高误差变体的吞吐量。在相同错误率下，批量过滤器查找性能提升11.35倍，构建性能提升15.4倍，在B200 GPU上达到实用速度极限的92%以上。

Conclusion: 该研究填补了GPU上Bloom过滤器优化设计的空白，提出的方法在保持高精度的同时实现了显著的性能提升，提供了模块化的CUDA/C++实现，并将开源发布。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [3] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX系统通过动态重批处理技术优化早期退出LLM推理，在保持输出质量的同时提升吞吐量2-12%，完全消除非自愿退出


<details>
  <summary>Details</summary>
Motivation: 传统批处理框架不适合早期退出LLM架构，因为批处理中所有请求可能不会同时准备好退出。现有解决方案要么强制统一决策而错过早期退出机会，要么通过强制提前退出降低输出质量

Method: 提出动态重批处理解决方案：在早期退出点动态重组批次。满足退出条件的请求立即处理，继续的请求保留在缓冲区，重新分组为新批次并转发到更深层。实现DREX系统，包含两个关键优化：1) 无复制重批处理缓冲区避免物理数据移动；2) EE和SLA感知调度器分析预测重批操作是否有利。同时高效处理跳过层的缺失KV缓存

Result: 评估显示DREX相比基线方法提升吞吐量2-12%，同时保持输出质量。关键的是，DREX完全消除了非自愿退出，为保持EE模型预期的输出质量提供了关键保证

Conclusion: DREX通过动态重批处理有效解决了早期退出LLM的批处理挑战，在保持模型质量的同时显著提升推理效率，为实际部署提供了实用解决方案

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [4] [Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis](https://arxiv.org/abs/2512.15398)
*Zanxiang He,Meng Li,Liyun Shi,Weiye Daia,Liming Nie*

Main category: cs.MA

TL;DR: 提出了Mapis，首个基于知识的多智能体框架，专门用于基于指南的多囊卵巢综合征诊断，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: PCOS影响10%育龄女性，现有机器学习方法依赖大量标注数据且缺乏可解释性，医疗多智能体系统缺乏领域专业知识，需要专门针对PCOS诊断的框架

Method: 将2023国际指南转化为结构化协作工作流，模拟临床诊断过程，包含妇科内分泌智能体、放射学智能体协作验证纳入标准，排除智能体严格排除其他病因，并构建PCOS知识图谱确保基于证据的决策

Result: 在公共基准和临床数据集上，Mapis显著优于9种基线方法，在临床数据集上比传统机器学习模型准确率提高13.56%，比单智能体提高6.55%，比先前医疗多智能体系统提高7.05%

Conclusion: Mapis是首个专门针对指南的PCOS诊断知识驱动多智能体框架，通过模拟临床工作流程和整合领域知识，实现了更准确、可解释的诊断

Abstract: Polycystic Ovary Syndrome (PCOS) constitutes a significant public health issue affecting 10% of reproductive-aged women, highlighting the critical importance of developing effective diagnostic tools. Previous machine learning and deep learning detection tools are constrained by their reliance on large-scale labeled data and an lack of interpretability. Although multi-agent systems have demonstrated robust capabilities, the potential of such systems for PCOS detection remains largely unexplored. Existing medical multi-agent frameworks are predominantly designed for general medical tasks, suffering from insufficient domain integration and a lack of specific domain knowledge. To address these challenges, we propose Mapis, the first knowledge-grounded multi-agent framework explicitly designed for guideline-based PCOS diagnosis. Specifically, it built upon the 2023 International Guideline into a structured collaborative workflow that simulates the clinical diagnostic process. It decouples complex diagnostic tasks across specialized agents: a gynecological endocrine agent and a radiology agent collaborative to verify inclusion criteria, while an exclusion agent strictly rules out other causes. Furthermore, we construct a comprehensive PCOS knowledge graph to ensure verifiable, evidence-based decision-making. Extensive experiments on public benchmarks and specialized clinical datasets, benchmarking against nine diverse baselines, demonstrate that Mapis significantly outperforms competitive methods. On the clinical dataset, it surpasses traditional machine learning models by 13.56%, single-agent by 6.55%, and previous medical multi-agent systems by 7.05% in Accuracy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文提出将Transformer的自注意力机制和残差流解释为近似的向量符号架构(VSA)，用代数视角分析语言模型的推理行为及其失败模式，并提出VSA启发的架构改进方案。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型展现出类似推理的行为，但在需要稳定符号操作的任务上仍然脆弱。作者希望通过向量符号架构的视角统一理解这些现象，解释模型内部机制与推理行为的关系。

Method: 将自注意力机制解释为VSA：查询和键定义角色空间，值编码填充物，注意力权重执行软解绑，残差连接实现多个绑定结构的叠加。基于此视角分析思维链、程序推理和记忆增强工具使用，并提出VSA启发的架构偏置（如显式绑定/解绑头、超维记忆层）和训练目标。

Result: 建立了Transformer内部机制与向量符号计算之间的对应关系，解释了变量混淆和逻辑相关提示不一致等特征性失败模式，提出了促进角色-填充物分离和鲁棒叠加的训练目标。

Conclusion: 将注意力视为软向量符号计算为构建更可解释和逻辑可靠的推理系统提供了原则性途径，并提出了衡量"VSA相似性"和逻辑组合性的度量标准以及理论和架构上的开放问题。

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [6] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 论文提出了一种在知识图谱不完整情况下的评估方法，并开发了自适应图推理智能体（GR-Agent）来解决知识图谱问答中的推理能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答（KGQA）基准测试通常假设知识图谱是完整的，这忽略了现实中知识图谱往往不完整的事实。当直接支持的三元组缺失时，需要从现有事实中进行推理，而现有方法在这种情况下的推理能力有限。

Method: 1. 提出构建知识图谱不完整性基准的方法论：移除直接支持的三元组，同时确保保留推断答案所需的替代推理路径。2. 开发自适应图推理智能体（GR-Agent）：从知识图谱构建交互环境，将KGQA形式化为智能体与环境交互；使用包含图推理工具的动作空间，维护潜在支持推理证据的记忆。

Result: 实验表明：1. 现有方法在不完整性情况下性能持续下降，突显其推理能力有限。2. GR-Agent在完整和不完整设置下均优于非训练基线方法，与基于训练的方法性能相当。

Conclusion: 知识图谱不完整性是KGQA评估的重要现实因素，现有方法在此场景下推理能力不足。提出的GR-Agent通过智能体与环境交互的框架，有效提升了在不完整知识图谱上的推理能力，为KGQA研究提供了新的方向。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [7] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 该研究通过结构化配置知识注入，将LLM生成Terraform代码的整体成功率从27.1%提升至62.6%，但发现意图对齐存在瓶颈，揭示了"正确性-一致性鸿沟"。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成正确且意图对齐的基础设施即代码方面成功率较低，特别是在Terraform代码生成中表现不佳，需要改进方法。

Method: 1. 增强IaC-Eval基准测试，加入云仿真和自动错误分析；2. 开发LLM辅助IaC代码生成的错误分类法；3. 实施从朴素检索增强生成到图RAG的知识注入技术，包括图组件的语义丰富化和资源间依赖关系建模。

Result: 基线LLM性能较差（整体成功率27.1%），注入结构化配置知识后，技术验证成功率提升至75.3%，整体成功率提升至62.6%。但意图对齐出现平台期。

Conclusion: 虽然结构化知识注入显著提高了技术正确性，但意图对齐存在瓶颈，揭示了"正确性-一致性鸿沟"：LLM可以成为熟练的"编码者"，但在满足细微用户意图方面仍受限于作为"架构师"的能力。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [8] [Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study](https://arxiv.org/abs/2512.15044)
*Wenwen Xie,Geng Sun,Ruichen Zhang,Xuejie Liu,Yinqiu Liu,Jiacheng Wang,Dusit Niyato,Ping Zhang*

Main category: cs.AI

TL;DR: 本文探讨了智能体人工智能在集成感知与通信系统中的应用价值，提出了一个基于智能体AI的ISAC框架，并通过案例研究验证了其在优化ISAC性能方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着无线环境日益动态复杂，ISAC系统需要更智能的处理和更自主的操作来保持效率和适应性。智能体AI通过实现动态环境中的持续感知-推理-行动循环，为ISAC系统提供了可行的解决方案。

Method: 首先全面回顾了智能体AI和ISAC系统的关键特性；其次展示了ISAC系统的几种常见优化方法，并突出了基于生成式AI的智能体AI的显著优势；第三提出了一个新颖的智能体ISAC框架，并通过案例研究验证其优越性。

Result: 提出的智能体ISAC框架在优化ISAC性能方面表现出优越性，案例研究验证了该框架的有效性。

Conclusion: 智能体AI为ISAC系统提供了实现智能、自主和高效操作的可行解决方案，本文明确了基于智能体AI的ISAC系统未来的研究方向。

Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.

</details>


### [9] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER是一个受人类分层推理启发的弹性推理框架，通过动态选择最适合每个查询的推理策略来平衡LLM推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略主要依赖LLM自身的快速或慢速模式（如o1思维），难以在不同难度查询间平衡推理效率和准确性。

Method: 1) 评估查询复杂度并分配到预定义层级；2) 将策略选择建模为马尔可夫决策过程，用强化学习训练CogER-Agent；3) 引入认知工具辅助推理，使LLM能在思维链中自主调用外部工具。

Result: CogER在In-Domain任务上实现至少13%的相对平均精确匹配提升，在Out-of-Domain任务上实现8%的相对增益，优于最先进的测试时缩放方法。

Conclusion: CogER通过动态策略选择和工具集成，有效解决了LLM推理中效率与准确性的平衡问题，为不同难度查询提供了弹性推理解决方案。

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [10] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: 提出基于聚类的变量排序框架，通过将变量分组减少动态排序的搜索空间，在MWISP问题上显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 松弛决策图在离散优化中提供对偶边界，但其质量严重依赖变量排序和合并决策。动态变量排序虽然能收紧边界，但在整个变量集上评估会产生计算开销，需要平衡这种权衡

Method: 引入基于聚类的变量排序框架：先将变量分区成簇，然后利用这种结构分解指导排序过程。研究两种策略：1) Cluster-to-Cluster：使用问题特定聚合标准（如MWISP中的累积顶点权重）顺序处理簇；2) Pick-and-Sort：迭代地从每个簇中选择并排序代表性变量，平衡局部多样性与启发式指导。基于MWISP中DD大小增长的理论结果，提出了两种设置簇数量的策略

Result: 在MWISP基准实例上，将策略嵌入基于DD的分支定界算法中，相比标准动态变量排序基线，提出的方法持续降低了计算成本

Conclusion: 基于聚类的变量排序框架有效减少了动态排序的搜索空间，在MWISP问题上显著提高了计算效率，为离散优化中的决策图编译提供了更高效的变量排序方法

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [11] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: 该研究通过分析2025年韩国高考地球科学I试题，评估了GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro等多模态大语言模型的科学推理能力和认知局限，揭示了模型在感知-认知差距、计算-概念化差异和过程幻觉等方面的缺陷，为设计"抗AI问题"提供依据。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI完成作业的现象日益普遍，学术诚信和评估有效性受到威胁。研究旨在深入分析先进大语言模型的多模态科学推理能力及其认知局限，为应对AI在课程作业中的未经授权使用提供解决方案。

Method: 使用2025年韩国高考地球科学I试题，设计三种实验条件（整页输入、单项输入和优化多模态输入），定量评估GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro在不同数据结构下的表现，并进行定性分析。

Result: 非结构化输入导致性能显著下降；即使在优化条件下，模型仍表现出基本推理缺陷。定性分析发现"感知错误"占主导地位，存在"感知-认知差距"（模型能识别视觉数据但无法解释图表中的符号意义）、"计算-概念化差异"（能执行计算但无法应用基础科学概念）和"过程幻觉"（跳过视觉验证而依赖无根据的背景知识）。

Conclusion: 通过针对AI的特定认知弱点（如感知与认知之间的差距），教育者可以设计"抗AI问题"来区分真实的学生能力与AI生成的回答，从而确保评估的公平性。

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [12] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: 该论文旨在通过定性空间关系提升大语言模型为行人提供路线指引的能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在提供行人路线指引方面存在局限性，特别是在使用定性空间关系（如上/下、左/右、前/后等）进行精确导航方面能力不足，需要改进模型的空间推理和指令生成能力

Method: 通过引入定性空间关系来增强大语言模型，可能包括：1）构建包含定性空间关系的训练数据集；2）设计专门的模型架构或微调策略；3）集成空间推理模块；4）开发评估行人导航指令质量的指标

Result: 改进后的大语言模型能够生成更准确、更符合人类认知的行人导航指令，使用定性空间关系提高了路线描述的自然性和实用性

Conclusion: 通过集成定性空间关系，大语言模型在行人导航任务中的表现得到显著提升，为智能导航系统的发展提供了新的技术路径

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [13] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: 提出一个通过自我对弈AI游戏扩展人类专家游戏数据库的通用框架，提高多玩家纸牌游戏早期决策的预测准确性


<details>
  <summary>Details</summary>
Motivation: 在多玩家纸牌游戏（如Skat或Bridge）中，早期阶段（如叫牌、游戏选择和初始牌选择）对游戏成功至关重要，但当前计算限制下这些决策主要依赖人类专家游戏的统计信息，存在数据有限的问题

Method: 提出通用的自举外部学习框架，通过AI自我对弈生成数百万游戏扩展人类游戏数据库，使用完美特征哈希函数处理压缩表，实现持续自我改进的纸牌游戏引擎

Result: 在Skat游戏中的案例研究表明，该自动化方法可以有效支持游戏中的各种决策

Conclusion: 通过结合人类专家游戏和AI自我对弈数据，可以显著提高多玩家纸牌游戏早期决策的预测准确性，实现自我改进的游戏引擎

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [14] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: 本文提出了一种结合ASP和MILP的集成框架，用于处理城市空中交通中动态操作需求和模糊调度请求的调度问题。


<details>
  <summary>Details</summary>
Motivation: 由于城市空中交通系统资源受限，高效的垂直起降机场调度变得至关重要。同时需要处理动态操作需求和人类模糊的重新调度请求，这需要系统能够解释模糊的用户意图并提供透明的决策支持。

Method: 采用混合整数线性规划处理资源受限项目调度问题，结合三值逻辑解释模糊用户意图，使用决策树和答案集编程，提出ASP与MILP集成的系统框架。

Result: 开发了一个能够优化调度并透明支持人类输入的集成框架，为可解释、自适应的UAM调度提供了鲁棒结构。

Conclusion: 该集成系统能够有效处理城市空中交通中的动态调度需求，通过结合ASP和MILP实现了可解释、自适应且支持人类输入的调度优化。

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [15] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math是一个包含750万条解题轨迹的大规模数学推理数据集，整合了AoPS竞赛题和StackExchange-Math社区问题，支持高中低三种推理模式，并包含Python工具集成推理版本，显著提升了数学推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集在推理风格多样性、长形式解题轨迹和工具集成方面存在局限，需要更高质量、更大规模的监督数据来提升数学推理模型的性能。

Method: 利用GPT-OSS-120B的多模式生成能力，整合85K AoPS竞赛问题和262K StackExchange-Math社区问题，生成包含高中低三种推理模式的750万条解题轨迹，同时提供带Python工具集成推理的版本，并开发了顺序分桶策略来加速长上下文训练。

Result: Nemotron-Math在匹配的AoPS问题上持续优于原始OpenMathReasoning，整合StackExchange-Math显著提高了鲁棒性和泛化能力（特别是在HLE-Math上），同时保持了数学竞赛基准的准确性。在AIME 2024和2025上使用Python TIR实现了100% maj@16准确率。

Conclusion: Nemotron-Math通过提供大规模、多样化的数学推理数据，结合有效的训练优化策略，实现了最先进的数学推理性能，为高质量数学推理模型的训练提供了重要资源。

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [16] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: 论文提出了一个基于场景的科学发现评估框架，用于评估大语言模型在真实科学研究中的能力，发现当前模型在科学发现方面存在系统性弱点，距离通用科学"超级智能"还很遥远。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试主要评估去语境化的知识，而忽略了驱动科学发现的迭代推理、假设生成和观察解释等关键能力，需要开发更贴近真实科学研究过程的评估框架。

Method: 引入基于场景的基准测试框架，由领域专家定义真实研究项目并分解为模块化研究场景，从这些场景中采样经过验证的问题。框架在两个层面评估模型：(i) 场景相关问题的准确性；(ii) 项目层面表现，包括提出可测试假设、设计实验/模拟、解释结果等能力。

Result: 应用该框架评估最先进的大语言模型发现：相对于通用科学基准存在一致的性能差距；模型规模和推理能力的扩展收益递减；不同提供商的最优模型存在系统性弱点；研究场景中性能变化大，导致不同科学发现项目的最佳模型选择会变化。

Conclusion: 当前所有大语言模型距离通用科学"超级智能"还很遥远，但已在多种科学发现项目中显示出潜力，特别是在构成场景得分较低的情况下，突显了引导探索和偶然发现在科学发现中的作用。该框架为LLMs的科学发现相关评估提供了可复现的基准，并为其向科学发现方向发展指明了实用路径。

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [17] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: 论文提出了一个决策理论框架，用于在不确定条件下判断何时应将决策委托给AI系统，强调需要平衡价值对齐、认知准确性和行动范围三个因素。


<details>
  <summary>Details</summary>
Motivation: 现有价值对齐文献主要关注如何塑造AI价值观，但较少研究在不确定条件下如何判断不完美的对齐是否足以证明委托决策的合理性。需要建立原则性方法来决定何时AI在特定情境下足够对齐。

Method: 引入正式的决策理论框架，精确分析委托决策中的权衡关系，考虑委托者对AI价值对齐、认知准确性和行动范围的不确定性。开发了新的评分框架来量化这种事前决策。

Result: 分析揭示了两种委托情景的明显区别：1）通用委托（信任AI处理任何问题）需要近乎完美的价值对齐和完全的认知信任，实践中很少满足；2）情境特定委托即使在显著不对齐的情况下也可能是最优的，因为AI的更高准确性或更广行动范围可能提供更好的整体决策问题。

Conclusion: 研究提供了确定AI在给定情境下是否足够对齐的原则性方法，将重点从实现完美对齐转向在不确定性下管理委托的风险和回报。情境特定委托可以在显著不对齐的情况下保持理性。

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


### [18] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC框架将推理与自我批判在每一步交织，通过混合强化学习优化推理质量和自我评估，在数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型通常将推理与验证分离：要么生成推理而不进行自我检查，要么依赖外部验证器事后检测错误。前者缺乏即时反馈，后者增加系统复杂性并阻碍同步学习。受人类批判性思维启发，需要一种统一框架。

Method: 提出Stepwise Think-Critique (STC)框架，在单个模型内每一步交织推理和自我批判。使用混合强化学习目标训练，结合推理奖励和批判一致性奖励，共同优化推理质量和自我评估。

Result: 在数学推理基准测试中，STC展现出强大的批判性思维能力，并产生更可解释的推理轨迹。

Conclusion: STC代表了向具有内置批判性思维的大语言模型迈出的一步，通过统一框架实现了推理与自我批判的有机结合。

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [19] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE框架通过构建有向属性图来改进LLM的上下文归因，量化每个生成内容如何受到提示和先前生成内容的影响，相比现有方法提升了40%的忠实度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能力强大，但其推理过程不透明，存在安全和信任问题。现有的上下文归因方法只直接关联生成标记和提示，忽略了生成过程中的代际影响，导致解释不完整。

Method: 提出CAGE框架，构建有向属性图来量化每个生成内容如何受到提示和所有先前生成内容的影响。该图保持因果关系和行随机性，通过沿图中路径边缘化中间贡献来计算上下文归因。

Result: 在多个模型、数据集、指标和方法上，CAGE显著提高了上下文归因的忠实度，平均提升高达40%。

Conclusion: CAGE框架通过考虑生成过程中的代际影响，提供了更完整和忠实的LLM行为解释，有助于提高模型的安全性和可信度。

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>
