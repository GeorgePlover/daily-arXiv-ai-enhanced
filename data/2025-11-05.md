<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 13]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文对已部署和正在使用的工作负载调度器解决方案进行分析，提出了基于架构和设计的层次化分类法，特别关注影响吞吐量和可扩展性的关键设计因素，并以Google的Borg系统为重点研究对象。


<details>
  <summary>Details</summary>
Motivation: 现有的分类法不足以充分描述工作负载调度器的关键设计特征，需要建立更关注吞吐量和可扩展性影响因素的分类体系，以更好地理解这些系统的架构演进和改进。

Method: 采用层次化分类方法，基于架构和设计特征对工作负载调度器进行分组分析，特别关注影响系统性能的关键设计因素和渐进式改进。

Result: 建立了新的工作负载调度器分类法，识别了影响吞吐量和可扩展性的关键设计因素，并详细分析了Google Borg等先进系统的架构特点。

Conclusion: 提出的分类法能够更好地描述工作负载调度器的设计特征，为理解系统架构演进和性能优化提供了有价值的框架，Google Borg代表了这类系统的先进水平。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [2] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 本概念设计报告介绍了德国达姆施塔特FAIR研究设施的计算基础设施规划，涵盖2028年起的各阶段，目标是构建一个联邦式、集中协调的基础设施，满足多样化研究需求并应对未来数据挑战。


<details>
  <summary>Details</summary>
Motivation: 为FAIR研究设施的各种研究团队提供计算需求支持，建立能够应对未来数据挑战的可扩展、灵活的计算和存储基础设施。

Method: 提出联邦式、集中协调的计算模型架构，包括开放数据、软件和服务政策，涵盖从2028年"首批科学（加）"阶段到模块化启动版本的各个时期。

Result: 制定了全面的计算基础设施规划，包括研究需求分析、计算存储政策、开放数据策略等，为FAIR设施的未来发展提供了系统性的技术框架。

Conclusion: 成功设计了一个能够服务多样化研究领域、具备足够可扩展性和灵活性的计算基础设施模型，为FAIR研究设施的长期发展奠定了坚实基础。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [3] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 云计算已成为AI/ML领域的主要创新驱动力，但当前的资源成本模型不适合科学计算需求，可能导致科学工作负载在不适合的环境中运行。


<details>
  <summary>Details</summary>
Motivation: 分析云计算成本模型从科学计算向AI/ML主导转变的历史演变，探讨这种转变对科学计算的影响和可能的未来发展方向。

Method: 通过历史回顾和趋势分析，讨论云计算成本模型的演变过程及其对科学计算的影响。

Result: 发现AI/ML需求主导的云计算创新导致资源模型不再适合科学计算需求，资源争用可能使科学工作负载在不适合的环境中运行。

Conclusion: 需要重新审视和调整云计算成本模型，以确保科学发现和研究的持续支持，避免科学计算被迫在不适合的环境中运行。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [4] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning系统研究了在边缘GPU上部署推理型大语言模型的延迟-精度权衡，通过评估不同架构、模型大小、token压缩技术和测试时扩展方法，为边缘部署提供优化指导。


<details>
  <summary>Details</summary>
Motivation: 边缘智能范式在自主系统中需求增长，但边缘GPU部署推理型LLM面临严格延迟约束和有限计算资源的挑战，缺乏关于架构选择、模型大小、token预算分配和测试时扩展策略组合的指导。

Method: 系统量化不同LLM架构和模型大小的延迟-精度权衡；评估基于提示和模型调优的推理token长度压缩技术；分析不同并行度的测试时扩展方法。

Result: 通过全面分析，EdgeReasoning绘制了可实现的精度-延迟配置的帕累托前沿，为推理型LLM的边缘部署提供系统指导。

Conclusion: 该研究为在严格延迟预算下优化边缘GPU上推理型LLM的部署提供了系统的方法论和配置指导，帮助开发者在资源受限环境中做出最佳设计决策。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [5] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文提出了多核处理器结构分析和效率指标评估模型，包括可靠性、容错性、生存性和灵活性评估，使用逻辑概率方法开发了多种评估模型，并分析了双核和四核处理器的结构分析结果。


<details>
  <summary>Details</summary>
Motivation: 开发多核处理器结构分析和效率指标评估的模型，以评估具有可变结构和多功能核心的多核处理器的性能指标。

Method: 使用逻辑概率方法开发了可靠性、容错性评估模型，最短路径、灵活性和性能条件的逻辑概率模型，以及考虑所有可能性能状态的可靠性、容错性和寿命估计模型。

Result: 提出了双核和四核处理器的结构分析结果，并展示了多核处理器效率指标的增长趋势。

Conclusion: 通过逻辑概率方法成功开发了多核处理器结构分析和效率指标评估的综合模型，为多核处理器性能优化提供了理论支持。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [6] [Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions](https://arxiv.org/abs/2511.01888)
*Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Roadrunner是一个sidecar shim，通过零拷贝和免序列化数据传输，显著提升WebAssembly无服务器函数的通信性能。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算中函数间数据传输需要序列化/反序列化，导致多副本拷贝和上下文切换，增加延迟和资源消耗。

Method: 使用sidecar shim映射函数内存，通过专用虚拟数据管道移动数据，绕过序列化/反序列化过程，实现近零拷贝数据传输。

Result: 函数间通信延迟降低44%-89%，97%的数据传输消除了序列化开销，吞吐量相比现有WebAssembly无服务器函数提升69倍。

Conclusion: Roadrunner通过近零拷贝和免序列化数据传输，显著改善了WebAssembly无服务器函数的性能表现。

Abstract: Serverless computing provides infrastructure management and elastic
auto-scaling, therefore reducing operational overhead. By design serverless
functions are stateless, which means they typically leverage external remote
services to store and exchange data. Transferring data over a network typically
involves serialization and deserialization. These operations usually require
multiple data copies and transitions between user and kernel space, resulting
in overhead from context switching and memory allocation, contributing
significantly to increased latency and resource consumption. To address these
issues, we present Roadrunner, a sidecar shim that enables near-zero copy and
serialization-free data transfer between WebAssembly-based serverless
functions. Roadrunner reduces the multiple copies between user space and kernel
space by mapping the function memory and moving the data along a dedicated
virtual data hose, bypassing the costly processes of serialization and
deserialization. This approach reduces data movement overhead and context
switching, achieving near-native latency performance for WebAssembly-based
serverless functions. Our experimental results demonstrate that Roadrunner
significantly improves the inter-function communication latency from 44% up to
89%, reducing the serialization overhead in 97% of data transfer, and
increasing throughput by 69 times compared to state-of-the-art
WebAssembly-based serverless functions.

</details>


### [7] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLRI是一种基于记忆化技术优化ADMM-FFT算法的迭代方法，通过替换耗时的FFT操作，显著提升了计算性能和内存效率，能够处理更大规模的数据重建问题。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT算法在层析成像重建中具有高精度，但存在计算时间过长和内存消耗过大的问题，限制了其在大规模问题中的应用。

Method: 利用记忆化技术替换重复的FFT操作，引入一系列技术确保性能提升和可扩展性，采用变量卸载技术节省CPU内存，并在节点内和跨节点间扩展GPU使用。

Result: 成功将ADMM-FFT扩展到2Kx2Kx2K的输入问题，这是层析成像重建中ADMM-FFT解决方案处理过的最大规模问题；相比原始ADMM-FFT，平均性能提升52.8%，最高可达65.4%。

Conclusion: mLRI方法有效解决了ADMM-FFT算法的计算效率和内存限制问题，使其能够处理更大规模的重建任务，同时显著提升了性能表现。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [8] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 该论文分析了五个主要PoS区块链的地理空间去中心化问题，发现少数地理区域主导了共识投票权，并提出了地理空间感知PoS（GPoS）解决方案来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 地理空间去中心化对区块链的监管弹性、鲁棒性和公平性至关重要，但现有PoS区块链存在少数地理区域主导共识投票权的问题。

Method: 提出了地理空间感知PoS（GPoS），将地理空间多样性与基于权益的投票权相结合，并在HotStuff和CometBFT等BFT协议中进行了实验评估。

Result: 实验评估显示，通过特征向量中心性的基尼系数衡量，地理空间去中心化平均改善了45%，同时对共识性能的开销最小。

Conclusion: GPoS能够显著改善地理空间去中心化，同时在实验中仅产生最小的性能开销。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [9] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 本文提出超越传统BSP模型的分布式GPU执行方法，通过细粒度编程模式消除三大性能税，在LLM关键内核上实现10-20%的端到端延迟加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，分布式GPU执行中的传统BSP模型存在显著性能低效问题，需要新的执行范式来解决瓶颈。

Method: 利用Iris for Triton等库提供的核内通信原语，设计新颖的细粒度编程模式，创建直接的瓦片级生产者-消费者流水线，用细粒度数据流同步替代全局屏障。

Result: 在从基础的All-Gather+矩阵乘法到复杂的Flash Decode算法等关键内核上，相比基于BSP的方法实现了10-20%的端到端延迟加速。

Conclusion: 建立了一个更可编程和高效的分布式LLM工作负载范式，通过消除三大性能税显著提升了分布式GPU执行的性能。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [10] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 提出了一种基于算子级别的自动扩缩容框架，通过细粒度资源分配优化大模型推理服务，相比传统模型级方法显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型服务方案采用静态资源配置或模型级自动扩缩容，将模型视为整体进行资源管理，导致在动态推理流量下性能下降或资源利用率低。

Method: 通过分析生成式模型的内部结构（由相互连接的算子组成），发现算子在计算和内存需求上具有异质性，提出算子级自动扩缩容框架，基于单个算子特征进行扩缩容、批处理和放置优化。

Result: 在生产规模流量测试中，该方法在保持SLO的前提下减少40%GPU和35%能耗，或在固定资源下实现1.6倍吞吐量提升和5%能耗降低。

Conclusion: 算子而非模型是扩展大型生成工作负载的更有效单元，细粒度资源管理能显著提升大模型服务效率。

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [11] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出了两种新颖的调度算法，用于优化LQCD模拟中的相关函数计算，通过重新排序张量收缩操作来提高时间局部性和减少内存使用。


<details>
  <summary>Details</summary>
Motivation: LQCD模拟中的相关函数计算涉及大量二进制批量张量收缩，每个张量可能占用数百MB内存。在GPU加速器上执行这些收缩面临调度挑战，需要优化张量重用和数据传输。

Method: 提出了两种快速调度算法，利用应用特定特征（如二进制收缩和收缩树内的局部性）来重新排序收缩操作，通过输入/中间张量重用增加时间局部性，优化最小化峰值内存的目标。

Result: 调度器实现了高达2.1倍的峰值内存改进，反映在高达4.2倍的驱逐减少、高达1.8倍的数据传输减少，以及高达1.9倍的相关函数计算时间加速。

Conclusion: 这些调度算法被集成到LQCD分析软件套件Redstar中，显著改善了求解时间，为LQCD模拟中的张量收缩计算提供了有效的优化方案。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [12] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文比较了四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）在科学计算应用中的表现，发现在Polaris超级计算机上不同框架性能差异显著，OCCA在小规模验证问题中表现最佳但可扩展性受限，OpenMP在结构化网格模拟中表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算在百亿亿次时代需要跨硬件平台的高性能计算框架，以实现代码的最小修改和高效执行。

Method: 使用分布式内存方法和硬件加速，通过四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）测试了两个代表性科学计算应用：N体模拟和结构化网格模拟，实验在Polaris超级计算机的单个节点上使用四个NVIDIA A100 GPU进行。

Result: 实验结果显示框架间性能差异显著：OCCA在小规模验证问题中执行时间最快（可能由于JIT编译），但其缺乏优化的归约算法可能限制大规模模拟的可扩展性；OpenMP在结构化网格模拟中表现最差，可能由于节点间数据同步和通信效率低下。

Conclusion: 需要进一步优化以最大化各框架能力，未来工作将重点改进归约算法、数据通信、内存管理，并进行可扩展性研究和全面的统计分析来评估比较框架性能。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [13] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: EPaxos*是一个更简单且正确的Egalitarian Paxos变体，通过简化的故障恢复算法解决了原协议的复杂性和错误问题，并优化了故障阈值范围。


<details>
  <summary>Details</summary>
Motivation: 传统的Paxos协议依赖单一领导者，存在单点故障和延迟问题。Egalitarian Paxos引入了无领导者方法，但协议复杂、规范模糊且存在严重错误。

Method: 提出了EPaxos*，关键技术创新是简化的故障恢复算法，并进行了严格正确性证明。协议将Egalitarian Paxos推广到整个故障阈值范围f和e，满足n ≥ max{2e+f-1, 2f+1}的最优进程数条件。

Result: EPaxos*在最多f个进程故障时仍能保持非零吞吐量，在不超过e个其他进程故障且并发命令可交换时，可在2个消息延迟内快速执行命令。

Conclusion: EPaxos*提供了一个更简单、正确且最优的Egalitarian Paxos变体，改进了原协议的复杂性和可靠性问题。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 本文展示了如何使用基础模型通过文本指令自动搜索奖励函数，为Gran Turismo 7赛车游戏生成理想的强化学习智能体。结合LLM奖励生成、VLM偏好评估和人类反馈，系统能产生与冠军级赛车智能体GT Sophy竞争的表现，并生成新颖行为。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，通过奖励函数定义期望的智能体行为是一个困难的过程，特别是在复杂环境如自动驾驶赛车中。传统方法难以将期望行为映射到合适的奖励函数。

Method: 使用基础模型搜索奖励函数空间：1）基于LLM的奖励生成；2）基于VLM的偏好评估；3）结合人类反馈。系统仅需文本指令即可为Gran Turismo 7游戏生成强化学习智能体。

Result: 系统能够产生与冠军级赛车智能体GT Sophy竞争的表现，同时还能生成新颖的行为模式，证明了在真实世界应用中实现自动化奖励设计的可行性。

Conclusion: 该方法为实际应用中的自动化奖励设计铺平了道路，展示了基础模型在复杂环境中自动设计强化学习奖励函数的有效性。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [15] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark (DVB) 是一个评估框架，用于测试大语言模型是否学习到深层人类价值观而非表面偏好，结果显示模型普遍无法有效泛化深层价值观。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是否学习到深层人类价值观还是仅捕获表面偏好模式，这对AI对齐至关重要，因为只有理解深层价值观的系统才能稳健泛化人类意图。

Method: 采用新颖的实验设计，在训练阶段让LLMs接触深度和浅层特征故意相关的人类偏好数据，测试阶段打破这些相关性，测量模型的深度价值观泛化率(DVGR)。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型对深层价值观的泛化都低于随机水平，且更大模型的DVGR略低于较小模型。

Conclusion: 当前大语言模型在泛化深层人类价值观方面表现不佳，DVB为AI对齐的核心特征提供了可解释的衡量标准。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [16] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 本研究提出了InsurAgent，一个基于大语言模型的智能体，用于模拟洪水保险购买决策行为，通过检索增强生成和推理模块解决了LLM在定量概率估计方面的不足。


<details>
  <summary>Details</summary>
Motivation: 美国高风险人群的洪水保险参与率极低，需要理解保险决策的行为机制，而大语言模型为模拟人类决策提供了有前景的工具。

Method: 构建基准数据集评估LLM能力，提出InsurAgent智能体，包含感知、检索、推理、行动和记忆五个模块，其中检索模块使用RAG技术基于调查数据，推理模块利用LLM常识进行外推。

Result: LLM对因素有定性理解但定量概率估计不足，InsurAgent通过RAG实现了边际和双变量概率的准确估计，并能捕捉传统模型难以处理的上下文信息。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具，能够模拟时间决策演化过程。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [17] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC是一种自适应奖励预测方法，能够根据上下文预测未来奖励期望值，作为未来思考token数量的函数。该方法通过训练轻量级适配器在推理模型上，实现更长的推理和更大模型下的改进预测。


<details>
  <summary>Details</summary>
Motivation: 为了解决推理过程中的计算效率问题，实现动态推理长度控制，同时提前估计计算时间，优化模型和思考长度的选择。

Method: 训练轻量级适配器在推理模型上，通过自适应奖励预测来预测未来奖励期望值，支持动态推理长度控制。

Result: Re-FORC实现了：1）提前停止无希望的推理链，减少26%计算量同时保持准确性；2）优化模型和思考长度选择，在相同计算量下获得4%更高准确性，在相同准确性下减少55%计算量；3）自适应测试时扩展，在高计算模式下提高11%准确性，在低计算模式下提高7%准确性。

Conclusion: Re-FORC通过动态推理长度控制和成本每token阈值，实现了计算效率的显著提升，同时能够提前估计计算时间。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [18] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 本文提出了ATHENA框架，通过结合符号效用函数发现和个体语义适应，解决个人决策与群体最优预测之间的差距问题，在旅行方式和疫苗选择任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 个人决策模型与群体最优预测存在差距，这种差距源于个体决策过程的独特性，包括数值属性（如成本、时间）和语言影响（如个人偏好和约束）。

Method: ATHENA框架包含两个阶段：首先通过LLM增强的符号发现找到稳健的群体级符号效用函数；然后进行个体级语义适应，创建由最优效用指导的个性化语义模板来建模个性化选择。

Result: 在真实世界的旅行模式和疫苗选择任务验证中，ATHENA始终优于基于效用、机器学习和其他基于LLM的模型，F1分数比最强前沿模型至少提高6.5%。消融研究证实两个阶段都至关重要且互补。

Conclusion: 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [19] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出了一个名为\method的框架，用于提升大型语言模型在复杂表格数据推理上的性能，该框架包含查询分解器、表格清理器和基于程序思维的推理器，并在新数据集CalTab151上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂表格数据推理中的性能不足问题，包括复杂查询处理、噪声数据和数值能力限制等挑战。

Method: 框架包含三个核心组件：(1)查询分解器将复杂问题分解为子问题，(2)表格清理器清理和过滤噪声表格，(3)基于程序思维的推理器生成可执行代码从清理后的表格中推导最终答案。

Result: 在TAT-QA、TableBench和\method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到了最先进的性能水平。

Conclusion: 该框架有效提升了大型语言模型在复杂表格数值推理任务上的性能，并能与主流LLMs无缝集成，为复杂表格分析提供了稳健的解决方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [20] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一个分解多模态大语言模型（MLLMs）中模态跟随行为的新框架，将其分解为相对推理不确定性和固有模态偏好两个基本因素，揭示了模态跟随概率随相对不确定性单调下降的普遍规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅使用粗粒度的数据集级统计来衡量多模态大语言模型在不同模态提供矛盾信息时的行为，忽视了模型在单模态推理中置信度的影响。

Method: 构建了一个可控数据集，系统性地改变视觉和文本输入的推理难度；使用熵作为细粒度不确定性度量；通过层间预测探测内部机制。

Result: 发现模态跟随概率随相对不确定性单调下降的普遍规律；在平衡点处揭示了模型的固有模态偏好；发现模型在模糊区域会在不同层间在模态间摇摆。

Conclusion: 相对不确定性和固有偏好是模态跟随的两个控制原则，为理解MLLMs如何解决冲突信息提供了定量框架和机制性见解。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [21] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文分析了多智能体推理中的懒惰行为问题，提出了因果影响度量和可验证奖励机制来改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多智能体推理系统存在懒惰行为问题，即一个智能体主导而另一个贡献很少，导致协作失效，退化为单智能体系统。

Method: 1) 理论分析懒惰行为的产生原因；2) 引入稳定高效的因果影响度量方法；3) 提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验表明，该框架有效缓解了懒惰智能体行为，释放了多智能体框架在复杂推理任务中的全部潜力。

Conclusion: 通过因果影响度量和可验证奖励机制，成功解决了多智能体推理中的协作问题，提升了复杂推理任务的性能。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [22] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 提出ProQ-BERT框架，基于transformer架构和多模态电子健康记录预测慢性肾病进展，在91,816患者队列中表现优异，ROC-AUC达0.995。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病影响全球近10%人口，准确预测疾病进展对于及时干预和资源优化至关重要。

Method: 使用transformer框架整合人口统计、临床和实验室数据，采用基于量化的标记化处理连续实验室值，通过注意力机制提高可解释性，先进行掩码语言建模预训练，再针对不同随访和评估期进行二分类微调。

Result: 在91,816患者队列评估中，模型持续优于CEHR-BERT，短期预测ROC-AUC达0.995，PR-AUC达0.989。

Conclusion: 结果表明transformer架构和时间设计选择在临床预后建模中的有效性，为个性化慢性肾病护理提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [23] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 本研究提出了一种基于模糊软集理论的专家系统，用于通过可测量的临床和生理参数评估乳腺癌风险。系统整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算进行风险评估。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的主要原因之一，早期诊断对于有效治疗和提高生存率至关重要。但由于疾病的复杂性和患者风险因素的变异性，及时检测仍然是一个挑战。

Method: 开发基于模糊软集理论的专家系统，使用BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算进行乳腺癌风险评估。数据集来自UCI机器学习库。

Result: 该系统能够通过常规血液分析获得参数，提供非侵入性和可访问的初步评估方法，支持医疗专业人员识别高风险患者。

Conclusion: 该专家系统有助于确定是否需要进行进一步的诊断程序（如活检），为乳腺癌风险评估提供了一种有效的辅助工具。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [24] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 本文提出了一种基于二元分类视角的生成模型精度-召回率曲线估计新框架，进行了统计分析和风险上界推导，并扩展了现有PR指标。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在图像和文本领域的成功，其评估方法受到广泛关注。虽然现有方法主要依赖标量指标，但精度-召回率曲线为分析提供了更丰富的视角，然而其估计面临诸多挑战。

Method: 提出基于二元分类视角的新框架来估计完整的PR曲线，进行统计分析和最小最大风险上界推导，并扩展现有仅关注曲线极值的PR指标。

Result: 获得了PR估计风险的最小最大上界，框架能够扩展到文献中的多个重要PR指标，并在不同实验设置下研究了曲线的行为差异。

Conclusion: 该框架为生成模型的PR曲线估计提供了理论基础和实用方法，能够更全面地分析生成模型的性能特征。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [25] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 提出VMR方法，将开放式任务转化为可验证的多选格式，使RLVR训练范式能够应用于缺乏标准答案的开放式任务，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在数学、编程等有标准答案的领域表现出色，但在缺乏标准答案的开放式任务（如创意写作、指令遵循）中被视为非推理场景，忽视了推理能力的潜在价值。

Method: 提出可验证多选重构（VMR）训练策略，将开放式数据重构为可验证的多选格式，从而在缺乏显式标准答案的情况下实现有效训练。

Result: 在多个基准测试上的实验结果表明，该方法能有效提升LLM在开放式任务上的性能，在八个开放式基准测试中平均比基线提升5.99分。

Conclusion: VMR方法成功将RLVR训练范式扩展到开放式领域，证明了强化推理能力能够显著提升LLM在开放式任务中的表现。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [26] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 本文提出KLPEG框架，通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并基于知识图谱进行多跳推理，生成针对更新的测试用例，显著提升游戏测试的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏的快速迭代和频繁更新对测试效率和针对性提出了重大挑战。现有的基于大语言模型的自动化游戏测试方法缺乏结构化知识积累机制，难以针对增量游戏更新进行精确高效的测试。

Method: 提出KLPEG框架：1）构建和维护知识图谱来系统建模游戏元素、任务依赖和因果关系；2）利用LLM解析自然语言更新日志；3）通过知识图谱上的多跳推理识别影响范围；4）生成针对更新的测试用例。

Result: 在Overcooked和Minecraft两个代表性游戏环境中的实验表明，KLPEG能够更准确地定位受更新影响的功能，并以更少的步骤完成测试，显著提高了游戏测试的有效性和效率。

Conclusion: KLPEG框架通过知识图谱和LLM的结合，解决了游戏增量更新测试中的知识积累和重用问题，为自动化游戏测试提供了一种有效的解决方案。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [27] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA基准测试评估了5个最先进的大语言模型在500个跨领域定量推理任务上的表现，结果显示准确率仅为45-63%，主要错误来自舍入(35%)和计算错误(33%)。模型在数学和工程领域表现较好，但在物理和自然科学领域较弱。


<details>
  <summary>Details</summary>
Motivation: 现有数学数据集无法充分评估大语言模型在真实跨领域定量推理任务中的表现，特别是逐步推理、数值精度和领域泛化能力。

Method: 使用Omni计算引擎验证的输出来构建包含金融、物理、健康、统计等领域的500个自然语言任务基准测试。

Result: 五个最先进系统(ChatGPT-5、Gemini 2.5 Flash、Claude Sonnet 4.5、Grok 4、DeepSeek V3.2)准确率为45-63%，主要错误类型为舍入错误(35%)和计算错误(33%)。相关性分析(r≈0.40-0.65)显示模型经常同时失败但错误类型不同。

Conclusion: 大语言模型在跨领域定量推理任务中表现有限，存在显著的数值精度问题，但模型之间存在部分互补性而非完全冗余。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [28] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违反并使用归纳逻辑编程在线修复规范，确保屏蔽器优雅演化并保持最优奖励和逻辑合规性。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法假设固定的逻辑规范和手工抽象，在环境假设被违反时无法适应，导致安全性失效。

Method: 开发基于GR(1)规范的自适应屏蔽框架，运行时检测环境假设违反，使用归纳逻辑编程在线自动修复GR(1)规范。

Result: 在Minepump和Atari Seaquest案例研究中显示，自适应屏蔽相比静态屏蔽能保持接近最优的奖励和完美的逻辑合规性。

Conclusion: 自适应屏蔽框架能够有效解决静态屏蔽在环境假设违反时的适应性问题，确保系统安全性和性能的平衡。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [29] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析组合空间推理能力的大规模基准数据集和生成框架，包含超过500万个数据点，能够独立控制组合性的多个方面，并通过符号求解器验证数据正确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在空间推理任务中表现出组合性能力不足的问题，需要一种能够精细评估模型组合推理能力的基准数据集。

Method: 通过程序化生成方法构建数据集，独立控制生产力（推理深度）、可替换性（实体和语言变异性）、过度泛化（输入顺序、干扰项）和系统性（新语言元素）等组合性维度，并使用符号求解器验证数据正确性。

Result: 实验表明，大型语言模型在空间推理任务中难以进行生产性和系统性泛化，但对语言变异性具有更强的鲁棒性。

Conclusion: DecompSR提供了一个可证明正确且严谨的基准数据集，能够独立控制组合性的关键维度，为评估LLMs的组合推理能力提供了精细化的探测工具。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [30] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 该研究提出了一个协作迷宫求解基准，评估了32个领先AI模型在单独、同质和异质配对中的协作能力，发现存在'协作差距'：单独表现好的模型在协作时性能显著下降，并提出'接力推理'方法改善协作效果。


<details>
  <summary>Details</summary>
Motivation: AI发展轨迹表明我们将越来越依赖由独立开发、具有不同信息、权限和工具的智能体组成的系统，这些系统的成功关键取决于异构智能体之间的有效协作，但目前缺乏大规模评估智能体间协作的实证研究。

Method: 提出了一个协作迷宫求解基准框架，该框架能够：(i)隔离协作能力，(ii)调节问题复杂度，(iii)实现可扩展的自动评分，(iv)不施加输出格式约束以保持生态合理性。使用该框架评估了32个领先的开源和闭源模型在单独、同质和异质配对中的表现。

Result: 研究结果揭示了'协作差距'：单独表现良好的模型在需要协作时性能显著下降。协作可能完全崩溃，例如单独能很好解决迷宫的小型蒸馏模型在某些配对中几乎完全失败。发现从更强的智能体开始通常能改善结果，这启发了'接力推理'方法。

Conclusion: 研究主张：(1)协作感知的评估方法，(2)开发增强协作能力的训练策略，(3)可靠激发智能体潜在技能的交互设计，这些指导原则适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [31] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出了span query概念，将推理服务器接口泛化，支持聊天、RAG、推理时缩放和智能体工作负载，通过优化KV缓存和注意力局部性实现10-20倍的TTFT降低。


<details>
  <summary>Details</summary>
Motivation: 当前推理服务器主要针对聊天完成优化，但客户端已发展到包含各种推理时缩放和深度推理技术，需要更通用的接口来支持非聊天用例。

Method: 引入span query作为推理调用的表达式树，通过交换性约束连接，并自动优化KV缓存局部性，在vLLM中仅修改492行代码即可实现高性能执行。

Result: span query在两种不同的非聊天用例中实现了10-20倍的TTFT降低，注意力优化的span query在2b参数模型上优于标准推理服务器使用8b模型的准确性。

Conclusion: span query提供了一种通用且高效的接口，能够显著提升推理服务器的性能，特别是在处理非聊天工作负载时。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [32] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的半自动化方法，用于将控制工程领域的自然语言描述和数学定义转化为形式化知识图谱，以增强知识可访问性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 控制工程领域研究产出的快速增长需要新的方法来结构和形式化领域知识，以促进知识转移和协作。

Method: 基于Imperative Representation of Knowledge (PyIRK)框架，利用语言模型将自然语言描述和LaTeX数学定义转化为形式化知识图谱。

Result: 开发了"交互式语义层"来增强源文档，实现了人类可读性与机器可解释性的结合，并提高了表达能力。

Conclusion: 该方法为实现控制工程领域易于访问、协作和可验证的知识库愿景做出了贡献。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [33] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文提出了一种在复杂AI控制环境中优化攻击策略的方法，通过将攻击能力分解为五个技能组件并单独优化，使用概率模型解决数据不足问题，显著提升了攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得复杂且高风险，准确评估其风险变得至关重要。AI控制是一个评估框架，但好的控制评估需要强大的攻击策略，这在计算受限的复杂智能体环境中具有挑战性。

Method: 将攻击能力分解为五个组成技能：怀疑建模、攻击选择、计划合成、执行和隐蔽性，并分别优化每个组件。为解决数据不足问题，开发了攻击动态的概率模型，在模拟中优化攻击超参数，然后将结果迁移到SHADE-Arena环境。

Result: 该方法显著提高了攻击强度，将安全评分从基线0.87降低到0.41，表明攻击策略的有效性大幅提升。

Conclusion: 通过技能分解和概率建模的方法，可以在数据有限的情况下有效优化复杂AI环境中的攻击策略，为AI控制评估提供了更强大的攻击能力。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [34] [EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory](https://arxiv.org/abs/2511.01912)
*Wenzhe Fan,Ning Yan,Masood Mortazavi*

Main category: cs.MA

TL;DR: EvoMem是一个基于双演进记忆机制的多智能体框架，受认知心理学中的工作记忆模型启发，通过约束记忆和查询反馈记忆来增强多智能体规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体框架中的人类记忆角色尚未充分探索，理解智能体如何通过记忆协调对于自然语言规划中的迭代推理、约束跟踪和错误修正至关重要。

Method: 框架包含三个智能体（约束提取器、验证器、执行器）和两个记忆模块：跨查询演进的约束记忆（存储任务特定规则）和查询内演进的查询反馈记忆（积累迭代反馈）。

Result: 在旅行规划、会议规划和日历调度任务上的评估显示了一致的性能提升，证明了EvoMem的有效性。

Conclusion: 成功证明了记忆在增强多智能体规划中的重要性，为自然语言规划系统提供了新的记忆机制设计思路。

Abstract: Planning has been a cornerstone of artificial intelligence for solving
complex problems, and recent progress in LLM-based multi-agent frameworks have
begun to extend this capability. However, the role of human-like memory within
these frameworks remains largely unexplored. Understanding how agents
coordinate through memory is critical for natural language planning, where
iterative reasoning, constraint tracking, and error correction drive the
success. Inspired by working memory model in cognitive psychology, we present
EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The
framework consists of three agents (Constraint Extractor, Verifier, and Actor)
and two memory modules: Constraint Memory (CMem), which evolves across queries
by storing task-specific rules and constraints while remains fixed within a
query, and Query-feedback Memory (QMem), which evolves within a query by
accumulating feedback across iterations for solution refinement. Both memory
modules are reset at the end of each query session. Evaluations on trip
planning, meeting planning, and calendar scheduling show consistent performance
improvements, highlighting the effectiveness of EvoMem. This success
underscores the importance of memory in enhancing multi-agent planning.

</details>


### [35] [Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.02304)
*Beyazit Yalcinkaya,Marcell Vazquez-Chanlatte,Ameesh Shah,Hanna Krasowski,Sanjit A. Seshia*

Main category: cs.MA

TL;DR: 提出ACC-MARL框架，用于学习基于自动机的多任务多智能体强化学习策略，解决集中训练分散执行下的时序目标协作问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法样本效率低且仅限于单任务场景，需要开发能够处理多任务、多智能体协作时序目标的学习框架。

Method: 使用自动机表示任务，将复杂任务分解为可分配给智能体的子任务，提出ACC-MARL框架学习任务条件化的分散团队策略。

Result: 实验显示智能体出现了任务感知的多步协调行为，如按按钮开门、保持门开和短路任务等协作行为。

Conclusion: ACC-MARL框架可行且正确，学习到的策略价值函数可在测试时用于最优任务分配。

Abstract: We study the problem of learning multi-task, multi-agent policies for
cooperative, temporal objectives, under centralized training, decentralized
execution. In this setting, using automata to represent tasks enables the
decomposition of complex tasks into simpler sub-tasks that can be assigned to
agents. However, existing approaches remain sample-inefficient and are limited
to the single-task case. In this work, we present Automata-Conditioned
Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for
learning task-conditioned, decentralized team policies. We identify the main
challenges to ACC-MARL's feasibility in practice, propose solutions, and prove
the correctness of our approach. We further show that the value functions of
learned policies can be used to assign tasks optimally at test time.
Experiments show emergent task-aware, multi-step coordination among agents,
e.g., pressing a button to unlock a door, holding the door, and
short-circuiting tasks.

</details>
