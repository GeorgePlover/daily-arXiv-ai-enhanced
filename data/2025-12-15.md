<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 本文提出了CORL框架，使用强化学习端到端微调MILP方案，以最大化实际运营性能，将MILP求解过程转化为可微分的随机策略。


<details>
  <summary>Details</summary>
Motivation: 传统的混合整数线性规划（MILP）方法在建模随机现实问题时存在困难，导致实际性能不佳。现有机器学习方法通常依赖监督学习，需要真实最优决策，并使用MILP梯度的替代方法。

Method: 提出CORL概念验证框架，将分支定界算法求解的MILP转化为可微分的随机策略，使其与强化学习兼容，从而能够基于实际数据端到端微调MILP方案。

Result: 在简单的组合序列决策示例中验证了CORL方法的有效性。

Conclusion: CORL框架通过将MILP求解过程转化为可微分的强化学习策略，实现了基于实际运营性能的端到端优化，为组合序列决策问题提供了新的解决方案。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [2] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver框架通过模块化协作和双级规划，在固定预算下优化多智能体系统中的测试时计算分配，显著提升协作性能。


<details>
  <summary>Details</summary>
Motivation: 当前测试时计算扩展技术（如重复采样、自我验证等）在单智能体系统中能显著提升性能，但在多智能体系统中缺乏原则性的计算分配机制来促进协作，无法在明确预算约束下将测试时计算扩展应用于协作交互。

Method: 提出FutureWeaver框架：1）通过模块化协作，将可重用的多智能体工作流封装为可调用函数；2）通过自我游戏反思从历史轨迹中抽象出重复交互模式自动生成模块；3）采用双级规划架构，在推理当前任务状态的同时推测未来步骤，优化计算分配。

Result: 在复杂智能体基准测试中，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在多智能体协作推理时优化中的有效性。

Conclusion: FutureWeaver为多智能体系统中的测试时计算分配提供了系统化解决方案，通过模块化协作和前瞻性规划实现了在固定预算下的高效协作优化。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [3] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow是一个用于现实世界旅行规划的多智能体框架，通过检索、规划和治理三阶段流水线，将开放式用户请求转化为可执行的行程，在空间、时间和预算约束下满足用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在约束满足、工具协调和效率方面存在困难，经常产生不可行或成本过高的计划，无法满足现实世界旅行规划的需求。

Method: TriFlow采用渐进式多智能体框架，通过检索、规划和治理三阶段流水线，结合结构化推理和语言灵活性，使用规则-LLM协作组装约束一致的行程，并进行有界迭代优化。

Result: 在TravelPlanner和TripTailor基准测试中取得最先进结果，分别达到91.1%和97%的最终通过率，相比当前SOTA实现了超过10倍的运行效率提升。

Conclusion: TriFlow通过渐进式多智能体框架有效解决了现实世界旅行规划中的约束满足和效率问题，实现了高质量、可执行的行程规划。

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [4] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 该论文提出了首个专门针对大型视觉语言模型的CAPTCHA基准测试CAPTURE，覆盖4大类25小类CAPTCHA类型，用于全面评估LVLM在解决验证码任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉CAPTCHA的基准测试存在局限性，无法全面覆盖所有CAPTCHA类型，且缺乏专门针对大型视觉语言模型的专用基准。现有研究在设计基准和数据集时通常根据特定研究目标定制，导致评估不够全面。

Method: 提出了CAPTURE基准测试，涵盖来自31个供应商的4个主要CAPTCHA类型和25个子类型。该基准具有广泛的类别多样性、大规模数据以及专门为LVLM定制的标签，填补了先前研究在数据全面性和标签针对性方面的空白。

Result: 使用该基准测试评估当前的大型视觉语言模型时，发现它们在解决CAPTCHA任务上表现不佳。

Conclusion: CAPTURE基准测试为评估LVLM在CAPTCHA解决能力方面提供了全面、多维度的评估框架，填补了该领域专用基准的空白，并揭示了当前LVLM在此类任务上的局限性。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [5] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个任务完成框架，使基于LLM的智能体能在强化学习形式化的环境中，在明确的行为指导下可靠地行动，通过任务分析器、推理模块和生成模块的协同进化实现可信行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出强大能力，但在多轮任务中的行为往往缺乏可靠性和可验证性。需要一种框架来确保LLM智能体在结构化环境中能够按照明确的行为指导可靠地行动。

Method: 框架包含三个组件：1) 轻量级任务分析器，选择推理和生成策略；2) 推理模块，学习可验证的观察-动作映射；3) 生成模块，通过验证或确定性合成强制执行约束兼容的输出。这些组件在智能体与环境交互过程中协同进化。

Result: 该框架使LLM智能体能够在强化学习形式化的环境中（具有定义的观察、动作和奖励信号）按照明确的行为指导行动，实现可信赖的行为。

Conclusion: 通过任务分析器、推理模块和生成模块的协同进化，可以显著提高LLM智能体在多轮任务中的可靠性、可验证性和行为可信度，使其能够在结构化环境中按照明确指导执行任务。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [6] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，通过先选择骨干模型再设计拓扑结构的方法，相比现有方法在相同预算下获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统在构建时很少在明确的token成本和延迟预算约束下进行建模和优化，这导致当预算成为约束时，现有的拓扑优先设计方法往往成本效益不高。

Method: 采用"先骨干后拓扑"的设计方法：1) 骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干的智能体；2) 自适应MAS拓扑生成：通过智能体表示学习、门控机制和延迟感知拓扑合成来指导智能体间通信。

Result: 在包含14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。该框架还能作为现有MAS的插件，在相同约束下提升性能。

Conclusion: AgentBalance提供了一个在明确预算约束下构建成本效益多智能体系统的有效框架，通过先优化智能体骨干再设计通信拓扑的方法，显著提升了系统在预算约束下的性能表现，并具有良好的泛化能力。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [7] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 本文指出当前XAI中常用的保真度评估指标（插入和删除）存在严重问题：基线选择会偏向某些归因方法，甚至导致线性模型得出矛盾的最优方法结论。作者提出基线应满足两个理想属性：移除信息和不产生过度分布外图像，并开发了一种新的模型依赖基线来改进这一权衡。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能中广泛使用的归因方法评估存在根本缺陷。插入和删除等保真度指标依赖于基线函数来修改输入图像，但基线选择会系统性地偏向某些归因方法，导致评估结果不可靠。更严重的是，即使简单的线性模型使用常用基线也会得出矛盾的最优方法结论，这暴露了现有评估框架的根本问题。

Method: 作者首先分析了基线应满足的两个理想属性：信息移除和不过度产生分布外图像。然后对现有基线进行系统性测试，发现它们都无法同时满足这两个标准，存在权衡关系。最后，作者利用特征可视化技术开发了一种新的模型依赖基线，该基线能够移除信息而不产生过度分布外图像。

Result: 研究发现现有基线都无法同时满足信息移除和分布内保持两个标准，存在明显的权衡关系。作者提出的新基线在权衡方面优于现有基线，能够更好地移除信息而不产生过度分布外图像，从而提供更可靠的归因方法评估框架。

Conclusion: 归因方法的评估严重依赖于基线选择，而现有基线存在系统性偏差。作者提出的新基线通过改进信息移除与分布内保持的权衡，为更可靠、无偏的归因方法评估提供了解决方案。这项工作强调了重新审视XAI评估基准的重要性。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [8] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化方法与AI方法在No-Three-In-Line问题上的表现，ILP在19×19网格内获得最优解，PatternBoost在14×14网格内匹配最优性能，PPO在10×10网格表现完美但在11×11失败。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line问题是组合几何中的经典问题，传统整数线性规划方法虽然能保证最优解但面临指数级规模扩展问题，而机器学习方法为模式近似提供了有前景的替代方案，需要系统比较经典优化与AI方法的性能。

Method: 应用PatternBoost transformer学习和强化学习（PPO）方法首次解决该问题，并与整数线性规划（ILP）进行对比。ILP作为经典优化方法提供最优解基准，PatternBoost采用transformer架构学习模式，PPO使用策略优化进行强化学习。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格上获得完美解但在11×11网格上因约束违反而失败。

Conclusion: 经典优化方法对于精确解仍然必不可少，而AI方法在较小实例上提供有竞争力的性能，混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [9] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID框架系统评估AI文本检测器在人口统计学、年龄、教育水平、方言、正式程度、政治倾向和主题等7个维度的偏见，发现检测器对少数群体文本存在系统性偏见，特别是召回率偏低。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛应用，但之前的研究仅发现针对英语学习者的孤立偏见案例，缺乏对更广泛社会语言因素的系统性评估。

Method: 提出BAID评估框架，构建超过20万个样本，涵盖7个主要类别：人口统计学、年龄、教育水平、方言、正式程度、政治倾向和主题。为每个样本生成合成版本，精心设计提示以保留原始内容同时反映特定子群体的写作风格，然后评估4个开源最先进的AI文本检测器。

Result: 发现检测器在检测性能上存在一致的差异，特别是对来自代表性不足群体的文本召回率较低，表明AI文本检测器存在系统性偏见。

Conclusion: BAID框架提供了可扩展、透明的AI检测器审计方法，强调在将这些工具部署给公众使用之前需要进行偏见感知的评估。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [10] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 该研究首次实证确认大型语言模型在处理临床信息时表现出类似代谢功能障碍的特征，提出了"AI-代谢功能障碍相关脂肪性肝病(AI-MASLD)"的创新概念，为AI在医疗领域的应用提供了重要安全警示。


<details>
  <summary>Details</summary>
Motivation: 模拟真实临床场景，系统评估大型语言模型从充满噪声和冗余的患者主诉中提取核心医疗信息的能力，并验证其是否表现出类似代谢功能障碍相关脂肪性肝病(MASLD)的功能衰退。

Method: 采用基于标准化医疗探针的横断面分析设计，选择GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四种主流LLMs作为研究对象。使用包含五个核心维度、二十个医疗探针的评估系统模拟真实临床沟通环境，所有探针均有临床专家定义的金标准答案，并由两名独立临床医生通过双盲、逆评分量表进行评估。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差。在极端噪声条件下，大多数模型出现功能崩溃。值得注意的是，GPT-4o在深静脉血栓(DVT)继发肺栓塞(PE)的风险评估中出现了严重误判。

Conclusion: 首次实证确认LLMs在处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念。这些发现为AI在医疗领域的应用提供了关键安全警告，强调当前LLMs必须在人类专家监督下作为辅助工具使用，因为其理论知识与实际临床应用之间仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [11] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文提出AI基准测试需要从静态转向动态自适应框架，以应对AI快速发展带来的评估挑战，并引入"AI基准测试工艺学"概念来培养相关技能和教育。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试面临多重挑战：AI模型架构、规模、数据集和部署环境快速演变，大型语言模型容易记忆静态基准导致结果与真实性能脱节，现有基准过度强调顶级硬件峰值性能而缺乏实际应用指导。

Method: 提出动态自适应基准测试框架，包含持续演化的模型、更新数据、异构平台支持，同时保持透明度、可重复性和可解释性。强调通过"AI基准测试工艺学"进行系统性教育，培养基准设计和使用专业知识。

Result: 识别出关键障碍包括高资源需求、专用硬件访问限制、基准设计专业知识缺乏、结果与应用领域关联不确定性。提出社区努力可为AI基准测试工艺学提供基础。

Conclusion: 动态包容性基准测试能确保评估跟上AI发展步伐，支持负责任、可重复和可访问的AI部署。基准测试应支持应用相关比较，实现基于情境的明智决策。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [12] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构因果模型的能源需求预测方法，利用天气和日历信息的因果关系构建贝叶斯模型，在测试集上取得了3.84%的平均绝对百分比误差。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受天气条件（温度、湿度、风速、太阳辐射）和日历信息（小时、月份）等多因素影响，这些因素存在因果依赖关系，使得简单的基于相关性的学习方法难以满足需求。

Method: 提出结构因果模型来解释变量间的因果关系，通过全面分析验证因果信念。基于学到的因果洞察作为先验知识构建贝叶斯模型，在未见数据上进行训练和测试。

Result: 模型在测试集上取得了3.84%的MAPE（平均绝对百分比误差），表现出最先进的性能。跨两年数据的交叉验证平均MAPE为3.88%，显示出强大的鲁棒性。因果分析揭示了能源需求对温度波动的响应具有季节依赖性敏感性，冬季能源需求方差较低。

Conclusion: 通过结构因果模型和贝叶斯建模方法，能够有效捕捉能源需求预测中的复杂因果关系，实现高精度和鲁棒的预测性能，为能源管理提供重要洞察。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [13] [Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs](https://arxiv.org/abs/2512.11689)
*Manuela Chacon-Chamorro,Juan Sebastián Pinzón,Rubén Manrique,Luis Felipe Giraldo,Nicanor Quijano*

Main category: cs.MA

TL;DR: 比较人类与LLM智能体在公共资源困境中的合作韧性表现，发现人类通过沟通能达到最高韧性水平，LLM智能体虽受益于沟通但仍不及人类，为设计更具社会适应性的AI智能体提供参考。


<details>
  <summary>Details</summary>
Motivation: 建立多智能体系统合作韧性的基准测试框架，系统比较人类与基于大语言模型的智能体在面对持续破坏和间歇性环境冲击时的表现，探索沟通对合作韧性的影响，为设计具有社会适应性的AI智能体提供依据。

Method: 使用Melting Pot套件中的"公地悲剧"环境作为混合动机社会困境场景，设置持续破坏性消费机器人和间歇性环境冲击（随机移除共享资源），比较人类组和LLM智能体组在有/无明确沟通条件下的表现，并在更严苛的长期环境中进一步测试人类表现。

Result: 人类组通过沟通实现了最高的合作韧性；沟通也能提升LLM智能体的韧性，但其表现仍低于人类水平；在更严苛的长期环境中，人类能够维持共享资源并保持高韧性。

Conclusion: 人类在不利社会条件下的决策机制可以为设计促进亲社会行为和韧性的AI智能体提供重要参考，沟通是提升合作韧性的关键因素，但当前LLM智能体在复杂社会困境中的表现仍与人类存在差距。

Abstract: This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 提出一种基于CPU、磁盘和I/O利用率的多资源任务调度算法，通过提高资源利用率来降低云服务能耗


<details>
  <summary>Details</summary>
Motivation: 云服务能耗巨大，传统能源有限且对环境有温室效应，需要开发节能算法。现有研究主要关注最大化平均资源利用率或最小化完工时间，但未充分考虑物理机中不同类型的资源。

Method: 提出一种任务调度算法，通过适应度函数显式优化CPU、磁盘和I/O等资源的利用率。适应度值是CPU、磁盘、I/O利用率和任务处理时间的函数。

Result: 使用合成数据集进行仿真，将提出的算法与现有MaxUtil算法比较。结果显示提出的算法是更好的节能算法，比MaxUtil算法消耗更少的能量。

Conclusion: 通过显式优化多种资源利用率，提出的任务调度算法能有效提高活跃资源利用率，从而降低云服务能耗，为解决云计算的能源效率问题提供了有效方案。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [15] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个AI系统，能够大规模生成功能正确的Triton PyTorch ATen内核，用于新兴加速器平台，重点关注覆盖率和正确性而非性能优化。


<details>
  <summary>Details</summary>
Motivation: 为新兴加速器平台快速生成完整的PyTorch ATen后端，传统方法只关注少数高性能内核，而TritorX旨在实现整个算子集的广泛覆盖。

Method: 集成开源大语言模型、自定义linter、JIT编译和基于PyTorch OpInfo的测试框架，兼容真实MTIA芯片和硬件仿真环境。

Result: 成功为481个独特的ATen算子生成内核和包装器，通过所有对应的PyTorch OpInfo测试（总计超过20,000个）。

Conclusion: TritorX为实现新兴加速器平台一夜之间生成完整PyTorch ATen后端铺平了道路。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [16] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文对实时迁移技术进行了全面综述，重点关注容器和虚拟机迁移，分析了技术现状、实际挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述往往忽略实时迁移技术在实际应用中的关键技术细节和实际挑战，本文旨在填补这一空白，为研究人员和实践者提供全面的技术分析。

Method: 整合现有综述的研究成果，从迁移技术、迁移单元和基础设施特征等多个维度对实时迁移技术进行综合分析，特别关注容器和虚拟机两种迁移方式。

Result: 发现实时迁移技术虽然广泛研究，但其对多系统因素的依赖带来挑战，在某些情况下复杂性和资源需求可能超过收益；容器和虚拟机迁移在采用程度上存在差异；迁移目标和操作约束显著影响现有技术的可用性和有效性。

Conclusion: 本文通过概述当前技术挑战并提供未来研发方向指南，既为爱好者提供有价值的实时迁移资源，又有助于推动实时迁移技术在不同计算环境中的实际应用。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [17] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 该研究系统评估了GPU集群利用率低的问题，提出了三种动态调度器（HPS、PBS、SBS），在64-GPU集群上测试显示动态调度器显著优于静态调度策略，最高达到78.2%利用率和25.8作业/小时。


<details>
  <summary>Details</summary>
Motivation: GPU集群对现代AI系统至关重要，但实际部署中平均利用率仅约50%，主要原因是资源碎片化、异构工作负载和静态调度策略的限制，需要更高效的调度方案来提高利用率、公平性和吞吐量。

Method: 提出了三种专门的动态调度器：混合优先级调度器（HPS）、预测性回填调度器（PBS）和智能批处理调度器（SBS）。在包含1000个AI作业的64-GPU、8节点集群上进行控制模拟评估，工作负载包括训练、推理和研究任务。

Result: 静态调度器（FIFO、SJF等）GPU利用率为45-67%，吞吐量12.5-18.3作业/小时，存在严重饥饿问题（最多156个作业等待超过30分钟）。动态调度器显著优于静态策略：HPS达到最高利用率78.2%和最高吞吐量25.8作业/小时，饥饿作业减少到12个；PBS达到76.1%利用率；SBS达到74.6%利用率。

Conclusion: 动态多目标调度器在吞吐量、作业等待时间、公平性方差和饥饿问题等关键指标上一致优于单目标启发式方法。针对性的透明调度策略能够显著提高异构AI集群的GPU效率，为未来生产调度框架提供了实用基础。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [18] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 该研究比较了集中式分层联邦学习（HFL）与去中心化聚合联邦学习（AFL）和去中心化持续联邦学习（CFL）架构，通过Fashion MNIST和MNIST数据集评估，发现去中心化方法在多个指标上优于集中式方法。


<details>
  <summary>Details</summary>
Motivation: 集中式分层联邦学习（HFL）面临通信瓶颈和隐私问题，而去中心化的AFL和CFL通过分布式计算和聚合提供了有前景的替代方案，本研究旨在全面比较这些架构的性能差异。

Method: 研究采用对比分析方法，在Fashion MNIST和MNIST数据集上评估三种联邦学习架构：集中式HFL、去中心化AFL和去中心化CFL，重点关注精度、召回率、F1分数和平衡准确率等性能指标。

Result: 实验结果表明，去中心化的AFL和CFL在精度、召回率、F1分数和平衡准确率等多个指标上均优于集中式的HFL，证明了去中心化聚合机制在分布式设备协同模型训练中的有效性。

Conclusion: 去中心化联邦学习方法（AFL和CFL）在性能上优于集中式方法（HFL），为联邦学习领域提供了有价值的见解，指导研究者和从业者采用去中心化方法以获得更好的协同模型训练效果。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [19] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 论文提出三种GPU原生编译方法消除CPU-GPU数据传输瓶颈，理论分析显示可实现10-100倍代码迭代加速


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段存在显著的CPU-GPU数据传输延迟瓶颈，限制了代码迭代效率

Method: 提出三种互补的GPU原生编译方法：1）并行传统编译适配GPU执行；2）神经编译使用学习型序列到序列翻译与概率验证；3）结合两者的混合架构

Result: 理论分析显示：传统GPU编译通过消除传输实现2-5倍改进，神经编译通过大规模并行实现10-100倍加速，混合方法提供具有正确性保证的实用部署路径

Conclusion: GPU原生编译能显著加速代码迭代周期，概率验证框架允许在编译准确性和并行探索之间权衡，对自改进AI系统和未来模拟计算基板有重要影响

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [20] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux是一个用于强化学习后训练中rollout-training解耦架构的集群调度框架，通过跨集群编排消除同步依赖带来的空闲时间，将成本效率提升1.84倍。


<details>
  <summary>Details</summary>
Motivation: 在强化学习后训练中，rollout（内存密集型）和training（计算密集型）通常解耦到专用集群以提高硬件效率。但基于策略算法所需的严格同步导致严重依赖气泡，使得一个集群运行时另一个集群必须空闲，造成资源浪费。

Method: 提出RollMux框架，基于"一个作业的结构性空闲可被另一个作业的活动阶段利用"的洞察。引入协同执行组抽象，将集群划分为隔离的局部性域，实现两层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器编排可证明最优的轮询调度。组抽象还施加驻留约束，确保大模型状态缓存在主机内存中，实现"热启动"上下文切换。

Result: 在包含328个H20和328个H800 GPU的生产级测试平台上评估，RollMux相比标准解耦架构将成本效率提升1.84倍，相比最先进的共置基线提升1.38倍，同时实现100%的服务水平目标达成率。

Conclusion: RollMux通过跨集群编排有效回收了rollout-training解耦架构中的依赖气泡，显著提高了硬件利用率和成本效率，为强化学习后训练提供了高效的调度解决方案。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [21] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax是一个移动端DNN推理加速框架，通过计算图分区、分支感知内存管理和自适应调度，在不修改模型的情况下实现高达46%的延迟降低和30%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 移动设备上实时DNN应用需求增长，但现有框架在处理动态控制流操作符和不支持的核函数时性能不佳，导致CPU核心闲置、高延迟和内存峰值问题。

Method: 1) 对计算DAG进行分区以暴露并行性；2) 采用分支感知内存管理，使用专用内存池和缓冲区重用；3) 自适应调度器根据设备内存约束执行分支；4) 细粒度子图控制实现动态模型的异构推理。

Result: 在三种不同移动设备上评估五个代表性DNN模型，Parallax实现了高达46%的延迟降低，平均26.5%的内存开销控制，以及高达30%的能耗节省。

Conclusion: Parallax框架在不需模型重构或自定义操作符实现的情况下，显著提升了移动DNN推理性能，满足了实时移动推理的响应性需求。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [22] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一个视频分析框架，通过识别经历相似数据漂移的摄像头并为其训练共享模型，显著降低了计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 当前为每个摄像头单独重新训练模型的方法存在高计算和通信成本，难以扩展。数据漂移在相邻摄像头间通常具有时空相关性，这为优化提供了机会。

Method: ECCO包含三个核心组件：1) 轻量级分组算法动态形成和更新摄像头组；2) GPU分配器动态分配GPU资源以提高重训练准确性和公平性；3) 每个摄像头的传输控制器配置帧采样并协调带宽共享。

Result: 在三个不同数据集上的评估显示，ECCO相比现有方法，在相同计算和通信资源下将重训练准确性提高了6.7%-18.1%，或在相同准确性下支持3.3倍更多的并发摄像头。

Conclusion: ECCO通过利用摄像头间数据漂移的时空相关性，实现了资源高效的连续学习，显著提升了视频分析系统的可扩展性和效率。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>
