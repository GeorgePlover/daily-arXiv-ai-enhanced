<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AI](#cs.AI) [Total: 11]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [WOLF: Werewolf-based Observations for LLM Deception and Falsehoods](https://arxiv.org/abs/2512.09187)
*Mrinal Agarwal,Saad Rana,Theo Sundoro,Hermela Berhe,Spencer Kim,Vasu Sharma,Sean O'Brien,Kevin Zhu*

Main category: cs.MA

TL;DR: WOLF是一个基于狼人杀的多智能体社交推理基准，用于分离测量欺骗产生和检测能力，通过结构化游戏机制和标准化分类实现动态评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法将欺骗简化为静态分类，忽视了真实欺骗动态的交互性、对抗性和长期性。大语言模型能够产生有说服力的欺骗，但在检测同伴欺骗方面表现较弱。

Method: 基于狼人杀游戏构建多智能体社交推理基准，使用可编程LangGraph状态机，包含严格的昼夜循环、辩论轮次和多数投票机制。每个陈述作为独立分析单元，使用标准化分类法（遗漏、扭曲、捏造、误导）对欺骗进行分类，并通过纵向平滑的怀疑分数捕捉信任动态变化。

Result: 在7,320个陈述和100次运行中，狼人在31%的回合中产生欺骗性陈述，同伴检测达到71-73%的精确率和约52%的整体准确率。对狼人的怀疑从约52%上升到超过60%，而对村民和医生的怀疑稳定在44-46%左右。

Conclusion: WOLF将欺骗评估从静态数据集扩展到动态、受控的测试平台，能够测量对抗性多智能体交互中的欺骗和检测能力，扩展互动提高了对说谎者的召回率，而不会增加对诚实角色的错误。

Abstract: Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.

</details>


### [2] [Supporting Dynamic Agentic Workloads: How Data and Agents Interact](https://arxiv.org/abs/2512.09548)
*Ioana Giurgiu,Michael E. Nidd*

Main category: cs.MA

TL;DR: 提出面向智能体的数据架构，解决传统数据库无法适应LLM多智能体系统动态、上下文驱动、协作式工作负载的问题


<details>
  <summary>Details</summary>
Motivation: 传统数据库架构设计用于静态、定义明确的工作负载，而基于大语言模型的多智能体系统表现出动态、上下文驱动、协作的行为模式，导致传统查询优化器和缓存机制难以应对

Method: 提出智能体中心数据架构，采用注意力引导数据检索、语义微缓存、预测性数据预取和基于法定人数的数据服务等机制

Result: 使智能体能够更快、更高效地访问代表性数据，同时减少冗余查询、数据移动和系统间推理负载

Conclusion: 将数据系统重新定义为自适应协作者而非静态执行器，为行为响应式数据基础设施开辟新的研究方向，通过缓存、探测和编排实现动态推理驱动智能体间的高效、上下文丰富数据交换

Abstract: The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: METRO是一种针对内存受限场景的MoE模型专家并行服务路由算法，通过平衡GPU上激活的专家数量而非令牌数量，显著降低解码延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行方法通过平衡各GPU处理的令牌数量来解决负载不均衡问题，但在内存受限的MoE服务场景（特别是解码阶段）中，这种做法反而会降低性能，因为它增加了激活的专家数量，加剧了内存压力。

Method: 提出METRO（Minimum Expert Token ROuting）算法，在内存受限场景下平衡每个GPU激活的专家数量而非令牌数量。采用新颖的allGather方案收集全局top-k信息，相比传统的allToAll具有最小开销。算法同时优化计算效率和GPU并行处理能力。

Result: 在真实系统（8个A100 GPU上的vLLM）和专有模拟器（8-16个B200 GPU）上评估，METRO相比EPLB将解码延迟降低11-22%，Qwen3和DeepSeek-V3服务的总令牌吞吐量提高3-21%。在固定解码SLO下，METRO的解码吞吐量比EPLB提高最多4.11倍。

Conclusion: 在内存受限的MoE服务场景中，平衡激活的专家数量而非令牌数量是更有效的负载均衡策略。METRO算法通过最小化专家激活和高效的路由机制，显著提升了专家并行MoE服务的性能。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [4] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出了一种用于Vision Transformers的分布式分层卸载框架，通过在可信边缘设备上分割视觉数据并分发到多个独立云服务器，保护用户隐私，防止完整图像重建。


<details>
  <summary>Details</summary>
Motivation: 视觉智能工具计算需求高，超出移动和可穿戴设备能力。传统云卸载方案在传输和服务器端计算时存在显著的隐私漏洞，需要设计隐私保护的边缘-云协同方案。

Method: 使用本地可信边缘设备（如手机或Nvidia Jetson）作为边缘协调器，将用户视觉数据分割成小部分并分发到多个独立云服务器，确保没有单个外部服务器拥有完整图像，最终数据合并和聚合计算仅在可信边缘设备上进行。

Result: 以Segment Anything Model (SAM)为案例研究，该方法在保持接近基线分割性能的同时，显著降低了内容重建和用户数据暴露的风险，相比传统云方法大幅增强了内容隐私保护。

Conclusion: 该框架为边缘-云连续体中的视觉任务提供了一个可扩展的隐私保护解决方案，通过设计防止了完整数据重建，平衡了计算卸载需求与隐私保护要求。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [5] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN是一个分布式磁盘向量搜索系统，通过将查询状态完整发送到数据所在机器来保持单全局图的搜索效率，实现近线性吞吐扩展。


<details>
  <summary>Details</summary>
Motivation: 随着数据集扩展到数十亿向量，磁盘向量搜索成为实用方案，但未来需要处理单个服务器无法容纳的超大规模数据集，需要分布式解决方案。

Method: 提出分布式磁盘近似最近邻系统，核心创新是当访问存储在其他机器上的邻域时，将查询的完整状态发送到该机器继续执行，以提高局部性。

Result: 在1亿和10亿点数据集上，使用10台服务器，在0.95召回率下，吞吐量分别达到基线方法的6.21-6.49倍和2.5-5.10倍，平均延迟低于6毫秒。

Conclusion: BatANN是首个基于单全局图的开源分布式磁盘向量搜索系统，在标准TCP上实现了对数搜索效率和近线性吞吐扩展。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [6] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: WarmServe是一个多LLM服务系统，通过预知未来工作负载特征，使用通用GPU工作节点实现一对多GPU预热，显著提升首令牌时间性能，同时保持高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM服务系统在共享GPU集群中优化GPU利用率时，往往以牺牲推理性能为代价，特别是首令牌时间(TTFT)表现较差。研究发现这种折衷的根本原因是这些系统对未来工作负载特征缺乏认知，而实际工作负载具有高度周期性和长期可预测性。

Method: 提出通用GPU工作节点实现一对多GPU预热，基于此设计WarmServe系统：1) 采用驱逐感知模型放置策略缓解集群范围的预热干扰；2) 通过主动预热提前准备通用GPU工作节点；3) 使用零开销内存切换机制管理GPU内存。

Result: 在真实数据集评估中，WarmServe相比最先进的基于自动扩展的系统，TTFT提升高达50.8倍，同时相比GPU共享系统能够服务多达2.5倍的请求。

Conclusion: 通过利用工作负载的周期性和可预测性，WarmServe成功解决了多LLM服务中资源利用率与推理性能之间的权衡问题，实现了高性能和高资源效率的统一。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [7] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 本文提出了一种基于帕累托的混合鲸鱼-海鸥优化算法(PHWSOA)，用于解决云计算中的多目标任务调度问题，同时优化执行时间、虚拟机负载平衡和经济成本三个目标。


<details>
  <summary>Details</summary>
Motivation: 云计算中的任务调度是一个关键研究挑战。现有调度方案大多只优化单一或有限指标（如执行时间或资源利用率），缺乏全面的多目标优化方法。

Method: 提出PHWSOA算法，结合鲸鱼优化算法(WOA)和海鸥优化算法(SOA)的优势，弥补各自在局部开发和全局探索方面的不足。采用Halton序列初始化提高种群多样性，帕累托引导变异机制防止早熟收敛，并行处理加速收敛，并集成动态虚拟机负载重分配机制。

Result: 在CloudSim模拟器上使用NASA-iPSC和HPC2N真实工作负载进行实验，PHWSOA实现了：执行时间最多减少72.1%，虚拟机负载平衡改善36.8%，成本节省23.5%。性能显著优于WOA、GA、PEWOA和GCWOA等基准方法。

Conclusion: PHWSOA算法在云计算任务调度中表现出卓越的多目标优化能力，为实际云环境中的高效资源管理提供了强大潜力。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [8] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: SynthPix是一个基于JAX实现的高性能并行合成图像生成器，专门用于粒子图像测速（PIV），相比现有工具实现了几个数量级的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 开发SynthPix的主要动机是为了支持数据需求大的强化学习方法在流场估计中的训练，并减少快速流场估计方法开发时的迭代时间，特别是在需要实时PIV反馈的主动流体控制研究中。

Method: 基于JAX框架实现，充分利用加速器的并行计算能力，支持与现有工具相同的配置参数，但通过并行化架构大幅提升图像对生成速度。

Result: SynthPix在每秒图像对生成吞吐量上比现有工具高出几个数量级，能够显著加速PIV相关方法的开发和训练过程。

Conclusion: SynthPix是一个对流体动力学社区有用的软件包，能够支持数据密集型流场估计方法的开发和实时PIV反馈系统的研究。

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [9] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: STAR系统通过新的同步模式和资源重分配策略，有效解决了GPU深度学习训练中的straggler问题，相比现有方法显著降低了训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU深度学习训练很流行，但straggler问题的普遍性、原因和影响以及现有缓解方法的有效性尚未得到充分研究。研究发现straggler广泛存在，且现有方法（如从SSGD切换到ASGD）可能无法改善训练时间甚至产生更多straggler。

Method: 提出STAR系统，包含：1）新的同步模式，将工作者分组进行参数更新；2）启发式和机器学习方法选择最优同步模式以最小化训练时间；3）资源重分配支持所选模式，同时最小化对共存作业的影响；4）通过避免CPU和带宽过载来主动预防straggler。

Result: 在AWS上的trace驱动评估显示，STAR在PS架构中比最先进系统降低48-84%的训练时间，在all-reduce架构中降低51-70%，同时保持了SSGD的收敛精度。

Conclusion: STAR系统有效解决了GPU深度学习训练中的straggler问题，通过创新的同步模式和资源管理策略显著提高了训练效率，同时开源了代码。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [10] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 本文提出首个同时实现无锁和可恢复性的转换方法，将基于锁的实现转换为可恢复的无锁实现


<details>
  <summary>Details</summary>
Motivation: 现有系统需要在锁机制和无锁机制之间做出权衡，缺乏同时具备无锁性和可恢复性的解决方案。无锁算法通常难以实现可恢复性，而基于锁的系统在故障恢复方面存在限制。

Method: 从基于锁的实现出发，为锁获取和锁释放操作提供可恢复、无锁的替代方案。转换支持嵌套锁以确保通用性，并在不损害原始锁实现正确性的前提下确保可恢复性。

Result: 首次实现了同时具备无锁性和可恢复性的转换，支持嵌套锁，保持了原始锁实现的正确性，为并发系统提供了更好的故障恢复能力。

Conclusion: 该转换方法成功解决了无锁算法与可恢复性之间的冲突，为并发系统设计提供了新的可能性，能够在保持正确性的同时提升系统的可靠性和恢复能力。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 大语言模型的幻觉会影响用户信任，但不会导致全面不信任，而是引发情境化的信任校准。研究发现用户相关因素（期望、先前经验、专业知识）和直觉影响信任，情境因素（感知风险、决策风险）也起作用。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型产生的幻觉如何影响用户对LLM的信任以及用户与LLM的互动。幻觉是LLM输出的事实错误但看似合理的内容，需要了解这些错误如何影响日常使用中的信任关系。

Method: 采用定性研究方法，对192名参与者进行研究，探索日常使用中幻觉对信任的影响。基于Lee & See的校准信任模型和Afroogh等人的信任相关因素理论框架。

Result: 幻觉不会导致全面不信任，而是引发情境敏感的信任校准。确认了期望、先前经验、用户专业知识和领域知识作为用户相关信任因素，并发现直觉是幻觉检测的额外因素。信任动态还受情境因素影响，特别是感知风险和决策风险。验证了Blöbaum的递归信任校准过程，并扩展了直觉作为用户相关信任因素。

Conclusion: 基于研究发现，提出了负责任和反思性LLM使用的实用建议。研究扩展了信任校准理论，强调了直觉在检测幻觉中的重要性，并为理解用户如何适应LLM的不完美提供了框架。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [12] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: AI TIPS 2.0框架解决现有AI治理框架的三个关键缺陷：用例风险评估不足、原则缺乏可操作性控制、规模化实施机制缺失


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键问题：1) 缺乏针对具体用例的风险评估（如Humana集体诉讼案例）；2) 现有框架如ISO 42001和NIST AI RMF停留在概念层面，缺乏可操作控制；3) 组织缺乏规模化实施治理的机制

Method: 提出AI TIPS 2.0（人工智能信任集成支柱可持续性框架），这是2019年开发的全面操作框架的更新版本，比NIST AI风险管理框架早四年，直接针对上述挑战提供解决方案

Result: AI TIPS 2.0框架旨在解决现有治理框架的不足，提供可操作的治理方法，支持从董事会到数据科学家的角色化可见性

Conclusion: 需要新的AI治理框架来填补现有框架的空白，AI TIPS 2.0提供了一个全面的操作框架，能够解决用例风险评估、可操作性控制和规模化实施等关键挑战

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [13] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 论文提出一个形式化范畴框架，分析人类和LLM如何将内容转化为关于可能世界状态空间W的真值命题，论证LLM并非解决而是绕过了符号接地问题


<details>
  <summary>Details</summary>
Motivation: 研究人类和大型语言模型在将内容转化为真值命题时的认知差异，特别是探讨LLM是否真正解决了符号接地问题

Method: 使用形式化范畴理论框架，构建基于可能世界状态空间W的分析模型，比较人类和LLM在命题生成过程中的差异

Result: LLM通过统计模式而非真实语义理解来生成命题，实际上绕过了符号接地问题而非解决它

Conclusion: LLM缺乏真正的符号接地能力，其命题生成机制与人类认知存在本质差异，这对理解AI的语义处理能力有重要意义

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [14] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源Python工具包，集成了对话生成、评估和机制可解释性，为构建和分析基于LLM的对话系统提供端到端框架。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统开发缺乏统一的框架，难以系统地进行生成、评估和可解释性分析。SDialog旨在解决这一问题，为研究人员提供集成的工具来更系统地构建、基准测试和理解对话系统。

Method: 基于标准化的Dialog表示，SDialog提供：1）基于角色的多智能体模拟与可组合编排；2）结合语言指标、LLM作为评判者和功能正确性验证器的综合评估；3）通过特征消融和诱导进行激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的完整声学模拟音频生成。工具包与所有主要LLM后端集成。

Result: SDialog成功实现了对话生成、评估和机制可解释性的统一框架，支持混合后端实验，为研究人员提供了系统化构建和分析对话系统的工具。

Conclusion: 通过将生成、评估和可解释性耦合在对话中心架构中，SDialog使研究人员能够更系统地构建、基准测试和理解对话系统，推动了对话AI研究的系统化发展。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [15] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架，用于建模认知异构智能体之间的信念、动机和影响。每个智能体由个性化价值空间表示，信念被形式化为结构化向量，其传播通过线性解释映射进行。信念只有在避免这些映射的零空间时才能在交流中存活，这为可理解性、误解和信念消亡提供了结构标准。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个统一的框架来分析认知异构智能体之间的信念动态，解决信念传播、扭曲和消亡的结构性原因。传统方法依赖共享信息或理性假设，而该研究旨在通过几何和代数约束来理解信念在不同认知系统间的传播机制。

Method: 方法包括：1) 将每个智能体建模为个性化价值空间（向量空间）；2) 将信念形式化为结构化向量（抽象存在）；3) 使用线性解释映射来建模信念传播；4) 分析零空间条件对信念存活的影响；5) 推导出"无零空间领导条件"等代数约束。

Result: 主要结果：1) 信念扭曲、动机漂移、反事实评估和相互理解的限制都源于纯代数约束；2) "无零空间领导条件"将领导力特征化为表示可达性而非说服或权威；3) 解释了抽象存在如何在多样化认知几何中传播、变异或消失；4) 为信念动态分析提供了统一基础。

Conclusion: 该认知几何视角通过将意义保存建立在结构兼容性而非共享信息或理性的基础上，统一了概念空间、社会认识论和AI价值对齐的见解。这一框架阐明了人类和人工系统中影响的认知边界，为分析异构智能体间的信念动态提供了通用基础。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [16] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 对比人类与AI系统在模糊视觉刺激下的图像标注表现，分析两者在表征、推理和置信度校准方面的异同，为未来神经符号架构提供认知基础


<details>
  <summary>Details</summary>
Motivation: 研究人类和AI系统如何解释模糊视觉刺激，这能深入理解感知、推理和决策的本质。通过对比两者的处理策略，为构建更可解释、认知对齐的AI系统提供理论基础

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类策略（类比推理、形状识别、置信度调节）与AI的特征处理。基于Marr的三层次假设、Simon的有限理性和Thagard的表征情感框架，分析参与者反应与Grad-CAM可视化模型注意力。使用ACT-R和Soar认知架构解释人类行为

Result: 揭示了生物与人工系统在表征、推理和置信度校准方面的关键相似点和差异。人类表现出分层和启发式决策策略，而AI则基于特征处理

Conclusion: 分析为未来神经符号架构提供了动机，这种架构将结构化符号推理与连接主义表征相统一，结合具身性、可解释性和认知对齐原则，有望构建既高性能又可解释且具有认知基础的AI系统

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [17] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个整合片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效、可解释的闭环靶向分子设计，在结合亲和力、类药性和合成可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法耗时昂贵，现有生成模型存在泛化能力不足、可解释性有限、过度强调结合亲和力而忽视关键药理学性质等问题，限制了其转化应用价值。

Method: Trio框架整合三个关键组件：基于片段的分子语言建模实现上下文感知的片段组装；强化学习确保物理化学和合成可行性；蒙特卡洛树搜索平衡新颖化学型探索和结合口袋中有前景中间体的利用。

Result: Trio可靠地生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时将分子多样性扩展四倍以上。

Conclusion: Trio提供了一个有效且可解释的闭环靶向分子设计框架，解决了现有生成模型的局限性，在药物发现中展现出优越性能。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [18] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 本文提出了一种在连续动作空间中使用高斯过程回归来聚合多线程MCTS统计信息的方法，在6个不同领域中都优于现有聚合策略，仅需适度增加推理时间。


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，当计算时间有限但需要最佳性能时，如何有效聚合不同线程的统计信息是一个重要但尚未充分探索的问题。现有的根并行MCTS在连续动作空间中的统计聚合方法需要改进。

Method: 使用高斯过程回归来获取未在环境中试验过的有希望动作的价值估计。该方法通过回归模型来推断未采样动作的价值，从而更有效地聚合多线程统计信息。

Result: 在6个不同领域进行了系统评估，结果表明该方法优于现有的聚合策略，同时仅需要适度的推理时间增加。

Conclusion: 提出的基于高斯过程回归的统计聚合方法在连续动作空间的根并行MCTS中表现优异，为在线规划提供了更有效的解决方案。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [19] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT是一个基于强化学习的智能故障定位框架，通过将最坏情况故障搜索转化为序列决策问题，显著加速AI加速器的故障评估，减少测试向量需求99%，同时提供可操作的硬件保护策略。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模巨大，传统故障评估方法面临计算成本过高和关键故障模式覆盖不足的问题，需要更高效、可扩展的故障评估框架。

Method: RIFT将复杂的最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化、高影响力的测试套件。

Result: 在基于NVIDIA A100 GPU的十亿参数大语言模型工作负载上，RIFT相比进化方法实现2.2倍故障评估加速，相比随机故障注入减少99%以上测试向量需求，同时获得更优的故障覆盖率。RIFT指导的选择性错误校正码相比统一三重模块冗余保护，成本效益（单位面积覆盖率）提升12.8倍。

Conclusion: RIFT提供了一个可扩展的框架，能够自动化发现最小化、高影响力的故障场景，显著提高AI加速器故障评估效率，同时生成可直接集成到商业RTL验证工作流程的UVM兼容验证工件。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [20] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ平台结合AI和人工参与（包括众包）来支持元数据词汇表开发，在材料科学领域进行了概念验证，展示了AI-人工协同模型在增强语义透明度和减少共识构建时间方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇表对推进FAIR和FARR数据原则至关重要，但其发展受到人力资源有限和标准化实践不一致的制约。需要新的方法来支持元数据词汇表开发。

Method: 开发了MatSci-YAMZ平台，整合人工智能和人工参与（包括众包）来支持元数据词汇表开发。在材料科学领域进行了概念验证，6名参与者通过平台提供术语定义和示例，引导AI定义精炼，形成迭代反馈循环。

Result: 成功生成了19个AI生成的定义，迭代反馈循环证明了AI-人工协同精炼的可行性。研究确认了AI-人工协同模型的可行性，包括：1）成功的概念验证；2）与FAIR和开放科学原则的一致性；3）指导未来研究的研究协议；4）跨领域扩展的潜力。

Conclusion: MatSci-YAMZ的基础模型有能力增强语义透明度，减少共识构建和元数据词汇表开发所需的时间，为跨领域扩展提供了潜力。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [21] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标仅初始化时预训练轻量学生模型，显著提升效率但牺牲可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的规划方法存在计算成本高、部署效率低、LLM参数固定无法适应目标任务等问题，需要更高效的规划方法。

Method: 提出SCOPE方法：1）仅在初始化时使用LLM生成子目标；2）从示例轨迹直接推导子目标；3）预训练轻量学生模型；4）避免训练和推理期间重复查询LLM。

Result: 在TextCraft环境中，SCOPE达到0.56成功率，优于ADaPT的0.52；推理时间从164.4秒大幅减少到3.0秒，效率显著提升。

Conclusion: LLM生成的子目标即使非最优，仍能为基于文本的分层规划任务提供良好的起点，SCOPE在效率与性能间取得良好平衡。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>
