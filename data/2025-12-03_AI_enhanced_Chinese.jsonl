{"id": "2512.02410", "categories": ["cs.MA", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.02410", "abs": "https://arxiv.org/abs/2512.02410", "authors": ["Yepeng Ding", "Ahmed Twabi", "Junwei Yu", "Lingfeng Zhang", "Tohru Kondo", "Hiroyuki Sato"], "title": "Decentralized Multi-Agent System with Trust-Aware Communication", "comment": null, "summary": "The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u7684\u5355\u70b9\u6545\u969c\u3001\u5ba1\u67e5\u8106\u5f31\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u4fe1\u4efb\u95ee\u9898\uff0c\u901a\u8fc7\u52a0\u5bc6\u539f\u8bed\u548c\u94fe\u4e0a\u64cd\u4f5c\u5b9e\u73b0\u53ef\u4fe1\u901a\u4fe1\u534f\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u4e86\u81ea\u4e3b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53d1\u5c55\uff0c\u4f46\u4f20\u7edf\u96c6\u4e2d\u5f0f\u67b6\u6784\u5b58\u5728\u5355\u70b9\u6545\u969c\u3001\u5ba1\u67e5\u8106\u5f31\u6027\u3001\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u4fe1\u4efb\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u53bb\u4e2d\u5fc3\u5316\u667a\u80fd\u4f53\u8fd0\u884c\u65f6\uff0c\u5f62\u5f0f\u5316\u53ef\u4fe1\u901a\u4fe1\u534f\u8bae\uff0c\u5229\u7528\u52a0\u5bc6\u539f\u8bed\u548c\u94fe\u4e0a\u64cd\u4f5c\u786e\u4fdd\u5b89\u5168\u6027\u3002", "result": "\u901a\u8fc7\u5168\u9762\u5b89\u5168\u5206\u6790\u9a8c\u8bc1\u4e86\u534f\u8bae\u7684\u5b89\u5168\u6027\u5c5e\u6027\uff1a\u53ef\u9a8c\u8bc1\u4ea4\u4e92\u5468\u671f\u3001\u901a\u4fe1\u5b8c\u6574\u6027\u3001\u771f\u5b9e\u6027\u3001\u4e0d\u53ef\u5426\u8ba4\u6027\u548c\u6761\u4ef6\u4fdd\u5bc6\u6027\uff0c\u6027\u80fd\u5206\u6790\u8868\u660e\u7cfb\u7edf\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\u662f\u6784\u5efa\u53ef\u4fe1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u96c6\u4e2d\u5f0f\u67b6\u6784\u7684\u6839\u672c\u95ee\u9898\u3002"}}
{"id": "2512.02561", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02561", "abs": "https://arxiv.org/abs/2512.02561", "authors": ["Jinming Yang", "Zimu Ji", "Weiqi Luo", "Gaoxi Wang", "Bin Ma", "Yueling Deng"], "title": "EZYer: A simulacrum of high school with generative agent", "comment": "AgentIR@SIGIR 2025", "summary": "With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in terms of courseware generation, interactive notes and quality assurance of content. In particular, the proposed generative agent EZYer : 1) Teacher Module: Integrating the Text Corpus retrieval and in-depth generation technologies, it automatically generates structured teaching materials and LaTeX Beamer courseware in line with the high school mathematics syllabus and supports user-defined image insertion. 2) Student Module: Throughout the collaborative interaction of the four roles of Teacher, Assistant, Top Student and Struggling Student, Note Taker summarizes and generates academic notes to enhance the depth and interest of learning. 3) Controller: set up keyword filtering system, content scoring system, role co-validation system, and dynamic content correction system. This ensure academic strictness and pedagogical propriety of EZYer inputs and outputs. In order to evaluate EZYer, this paper designs five-dimensional evaluation indexes of content accuracy, knowledge coverage, usability, formatting correctness and visual design and appeal, and scores 100 Beamer and Notes generated by EZYer by five large language models, separately, and the results show that the quality of EZYer-generated content is excellent and has a good application prospect.", "AI": {"tldr": "EZYer\u662f\u4e00\u4e2a\u9762\u5411\u9ad8\u4e2d\u6570\u5b66\u6559\u80b2\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff0c\u5305\u542b\u6559\u5e08\u6a21\u5757\u3001\u5b66\u751f\u6a21\u5757\u548c\u63a7\u5236\u5668\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u6559\u5b66\u6750\u6599\u3001LaTeX\u8bfe\u4ef6\u548c\u5b66\u672f\u7b14\u8bb0\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u9a8c\u8bc1\u5176\u5185\u5bb9\u8d28\u91cf\u4f18\u79c0\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u6559\u80b2\u5de5\u5177\u5728\u8bfe\u4ef6\u751f\u6210\u3001\u4ea4\u4e92\u5f0f\u7b14\u8bb0\u548c\u5185\u5bb9\u8d28\u91cf\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u670d\u52a1\u4e0d\u5b8c\u6574\u3001\u6027\u80fd\u4e0d\u8db3\u548c\u4ea4\u4e92\u6027\u5f31\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u6559\u80b2\u751f\u6210\u7cfb\u7edf\u3002", "method": "EZYer\u91c7\u7528\u4e09\u6a21\u5757\u67b6\u6784\uff1a1\uff09\u6559\u5e08\u6a21\u5757\u6574\u5408\u6587\u672c\u8bed\u6599\u68c0\u7d22\u548c\u6df1\u5ea6\u751f\u6210\u6280\u672f\uff0c\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u6559\u5b66\u6750\u6599\u548cLaTeX Beamer\u8bfe\u4ef6\uff1b2\uff09\u5b66\u751f\u6a21\u5757\u901a\u8fc7\u6559\u5e08\u3001\u52a9\u6559\u3001\u4f18\u7b49\u751f\u548c\u5b66\u56f0\u751f\u56db\u89d2\u8272\u534f\u4f5c\u4ea4\u4e92\uff0c\u751f\u6210\u5b66\u672f\u7b14\u8bb0\uff1b3\uff09\u63a7\u5236\u5668\u8bbe\u7f6e\u5173\u952e\u8bcd\u8fc7\u6ee4\u3001\u5185\u5bb9\u8bc4\u5206\u3001\u89d2\u8272\u534f\u540c\u9a8c\u8bc1\u548c\u52a8\u6001\u5185\u5bb9\u6821\u6b63\u7cfb\u7edf\u786e\u4fdd\u5b66\u672f\u4e25\u8c28\u6027\u3002", "result": "\u901a\u8fc7\u8bbe\u8ba1\u5185\u5bb9\u51c6\u786e\u6027\u3001\u77e5\u8bc6\u8986\u76d6\u5ea6\u3001\u53ef\u7528\u6027\u3001\u683c\u5f0f\u6b63\u786e\u6027\u548c\u89c6\u89c9\u8bbe\u8ba1\u5438\u5f15\u529b\u4e94\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u7531\u4e94\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9EZYer\u751f\u6210\u7684100\u4e2aBeamer\u8bfe\u4ef6\u548c\u7b14\u8bb0\u8fdb\u884c\u8bc4\u5206\uff0c\u7ed3\u679c\u663e\u793aEZYer\u751f\u6210\u7684\u5185\u5bb9\u8d28\u91cf\u4f18\u79c0\uff0c\u5177\u6709\u826f\u597d\u7684\u5e94\u7528\u524d\u666f\u3002", "conclusion": "EZYer\u4f5c\u4e3a\u4e00\u4e2a\u751f\u6210\u5f0f\u6559\u80b2\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u6559\u80b2\u5de5\u5177\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u5757\u534f\u540c\u548c\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\u673a\u5236\uff0c\u4e3a\u9ad8\u4e2d\u6570\u5b66\u6559\u80b2\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u81ea\u52a8\u5316\u5185\u5bb9\u751f\u6210\u670d\u52a1\u3002"}}
{"id": "2512.02682", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02682", "abs": "https://arxiv.org/abs/2512.02682", "authors": ["Piercosma Bisconti", "Marcello Galisai", "Federico Pierucci", "Marcantonio Bracale", "Matteo Prandi"], "title": "Beyond Single-Agent Safety: A Taxonomy of Risks in LLM-to-LLM Interactions", "comment": null, "summary": "This paper examines why safety mechanisms designed for human-model interaction do not scale to environments where large language models (LLMs) interact with each other. Most current governance practices still rely on single-agent safety containment, prompts, fine-tuning, and moderation layers that constrain individual model behavior but leave the dynamics of multi-model interaction ungoverned. These mechanisms assume a dyadic setting: one model responding to one user under stable oversight. Yet research and industrial development are rapidly shifting toward LLM-to-LLM ecosystems, where outputs are recursively reused as inputs across chains of agents. In such systems, local compliance can aggregate into collective failure even when every model is individually aligned. We propose a conceptual transition from model-level safety to system-level safety, introducing the framework of the Emergent Systemic Risk Horizon (ESRH) to formalize how instability arises from interaction structure rather than from isolated misbehavior. The paper contributes (i) a theoretical account of collective risk in interacting LLMs, (ii) a taxonomy connecting micro, meso, and macro-level failure modes, and (iii) a design proposal for InstitutionalAI, an architecture for embedding adaptive oversight within multi-agent systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4e3a\u4ec0\u4e48\u9488\u5bf9\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u7684\u5b89\u5168\u673a\u5236\u65e0\u6cd5\u6269\u5c55\u5230LLM\u76f8\u4e92\u4ea4\u4e92\u7684\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u4e86\u4ece\u6a21\u578b\u7ea7\u5b89\u5168\u5411\u7cfb\u7edf\u7ea7\u5b89\u5168\u7684\u6982\u5ff5\u8f6c\u53d8\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u5174\u7cfb\u7edf\u6027\u98ce\u9669\u89c6\u91ce\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u6cbb\u7406\u5b9e\u8df5\u4e3b\u8981\u4f9d\u8d56\u5355\u667a\u80fd\u4f53\u5b89\u5168\u63a7\u5236\uff0c\u5982\u63d0\u793a\u5de5\u7a0b\u3001\u5fae\u8c03\u548c\u5ba1\u6838\u5c42\uff0c\u8fd9\u4e9b\u673a\u5236\u5047\u8bbe\u4e86\u5355\u6a21\u578b\u5bf9\u5355\u7528\u6237\u7684\u4e8c\u5143\u8bbe\u7f6e\u3002\u7136\u800c\uff0c\u7814\u7a76\u548c\u5de5\u4e1a\u53d1\u5c55\u6b63\u5feb\u901f\u8f6c\u5411LLM\u95f4\u76f8\u4e92\u4ea4\u4e92\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u5728\u8fd9\u79cd\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u6bcf\u4e2a\u6a21\u578b\u90fd\u5355\u72ec\u5bf9\u9f50\uff0c\u5c40\u90e8\u5408\u89c4\u4e5f\u53ef\u80fd\u805a\u5408\u6210\u96c6\u4f53\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u5174\u7cfb\u7edf\u6027\u98ce\u9669\u89c6\u91ce\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u6027\u5982\u4f55\u4ece\u4ea4\u4e92\u7ed3\u6784\u4e2d\u4ea7\u751f\uff0c\u800c\u975e\u5b64\u7acb\u7684\u4e0d\u5f53\u884c\u4e3a\u3002\u8d21\u732e\u5305\u62ec\uff1a(i)\u4ea4\u4e92LLM\u4e2d\u96c6\u4f53\u98ce\u9669\u7684\u7406\u8bba\u89e3\u91ca\uff1b(ii)\u8fde\u63a5\u5fae\u89c2\u3001\u4e2d\u89c2\u548c\u5b8f\u89c2\u5c42\u9762\u6545\u969c\u6a21\u5f0f\u7684\u5206\u7c7b\u6cd5\uff1b(iii)InstitutionalAI\u67b6\u6784\u8bbe\u8ba1\u63d0\u6848\uff0c\u7528\u4e8e\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5d4c\u5165\u81ea\u9002\u5e94\u76d1\u7763\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u4e86\u5f53\u524d\u5b89\u5168\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u5728LLM\u95f4\u4ea4\u4e92\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u5373\u4f7f\u4e2a\u4f53\u6a21\u578b\u90fd\u7b26\u5408\u5b89\u5168\u6807\u51c6\uff0c\u4ea4\u4e92\u7ed3\u6784\u672c\u8eab\u4e5f\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u6027\u98ce\u9669\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u6cbb\u7406\u591a\u6a21\u578b\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u98ce\u9669\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6846\u67b6\u3002", "conclusion": "\u9700\u8981\u4ece\u6a21\u578b\u7ea7\u5b89\u5168\u8303\u5f0f\u8f6c\u5411\u7cfb\u7edf\u7ea7\u5b89\u5168\u8303\u5f0f\uff0c\u901a\u8fc7InstitutionalAI\u7b49\u67b6\u6784\u8bbe\u8ba1\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5d4c\u5165\u81ea\u9002\u5e94\u76d1\u7763\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9LLM\u95f4\u4ea4\u4e92\u751f\u6001\u7cfb\u7edf\u4e2d\u51fa\u73b0\u7684\u65b0\u5174\u7cfb\u7edf\u6027\u98ce\u9669\u3002"}}
{"id": "2512.02300", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02300", "abs": "https://arxiv.org/abs/2512.02300", "authors": ["Haoyu Zheng", "Shouwei Gao", "Jie Ren", "Wenqian Dong"], "title": "DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications", "comment": null, "summary": "Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant chal- lenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while pro- viding quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging dis- aggregated memory in HPC domains while minimally compromis- ing application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degrada- tion to less than 16% while reducing local memory usage by up to 63%, on average.", "AI": {"tldr": "DOLMA\u662f\u4e00\u4e2a\u4e3aHPC\u5e94\u7528\u8bbe\u8ba1\u7684\u6570\u636e\u5bf9\u8c61\u7ea7\u5185\u5b58\u89e3\u805a\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u8bc6\u522b\u548c\u5378\u8f7d\u6570\u636e\u5bf9\u8c61\u5230\u8fdc\u7a0b\u5185\u5b58\uff0c\u5728\u51cf\u5c11\u672c\u5730\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\u5c06\u6027\u80fd\u4e0b\u964d\u63a7\u5236\u572816%\u4ee5\u5185\u3002", "motivation": "\u5185\u5b58\u89e3\u805a\u5408\u6280\u672f\u6709\u671b\u6269\u5c55HPC\u7cfb\u7edf\u7684\u5185\u5b58\u5bb9\u91cf\u5e76\u63d0\u9ad8\u5229\u7528\u7387\uff0c\u4f46\u8bbf\u95ee\u8fdc\u7a0b\u5185\u5b58\u7684\u6027\u80fd\u5f00\u9500\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578bHPC\u5e94\u7528\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u8fd9\u4e9b\u5e94\u7528\u7684\u6267\u884c\u65f6\u95f4\u5bf9\u6570\u636e\u5c40\u90e8\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "method": "DOLMA\u6846\u67b6\u667a\u80fd\u8bc6\u522b\u5e76\u5378\u8f7d\u6570\u636e\u5bf9\u8c61\u5230\u8fdc\u7a0b\u5185\u5b58\uff0c\u63d0\u4f9b\u5b9a\u91cf\u5206\u6790\u4ee5\u786e\u5b9a\u5408\u9002\u7684\u672c\u5730\u5185\u5b58\u5927\u5c0f\u3002\u5b83\u5229\u7528HPC\u5e94\u7528\u5178\u578b\u53ef\u9884\u6d4b\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u901a\u8fc7\u53cc\u7f13\u51b2\u533a\u8bbe\u8ba1\u5b9e\u73b0\u8fdc\u7a0b\u5185\u5b58\u9884\u53d6\uff0c\u5e76\u4ed4\u7ec6\u5e73\u8861\u672c\u5730\u548c\u8fdc\u7a0b\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u7ebf\u7a0b\u5e76\u53d1\u6027\u3002", "result": "\u5728\u516b\u4e2aHPC\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8ba1\u7b97\u5185\u6838\u7684\u8bc4\u4f30\u4e2d\uff0cDOLMA\u5c06\u6027\u80fd\u4e0b\u964d\u9650\u5236\u572816%\u4ee5\u5185\uff0c\u540c\u65f6\u5c06\u672c\u5730\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe63%\uff08\u5e73\u5747\uff09\u3002", "conclusion": "DOLMA\u4e3aHPC\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u9ad8\u6548\u7684\u5185\u5b58\u89e3\u805a\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6700\u5c0f\u5316\u5f71\u54cd\u5e94\u7528\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u5229\u7528\u89e3\u805a\u5408\u5185\u5b58\u3002"}}
{"id": "2512.02080", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.02080", "abs": "https://arxiv.org/abs/2512.02080", "authors": ["PIerre Dantas", "Lucas Cordeiro", "Youcheng Sun", "Waldir Junior"], "title": "The 4/$\u03b4$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee", "comment": "32 pages, 9 figures", "summary": "The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($\u03b4$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $\u03b4> 0$, with an expected iteration count bounded by $\\mathbb{E}[n] \\leq 4/\u03b4$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684LLM-\u9a8c\u8bc1\u5668\u6536\u655b\u5b9a\u7406\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21LLM\u4e0e\u9a8c\u8bc1\u5668\u7684\u4ea4\u4e92\uff0c\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4efb\u4f55\u03b4>0\uff0c\u7a0b\u5e8f\u90fd\u80fd\u4ee5\u671f\u671b\u8fed\u4ee3\u6b21\u6570\u22644/\u03b4\u7684\u6982\u7387\u6536\u655b\u5230\u9a8c\u8bc1\u72b6\u6001\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u6269\u5c55\u4e86\u8f6f\u4ef6\u9a8c\u8bc1\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u9a8c\u8bc1\u8fc7\u7a0b\u4e0d\u53ef\u9760\u3001\u53ef\u80fd\u9677\u5165\u5faa\u73af\u6216\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u5efa\u7acb\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u5c06LLM\u4e0e\u9a8c\u8bc1\u5668\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u72b6\u6001\u8f6c\u79fb\u7531\u5173\u952e\u53c2\u6570\u03b4\uff08\u9519\u8bef\u51cf\u5c11\u6982\u7387\uff09\u51b3\u5b9a\u3002\u5f00\u53d1\u4e86LLM-\u9a8c\u8bc1\u5668\u6536\u655b\u5b9a\u7406\uff0c\u8bc1\u660e\u5bf9\u4e8e\u4efb\u4f55\u03b4>0\uff0c\u7a0b\u5e8f\u90fd\u80fd\u4ee5\u671f\u671b\u8fed\u4ee3\u6b21\u6570\u22644/\u03b4\u7684\u6982\u7387\u6536\u655b\u5230\u9a8c\u8bc1\u72b6\u6001\u3002\u901a\u8fc7\u8d85\u8fc790,000\u6b21\u8bd5\u9a8c\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u8bc1\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff1a\u6240\u6709\u8fd0\u884c\u90fd\u8fbe\u5230\u4e86\u9a8c\u8bc1\u72b6\u6001\uff0c\u6536\u655b\u56e0\u5b50\u7d27\u5bc6\u805a\u96c6\u5728Cf\u22481.0\u9644\u8fd1\u3002\u57fa\u4e8e\u6b64\u5efa\u7acb\u4e86\u4e09\u4e2a\u64cd\u4f5c\u533a\u57df\uff08\u8fb9\u7f18\u3001\u5b9e\u7528\u3001\u9ad8\u6027\u80fd\uff09\u7684\u8bbe\u8ba1\u9608\u503c\uff0c\u4e3aLLM\u8f85\u52a9\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u9884\u6d4b\u7684\u8d44\u6e90\u89c4\u5212\u548c\u6027\u80fd\u9884\u7b97\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u8bc1\u636e\u4e3aLLM\u8f85\u52a9\u9a8c\u8bc1\u5efa\u7acb\u4e86\u6e05\u6670\u7684\u67b6\u6784\u57fa\u7840\uff0c\u6d88\u9664\u4e86\u542f\u53d1\u5f0f\u8c03\u4f18\u7684\u9700\u6c42\uff0c\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u53ef\u9884\u6d4b\u7684\u8d44\u6e90\u89c4\u5212\u548c\u6027\u80fd\u9884\u7b97\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u73af\u5883\u7684\u90e8\u7f72\u3002"}}
{"id": "2512.02546", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02546", "abs": "https://arxiv.org/abs/2512.02546", "authors": ["Jan Meizner", "Maciej Malawski"], "title": "Solutions for Distributed Memory Access Mechanism on HPC Clusters", "comment": null, "summary": "Paper presents and evaluates various mechanisms for remote access to memory in distributed systems based on two distinct HPC clusters. We are comparing solutions based on the shared storage and MPI (over Infiniband and Slingshot) to the local memory access. This paper also mentions medical use-cases that would mostly benefit from the described solution. We have found out that results for remote access esp. backed by MPI are similar to local memory access.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8e\u4e24\u4e2a\u4e0d\u540cHPC\u96c6\u7fa4\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u673a\u5236\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5171\u4eab\u5b58\u50a8\u548cMPI\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u672c\u5730\u5185\u5b58\u8bbf\u95ee\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u57fa\u4e8eMPI\u7684\u8fdc\u7a0b\u8bbf\u95ee\u6027\u80fd\u63a5\u8fd1\u672c\u5730\u5185\u5b58\u8bbf\u95ee", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u673a\u5236\u7684\u6027\u80fd\uff0c\u4e3a\u9700\u8981\u8de8\u8282\u70b9\u5185\u5b58\u8bbf\u95ee\u7684\u5e94\u7528\uff08\u7279\u522b\u662f\u533b\u7597\u7528\u4f8b\uff09\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u5728\u4e24\u4e2a\u4e0d\u540c\u7684HPC\u96c6\u7fa4\u4e0a\uff0c\u6bd4\u8f83\u57fa\u4e8e\u5171\u4eab\u5b58\u50a8\uff08Infiniband\u548cSlingshot\uff09\u7684MPI\u89e3\u51b3\u65b9\u6848\u4e0e\u672c\u5730\u5185\u5b58\u8bbf\u95ee\u7684\u6027\u80fd", "result": "\u57fa\u4e8eMPI\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u6027\u80fd\u4e0e\u672c\u5730\u5185\u5b58\u8bbf\u95ee\u76f8\u4f3c\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u7528\u4f8b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c", "conclusion": "\u57fa\u4e8eMPI\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u673a\u5236\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u6027\u80fd\u63a5\u8fd1\u672c\u5730\u8bbf\u95ee\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u7597\u7b49\u9700\u8981\u8de8\u8282\u70b9\u5185\u5b58\u5171\u4eab\u7684\u5e94\u7528\u573a\u666f"}}
{"id": "2512.02170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02170", "abs": "https://arxiv.org/abs/2512.02170", "authors": ["Pritam Deka", "Barry Devereux"], "title": "Flowchart2Mermaid: A Vision-Language Model Powered System for Converting Flowcharts into Editable Diagram Code", "comment": "Submitted to EACL 2026 Demo Track", "summary": "Flowcharts are common tools for communicating processes but are often shared as static images that cannot be easily edited or reused. We present \\textsc{Flowchart2Mermaid}, a lightweight web system that converts flowchart images into editable Mermaid.js code which is a markup language for visual workflows, using a detailed system prompt and vision-language models. The interface supports mixed-initiative refinement through inline text editing, drag-and-drop node insertion, and natural-language commands interpreted by an integrated AI assistant. Unlike prior image-to-diagram tools, our approach produces a structured, version-controllable textual representation that remains synchronized with the rendered diagram. We further introduce evaluation metrics to assess structural accuracy, flow correctness, syntax validity, and completeness across multiple models.", "AI": {"tldr": "Flowchart2Mermaid\uff1a\u4e00\u4e2a\u5c06\u6d41\u7a0b\u56fe\u56fe\u50cf\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91Mermaid.js\u4ee3\u7801\u7684\u8f7b\u91cf\u7ea7Web\u7cfb\u7edf\uff0c\u652f\u6301\u6df7\u5408\u4e3b\u52a8\u5f0f\u7ec6\u5316\u7f16\u8f91\u548cAI\u52a9\u624b", "motivation": "\u6d41\u7a0b\u56fe\u4f5c\u4e3a\u6d41\u7a0b\u6c9f\u901a\u7684\u5e38\u7528\u5de5\u5177\uff0c\u901a\u5e38\u4ee5\u9759\u6001\u56fe\u50cf\u5f62\u5f0f\u5206\u4eab\uff0c\u96be\u4ee5\u7f16\u8f91\u548c\u91cd\u7528\u3002\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u5c06\u6d41\u7a0b\u56fe\u56fe\u50cf\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u3001\u7248\u672c\u53ef\u63a7\u7684\u6587\u672c\u8868\u793a\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7Web\u7cfb\u7edf\uff0c\u4f7f\u7528\u8be6\u7ec6\u7684\u7cfb\u7edf\u63d0\u793a\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u6d41\u7a0b\u56fe\u56fe\u50cf\u8f6c\u6362\u4e3aMermaid.js\u4ee3\u7801\u3002\u7cfb\u7edf\u652f\u6301\u5185\u8054\u6587\u672c\u7f16\u8f91\u3001\u62d6\u653e\u8282\u70b9\u63d2\u5165\u548c\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\uff0c\u5e76\u96c6\u6210\u4e86AI\u52a9\u624b\u8fdb\u884c\u6df7\u5408\u4e3b\u52a8\u5f0f\u7ec6\u5316\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u3001\u7248\u672c\u53ef\u63a7\u7684\u6587\u672c\u8868\u793a\uff0c\u5e76\u4e0e\u6e32\u67d3\u7684\u56fe\u8868\u4fdd\u6301\u540c\u6b65\u3002\u5f15\u5165\u4e86\u8bc4\u4f30\u6307\u6807\u6765\u8bc4\u4f30\u591a\u4e2a\u6a21\u578b\u7684\u7ed3\u6784\u51c6\u786e\u6027\u3001\u6d41\u7a0b\u6b63\u786e\u6027\u3001\u8bed\u6cd5\u6709\u6548\u6027\u548c\u5b8c\u6574\u6027\u3002", "conclusion": "Flowchart2Mermaid\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06\u9759\u6001\u6d41\u7a0b\u56fe\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91\u3001\u53ef\u91cd\u7528Mermaid.js\u4ee3\u7801\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u9759\u6001\u56fe\u50cf\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u534f\u4f5c\u7f16\u8f91\u548c\u7248\u672c\u63a7\u5236\u3002"}}
{"id": "2512.02646", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02646", "abs": "https://arxiv.org/abs/2512.02646", "authors": ["Alex Barcel\u00f3", "Sebasti\u00e1n A. Cajas Ordo\u00f1ez", "Jaydeep Samanta", "Andr\u00e9s L. Su\u00e1rez-Cetrulo", "Romila Ghosh", "Ricardo Sim\u00f3n Carbajo", "Anna Queralt"], "title": "Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems", "comment": "17 pages, 7 tables, 12 figures", "summary": "The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.\n  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.\n  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e3b\u52a8\u5b58\u50a8\u7684\u8ba1\u7b97\u8fde\u7eed\u4f53\u8f6f\u4ef6\u67b6\u6784\uff0c\u7528\u4e8e\u4f18\u5316AI\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u5d4c\u5165\u5b58\u50a8\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff0c\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6", "motivation": "AI\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\u589e\u957f\uff0c\u4f20\u7edf\u4e91\u67b6\u6784\u96be\u4ee5\u5904\u7406\u6d77\u91cf\u9ad8\u901f\u6570\u636e\uff0c\u73b0\u6709\u6846\u67b6\u7f3a\u4e4f\u8ba1\u7b97\u8fde\u7eed\u4f53\u6240\u9700\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u5e94\u5bf9\u8bbe\u5907\u5f02\u6784\u6027\u548c\u5feb\u901f\u53d8\u5316\u7684\u7b97\u6cd5\u6a21\u578b", "method": "\u63d0\u51fa\u8f6f\u4ef6\u67b6\u6784\uff0c\u5c06AI\u5de5\u4f5c\u8d1f\u8f7d\u65e0\u7f1d\u5206\u5e03\u5230\u8ba1\u7b97\u8fde\u7eed\u4f53\uff0c\u4f7f\u7528\u4e3b\u6d41Python\u5e93\u548cactive storage\u5e73\u53f0dataClay\u5b9e\u73b0\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b58\u50a8\u5c06\u8ba1\u7b97\u5d4c\u5165\u5b58\u50a8\u67b6\u6784", "result": "\u5b9e\u9a8c\u663e\u793a\u4e3b\u52a8\u5b58\u50a8\u5378\u8f7d\u5de5\u4f5c\u8d1f\u8f7d\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u8bc4\u4f30\u4e86\u5185\u5b58\u6d88\u8017\u3001\u5b58\u50a8\u9700\u6c42\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u6267\u884c\u6548\u7387\u7684\u6743\u8861", "conclusion": "\u4e3b\u52a8\u5b58\u50a8\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8AI\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\uff0c\u4f7f\u5206\u5e03\u5f0fAI\u90e8\u7f72\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u4e3a\u9886\u57df\u4e13\u5bb6\u548c\u5e94\u7528\u5f00\u53d1\u8005\u63d0\u4f9b\u6781\u4f4e\u5165\u95e8\u95e8\u69db"}}
{"id": "2512.02683", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02683", "abs": "https://arxiv.org/abs/2512.02683", "authors": ["Luiz A. Rodrigues", "Elias P. Duarte", "Luciana Arantes"], "title": "Distributed and Autonomic Minimum Spanning Trees", "comment": "This preprint is an English translation and slightly extended version of the paper published in Portuguese at the 32nd Brazilian Symposium on Computer Networks and Distributed Systems (2014), reference [1]", "summary": "The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eVCube\u865a\u62df\u62d3\u6251\u7684\u81ea\u9002\u5e94\u751f\u6210\u6811\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e7f\u64ad\u901a\u4fe1\uff0c\u652f\u6301\u52a8\u6001\u6545\u969c\u6062\u590d\uff0c\u6bcf\u4e2a\u8282\u70b9\u5ea6\u6570\u4e0d\u8d85\u8fc7log\u2082n", "motivation": "\u4f20\u7edf\u7684\u4e00\u5bf9\u591a\u5e7f\u64ad\u7b56\u7565\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4e0d\u53ef\u6269\u5c55\uff0c\u4f1a\u7ed9\u53d1\u9001\u8005\u5e26\u6765\u6c89\u91cd\u8d1f\u8f7d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u548c\u7ef4\u62a4\u751f\u6210\u6811\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u53ef\u6269\u5c55\u7684\u5e7f\u64ad\u901a\u4fe1\u3002", "method": "\u57fa\u4e8eVCube\u865a\u62df\u62d3\u6251\u6784\u5efa\u81ea\u9002\u5e94\u751f\u6210\u6811\u7b97\u6cd5\uff0c\u8be5\u62d3\u6251\u540c\u65f6\u4f5c\u4e3a\u6545\u969c\u68c0\u6d4b\u5668\u3002\u7b97\u6cd5\u52a8\u6001\u521b\u5efa\u751f\u6210\u6811\uff0c\u652f\u6301\u8fdb\u7a0b\u6545\u969c\u548c\u6062\u590d\u65f6\u7684\u900f\u660e\u91cd\u5efa\uff0c\u786e\u4fdd\u6bcf\u4e2a\u8282\u70b9\u7684\u5165\u5ea6\u548c\u6811\u6df1\u5ea6\u4e0d\u8d85\u8fc7log\u2082n\u3002", "result": "\u7b97\u6cd5\u80fd\u5bb9\u5fcd\u6700\u591an-1\u4e2a\u8fdb\u7a0b\u6545\u969c\uff0c\u6b63\u786e\u8fdb\u7a0b\u4ecd\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u751f\u6210\u6811\u4fdd\u6301\u8fde\u63a5\u3002\u5f53\u6240\u6709\u8fdb\u7a0b\u6b63\u5e38\u65f6\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u5ea6\u6570\u6070\u597d\u4e3alog\u2082n\u3002\u57fa\u4e8e\u8be5\u7b97\u6cd5\u63d0\u51fa\u4e86\u4e24\u79cd\u5e7f\u64ad\u7b97\u6cd5\uff1a\u5c3d\u529b\u800c\u4e3a\u5e7f\u64ad\u548c\u53ef\u9760\u5e7f\u64ad\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u751f\u6210\u6811\u7b97\u6cd5\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5e7f\u64ad\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7VCube\u62d3\u6251\u652f\u6301\u52a8\u6001\u6545\u969c\u6062\u590d\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u3002"}}
{"id": "2512.02818", "categories": ["cs.DC", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.02818", "abs": "https://arxiv.org/abs/2512.02818", "authors": ["Sean R. Wilkinson", "Patrick Widener", "Sarp Oral", "Rafael Ferreira da Silva"], "title": "Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science", "comment": null, "summary": "High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHPC\u4e2d\u5fc3\u5e94\u66f4\u79ef\u6781\u5730\u57f9\u80b2\u8de8\u5b66\u79d1\u7684FAIR\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec4\u4ef6\u5316\u65b9\u6cd5\u4f7f\u5de5\u4f5c\u6d41\u7ec4\u4ef6\u7b26\u5408FAIR\u539f\u5219\uff0c\u4ee5\u4fc3\u8fdb\u8ba1\u7b97\u7ec4\u4ef6\u7684\u53d1\u73b0\u3001\u5171\u4eab\u548c\u91cd\u7528\u3002", "motivation": "HPC\u4e2d\u5fc3\u7684\u57fa\u7840\u8bbe\u65bd\u4e0e\u7528\u6237\u672c\u5730\u7cfb\u7edf\u5dee\u5f02\u5de8\u5927\uff0c\u5bfc\u81f4\u7528\u6237\u5f00\u53d1\u4e0e\u7279\u5b9aHPC\u4e2d\u5fc3\u7d27\u5bc6\u8026\u5408\u7684\u6570\u5b57\u5236\u54c1\uff0c\u9020\u6210\u91cd\u590d\u52b3\u52a8\u3002\u73b0\u6709FAIR\u5b9e\u8df5\u591a\u4e3a\u9886\u57df\u7279\u5b9a\uff0c\u9650\u5236\u4e86\u8de8\u5b66\u79d1\u534f\u4f5c\u3002", "method": "\u57fa\u4e8e\u6b27\u6d32\u5f00\u653e\u79d1\u5b66\u4e91EOSC-Life FAIR\u5de5\u4f5c\u6d41\u534f\u4f5c\u5e73\u53f0\u7684\u67b6\u6784\uff0c\u63d0\u51fa\u9488\u5bf9HPC\u9700\u6c42\u7684\u6a21\u578b\uff0c\u5f3a\u8c03\u4f7f\u5355\u4e2a\u5de5\u4f5c\u6d41\u7ec4\u4ef6\u7b26\u5408FAIR\u539f\u5219\uff0c\u800c\u975e\u6574\u4e2a\u5de5\u4f5c\u6d41\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u4ef6\u5316\u7684FAIR\u65b9\u6cd5\uff0c\u66f4\u597d\u5730\u652f\u6301HPC\u7528\u6237\u591a\u6837\u5316\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u9700\u6c42\uff0c\u540c\u65f6\u6700\u5927\u5316\u5176\u5de5\u4f5c\u7684\u957f\u671f\u4ef7\u503c\u3002", "conclusion": "HPC\u4e2d\u5fc3\u5e94\u8bbe\u8ba1\u652f\u6301\u8de8\u5b66\u79d1FAIR\u751f\u6001\u7cfb\u7edf\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u7ec4\u4ef6\u5316FAIR\u65b9\u6cd5\u4fc3\u8fdb\u8ba1\u7b97\u7ec4\u4ef6\u7684\u6709\u6548\u53d1\u73b0\u3001\u5171\u4eab\u548c\u91cd\u7528\uff0c\u51cf\u5c11\u91cd\u590d\u52b3\u52a8\u5e76\u589e\u5f3a\u534f\u4f5c\u3002"}}
{"id": "2512.02280", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02280", "abs": "https://arxiv.org/abs/2512.02280", "authors": ["Noorbakhsh Amiri Golilarz", "Sindhuja Penchala", "Shahram Rahimi"], "title": "Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence", "comment": null, "summary": "Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5f53\u524dAI\u7cfb\u7edf\u7684\u4e03\u5927\u6838\u5fc3\u7f3a\u9677\uff0c\u6307\u51fa\u5176\u7f3a\u4e4f\u81ea\u6211\u76d1\u63a7\u3001\u5143\u8ba4\u77e5\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7b49\u80fd\u529b\uff0c\u4e3b\u5f20\u5411\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684AI\u67b6\u6784\u8f6c\u53d8\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u611f\u77e5\u3001\u8bed\u8a00\u3001\u63a8\u7406\u548c\u591a\u6a21\u6001\u9886\u57df\u53d6\u5f97\u5feb\u901f\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5728\u81ea\u6211\u76d1\u63a7\u3001\u81ea\u6211\u6821\u6b63\u548c\u81ea\u4e3b\u884c\u4e3a\u8c03\u8282\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7a33\u5065\u7684\u6cdb\u5316\u3001\u7ec8\u8eab\u9002\u5e94\u6027\u548c\u771f\u5b9e\u4e16\u754c\u81ea\u4e3b\u6027\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u548c\u5206\u6790\u5f53\u4ee3AI\u6a21\u578b\u7684\u4e03\u5927\u6838\u5fc3\u7f3a\u9677\uff0c\u7ed3\u5408\u4eba\u5de5\u7cfb\u7edf\u4e0e\u751f\u7269\u8ba4\u77e5\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u6574\u5408AI\u7814\u7a76\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u89c1\u89e3\uff0c\u8bba\u8bc1\u4e3a\u4ec0\u4e48\u5355\u7eaf\u6269\u5c55\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "result": "\u8bba\u6587\u786e\u5b9a\u4e86\u4e03\u4e2a\u9650\u5236\u5f53\u4ee3AI\u6a21\u578b\u7684\u5173\u952e\u7f3a\u9677\uff1a\u7f3a\u4e4f\u5185\u5728\u81ea\u6211\u76d1\u63a7\u3001\u5143\u8ba4\u77e5\u610f\u8bc6\u7f3a\u5931\u3001\u56fa\u5b9a\u975e\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\u3001\u65e0\u6cd5\u91cd\u7ec4\u76ee\u6807\u3001\u7f3a\u4e4f\u8868\u5f81\u7ef4\u62a4\u3001\u4e0d\u8db3\u7684\u5177\u8eab\u53cd\u9988\u4ee5\u53ca\u5185\u5728\u80fd\u52a8\u6027\u7f3a\u5931\u3002", "conclusion": "\u4e3b\u5f20\u5411\u8ba4\u77e5\u57fa\u7840AI\uff08\u8ba4\u77e5\u81ea\u4e3b\u6027\uff09\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u8fd9\u79cdAI\u80fd\u591f\u5b9e\u73b0\u81ea\u6211\u5bfc\u5411\u7684\u9002\u5e94\u3001\u52a8\u6001\u8868\u5f81\u7ba1\u7406\u548c\u6709\u610f\u56fe\u7684\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\uff0c\u540c\u65f6\u914d\u5907\u6539\u9769\u6027\u76d1\u7763\u673a\u5236\uff0c\u786e\u4fdd\u81ea\u4e3b\u7cfb\u7edf\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6cbb\u7406\u6027\u5e76\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002"}}
{"id": "2512.02283", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02283", "abs": "https://arxiv.org/abs/2512.02283", "authors": ["Bin Xu", "Ayan Banerjee", "Sandeep K. S. Gupta"], "title": "Model Recovery at the Edge under Resource Constraints for Physical AI", "comment": "Published in ECAI 2025, Frontiers in Artificial Intelligence and Applications, volume 413, pages 3904-3911", "summary": "Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.", "AI": {"tldr": "MERINDA\u662f\u4e00\u4e2aFPGA\u52a0\u901f\u7684\u6a21\u578b\u6062\u590d\u6846\u67b6\uff0c\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u81ea\u4e3b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u795e\u7ecf\u67b6\u6784\u66ff\u4ee3\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u80fd\u8017\u3002", "motivation": "\u6a21\u578b\u6062\u590d\uff08MR\uff09\u5728\u5173\u952e\u4efb\u52a1\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u80fd\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff08NODEs\uff09\u7684\u65b9\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u56e0\u4e3a\u8fed\u4ee3\u6c42\u89e3\u5668\u5728FPGA\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u5185\u5b58\u548c\u80fd\u8017\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faMERINDA\u6846\u67b6\uff0c\u7528\u53ef\u5e76\u884c\u5316\u7684\u795e\u7ecf\u67b6\u6784\u66ff\u4ee3NODEs\u4e2d\u7684\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u4e13\u95e8\u9488\u5bf9FPGA\u52a0\u901f\u4f18\u5316\uff0c\u51cf\u5c11DRAM\u4f7f\u7528\u5e76\u63d0\u9ad8\u8fd0\u884c\u6548\u7387\u3002", "result": "\u76f8\u6bd4\u79fb\u52a8GPU\uff0cMERINDA\u5b9e\u73b0\u4e86\u8fd111\u500d\u7684DRAM\u4f7f\u7528\u91cf\u964d\u4f4e\u548c2.2\u500d\u7684\u8fd0\u884c\u901f\u5ea6\u63d0\u5347\u3002\u5b9e\u9a8c\u8fd8\u63ed\u793a\u4e86\u5728\u56fa\u5b9a\u7cbe\u5ea6\u4e0b\u5185\u5b58\u4e0e\u80fd\u8017\u7684\u9006\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "MERINDA\u6846\u67b6\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u3001\u9700\u8981\u5b9e\u65f6\u64cd\u4f5c\u7684\u5173\u952e\u4efb\u52a1\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u80fd\u8017\u7684\u6a21\u578b\u6062\u590d\u3002"}}
{"id": "2512.02302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02302", "abs": "https://arxiv.org/abs/2512.02302", "authors": ["Varun Kumar Dasoju", "Qingsu Cheng", "Zeyun Yu"], "title": "Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization", "comment": "9 pages, 3 figures, 2 tables", "summary": "Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e73\u817a\u4e0a\u76ae\u7ec6\u80de\u5206\u5272\u7684\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528599\u5f20\u8bad\u7ec3\u56fe\u50cf\u5c31\u5b9e\u73b0\u4e8695.5%\u7684Dice\u5206\u6570\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u6240\u9700\u7684\u65f6\u95f4\u548c\u4e13\u5bb6\u8d44\u6e90\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u9700\u8981\u5927\u91cf\u65f6\u95f4\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u4e73\u817a\u4e0a\u76ae\u7ec6\u80de\u6570\u636e\u96c6\u9700\u8981\u75c5\u7406\u5b66\u5bb6\u6295\u5165\u6570\u767e\u5c0f\u65f6\u8fdb\u884c\u6807\u6ce8\u3002\u8fd9\u662f\u4e00\u4e2a\u4e34\u5e8a\u611f\u77e5AI\u53d1\u5c55\u7684\u57fa\u672c\u74f6\u9888\u3002", "method": "1. \u4f7f\u7528\u91cf\u5b50\u542f\u53d1\u7684\u591a\u5c3a\u5ea6Gabor\u6ee4\u6ce2\u5668\u8fdb\u884c\u8fb9\u7f18\u589e\u5f3a\uff0c\u521b\u5efa\u7b2c\u56db\u8f93\u5165\u901a\u9053\uff1b2. \u63d0\u51fa\u7a33\u5b9a\u7684\u591a\u7ec4\u4ef6\u635f\u5931\u51fd\u6570\uff0c\u96c6\u6210\u81ea\u9002\u5e94Dice\u635f\u5931\u548c\u8fb9\u754c\u611f\u77e5\u9879\uff1b3. \u5f15\u5165\u57fa\u4e8e\u590d\u6742\u5ea6\u7684\u52a0\u6743\u91c7\u6837\u7b56\u7565\uff1b4. \u91c7\u7528EfficientNet-B7/UNet++\u67b6\u6784\uff0c\u652f\u63014\u52303\u901a\u9053\u6295\u5f71\uff1b5. \u901a\u8fc7\u6307\u6570\u79fb\u52a8\u5e73\u5747\u548c\u7edf\u8ba1\u5f02\u5e38\u503c\u68c0\u6d4b\u8fdb\u884c\u9c81\u68d2\u9a8c\u8bc1\u3002", "result": "Dice\u5206\u6570\u8fbe\u523095.5% \u00b1 0.3%\uff0cIoU\u8fbe\u523091.2% \u00b1 0.4%\u3002\u91cf\u5b50\u589e\u5f3a\u4f7f\u8fb9\u754c\u7cbe\u5ea6\u63d0\u9ad82.1%\uff0c\u52a0\u6743\u91c7\u6837\u4f7f\u5c0f\u75c5\u7076\u68c0\u6d4b\u63d0\u9ad83.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u9650\u6807\u6ce8\u5b9e\u73b0\u4e86\u7a81\u7834\u6027\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u533b\u5b66\u4e13\u5bb6\u521b\u5efa\u6570\u636e\u96c6\u6240\u9700\u7684\u65f6\u95f4\uff0c\u89e3\u51b3\u4e86\u4e34\u5e8a\u611f\u77e5AI\u53d1\u5c55\u7684\u57fa\u672c\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2512.02306", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02306", "abs": "https://arxiv.org/abs/2512.02306", "authors": ["Boyu Zhu", "Xiaofei Wen", "Wenjie Jacky Mo", "Tinghui Zhu", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning", "comment": null, "summary": "Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.", "AI": {"tldr": "OmniGuard\u662f\u9996\u4e2a\u5168\u6a21\u6001\u5b89\u5168\u62a4\u680f\u7cfb\u7edf\uff0c\u901a\u8fc7\u7cbe\u5fc3\u63a8\u7406\u80fd\u529b\u5728\u6240\u6709\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\uff09\u4e0a\u6267\u884c\u5b89\u5168\u9632\u62a4\uff0c\u4f7f\u7528\u8d85\u8fc721\u4e07\u6837\u672c\u7684\u5168\u9762\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u591a\u79cd\u6a21\u6001\u6570\u636e\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6311\u6218\uff0c\u73b0\u6709\u62a4\u680f\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u5355\u6a21\u6001\u8bbe\u7f6e\uff0c\u4e14\u901a\u5e38\u5c06\u5b89\u5168\u9632\u62a4\u89c6\u4e3a\u4e8c\u5143\u5206\u7c7b\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u4e0d\u540c\u6a21\u6001\u548c\u4efb\u52a1\u95f4\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faOmniGuard\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u8d85\u8fc721\u4e07\u591a\u6837\u5316\u6837\u672c\u7684\u5168\u6a21\u6001\u5b89\u5168\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6240\u6709\u6a21\u6001\u7684\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u5e26\u6709\u7ed3\u6784\u5316\u5b89\u5168\u6807\u7b7e\u548c\u901a\u8fc7\u5b9a\u5411\u84b8\u998f\u4ece\u4e13\u5bb6\u6a21\u578b\u83b7\u5f97\u7684\u7cbe\u5fc3\u5b89\u5168\u8bc4\u6790\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOmniGuard\u5728\u5404\u79cd\u591a\u6a21\u6001\u5b89\u5168\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5168\u6a21\u6001\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "OmniGuard\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5728\u5168\u6a21\u6001\u4e2d\u6267\u884c\u7b56\u7565\u548c\u7f13\u89e3\u98ce\u9669\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u66f4\u5f3a\u5927\u7684\u5168\u6a21\u6001\u5b89\u5168\u9632\u62a4\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.02340", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02340", "abs": "https://arxiv.org/abs/2512.02340", "authors": ["Qiyao Xue", "Weichen Liu", "Shiqi Wang", "Haoming Wang", "Yuyang Wu", "Wei Gao"], "title": "Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective", "comment": "23 pages, 37 figures", "summary": "Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ReMindView-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u89c6\u56fe\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8de8\u89c6\u56fe\u5bf9\u9f50\u548c\u89c6\u89d2\u8f6c\u6362\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u89c6\u56fe\u7a7a\u95f4\u63a8\u7406\u4e2d\u96be\u4ee5\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u80fd\u591f\u9694\u79bb\u591a\u89c6\u56fe\u63a8\u7406\u4e0e\u5355\u89c6\u56fe\u611f\u77e5\u53ca\u65f6\u95f4\u56e0\u7d20\u7684\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86ReMindView-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u6027\u5730\u53d8\u5316\u89c6\u89d2\u7a7a\u95f4\u6a21\u5f0f\u548c\u67e5\u8be2\u7c7b\u578b\u6765\u63a2\u6d4b\u7a7a\u95f4\u8ba4\u77e5\u5173\u952e\u56e0\u7d20\uff1b\u4f7f\u7528LLM-as-a-judge\u548c\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u8fdb\u884c\u663e\u5f0f\u5206\u9636\u6bb5\u5206\u6790\uff0c\u4ee5\u53ca\u7ebf\u6027\u63a2\u6d4b\u548c\u71b5\u52a8\u6001\u5206\u6790\u8fdb\u884c\u9690\u5f0f\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u4e8615\u4e2a\u5f53\u524dVLMs\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u8de8\u89c6\u56fe\u5bf9\u9f50\u548c\u89c6\u89d2\u8f6c\u6362\u65b9\u9762\u5b58\u5728\u4e00\u81f4\u6027\u7684\u5931\u8d25\uff1b\u663e\u5f0f\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u5e27\u5185\u611f\u77e5\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u89c6\u56fe\u4fe1\u606f\u6574\u5408\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff1b\u9690\u5f0f\u5206\u6790\u663e\u793a\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u9010\u6e10\u4e22\u5931\uff0c\u6b63\u786e\u4e0e\u9519\u8bef\u8f68\u8ff9\u4e4b\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u79bb\u3002", "conclusion": "ReMindView-Bench\u4e3aVLM\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u8ba4\u77e5\u57fa\u7840\u7684\u8bca\u65ad\uff0c\u63ed\u793a\u4e86\u591a\u89c6\u56fe\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5f62\u6210\u3001\u9000\u5316\u548c\u5931\u7a33\u673a\u5236\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2512.02358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02358", "abs": "https://arxiv.org/abs/2512.02358", "authors": ["Ran Zhang", "Kun Ouyang", "Tiancheng Ma", "Yida Yang", "Dong Fang"], "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games", "comment": null, "summary": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.", "AI": {"tldr": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6784\u5efaMMO\u6e38\u620f\u6a21\u62df\u7cfb\u7edf\uff0c\u901a\u8fc7SFT\u548cRL\u8bad\u7ec3\u667a\u80fd\u4f53\u6a21\u4eff\u771f\u5b9e\u73a9\u5bb6\u884c\u4e3a\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u73af\u5883\u6a21\u578b\uff0c\u4e3a\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u6210\u672c\u9ad8\u6548\u7684\u6846\u67b6\u3002", "motivation": "\u4f20\u7edfMMO\u6e38\u620f\u6570\u503c\u7cfb\u7edf\u548c\u673a\u5236\u8bbe\u8ba1\u4f18\u5316\u4f9d\u8d56\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\u6216\u57fa\u4e8e\u9884\u5b9a\u4e49\u7edf\u8ba1\u6a21\u578b\u7684\u53c2\u6570\u8c03\u4f18\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u53ef\u80fd\u5e72\u6270\u73a9\u5bb6\u4f53\u9a8c\u3002\u867d\u7136\u7b80\u5316\u79bb\u7ebf\u6a21\u62df\u7cfb\u7edf\u53ef\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u4fdd\u771f\u5ea6\u6709\u9650\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u73a9\u5bb6\u7684\u63a8\u7406\u548c\u5bf9\u5e72\u9884\u7684\u53cd\u5e94\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53MMO\u6a21\u62df\u7cfb\u7edf\uff1a1) \u5728\u5927\u89c4\u6a21\u771f\u5b9e\u73a9\u5bb6\u884c\u4e3a\u6570\u636e\u4e0a\u5e94\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u5c06LLM\u4ece\u901a\u7528\u5148\u9a8c\u9002\u5e94\u5230\u6e38\u620f\u7279\u5b9a\u9886\u57df\uff1b2) \u57fa\u4e8e\u771f\u5b9e\u6e38\u620f\u65e5\u5fd7\u8bad\u7ec3\u6570\u636e\u9a71\u52a8\u7684\u73af\u5883\u6a21\u578b\uff0c\u91cd\u5efa\u52a8\u6001\u6e38\u620f\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u4e0e\u771f\u5b9e\u4e16\u754c\u73a9\u5bb6\u884c\u4e3a\u5177\u6709\u5f88\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5e72\u9884\u4e0b\u4ea7\u751f\u5408\u7406\u7684\u56e0\u679c\u54cd\u5e94\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u6210\u672c\u9ad8\u6548\u7684\u6846\u67b6\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6a21\u62df\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u73a9\u5bb6\u51b3\u7b56\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u73af\u5883\u6a21\u578b\uff0c\u4e3aMMO\u6e38\u620f\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02389", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02389", "abs": "https://arxiv.org/abs/2512.02389", "authors": ["David X. Wu", "Shreyas Kapur", "Anant Sahai", "Stuart Russell"], "title": "Synthetic Error Injection Fails to Elicit Self-Correction In Language Models", "comment": "13 pages, 12 figures", "summary": "Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u548c\u4eba\u5de5\u9519\u8bef\u6ce8\u5165\u6765\u57f9\u517b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u4f46\u53d1\u73b0\u8fd9\u79cd\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u65e0\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5373\u4f7f\u6a21\u578b\u80fd\u53d1\u73b0\u9519\u8bef\u4e5f\u5e38\u5e38\u91cd\u590d\u539f\u9519\u8bef\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u3002\u53d7\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u542f\u53d1\uff0c\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u548c\u5408\u6210\u9519\u8bef\u6ce8\u5165\u6765\u8bf1\u5bfc\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u94fe\u4e2d\u63d2\u5165\u4eba\u5de5\u9519\u8bef\uff0c\u7136\u540e\u63a9\u76d6\u8fd9\u4e9b\u9519\u8bef\uff0c\u5e76\u76d1\u7763\u6a21\u578b\u8bc6\u522b\u548c\u7ea0\u6b63\u8fd9\u4e9b\u9519\u8bef\u3002\u901a\u8fc7\u5408\u6210\u9519\u8bef\u6ce8\u5165\u7684\u65b9\u5f0f\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u81ea\u6211\u7ea0\u6b63\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8be5\u65b9\u6cd5\u5373\u4f7f\u5728\u7b80\u5355\u7684\u5408\u6210\u4efb\u52a1\u4e0a\u4e5f\u65e0\u6cd5\u663e\u8457\u63d0\u5347\u591a\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u3002\u5373\u4f7f\u6a21\u578b\u80fd\u591f\u53d1\u73b0\u81ea\u5df1\u7684\u9519\u8bef\uff0c\u4e5f\u5e38\u5e38\u4f1a\u91cd\u590d\u539f\u6765\u7684\u9519\u8bef\u3002\u5408\u6210\u9519\u8bef\u4e0e\u5728\u7ebf\u7b56\u7565\u9519\u8bef\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u663e\u8457\u964d\u4f4e\u4e86\u5fae\u8c03\u6a21\u578b\u7684\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\uff0c\u5373\u4f7f\u5408\u6210\u9519\u8bef\u8986\u76d6\u4e86\u5728\u7ebf\u7b56\u7565\u9519\u8bef\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5728\u7ebf\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6fc0\u53d1\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u65b9\u9762\u88ab\u8bc1\u660e\u662f\u72ec\u7279\u6709\u6548\u7684\u3002\u76d1\u7763\u5b66\u4e60\u4e0e\u5408\u6210\u9519\u8bef\u6ce8\u5165\u7684\u65b9\u6cd5\u867d\u7136\u76f4\u89c2\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u6709\u9650\uff0c\u65e0\u6cd5\u66ff\u4ee3\u5f3a\u5316\u5b66\u4e60\u5728\u57f9\u517b\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u65b9\u9762\u7684\u4f5c\u7528\u3002"}}
{"id": "2512.02436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02436", "abs": "https://arxiv.org/abs/2512.02436", "authors": ["Agostino Capponi", "Alfio Gliozzo", "Brian Zhu"], "title": "Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets", "comment": null, "summary": "Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fdAI\u7684\u7ba1\u9053\uff0c\u80fd\u591f\u81ea\u52a8\u805a\u7c7b\u9884\u6d4b\u5e02\u573a\u5e76\u8bc6\u522b\u5e02\u573a\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u4ea4\u6613\u7b56\u7565\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u5173\u7cfb\u7684\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u9884\u6d4b\u5e02\u573a\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u5305\u62ec\u91cd\u53e0\u95ee\u9898\u3001\u9690\u542b\u7b49\u4ef7\u6027\u548c\u9690\u85cf\u77db\u76fe\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u53d1\u73b0\u5e02\u573a\u95f4\u7684\u8bed\u4e49\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fdAI\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u5bf9\u5408\u540c\u6587\u672c\u548c\u5143\u6570\u636e\u8fdb\u884c\u805a\u7c7b\u5206\u6790\uff0c\u8bc6\u522b\u96c6\u7fa4\u5185\u5e02\u573a\u5bf9\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff08\u6b63\u76f8\u5173\u548c\u53cd\u76f8\u5173\uff09\u3002", "result": "\u5728Polymarket\u5386\u53f2\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cAI\u8bc6\u522b\u7684\u5173\u7cfb\u51c6\u786e\u7387\u8fbe\u523060-70%\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u5173\u7cfb\u7684\u4ea4\u6613\u7b56\u7565\u5728\u4e00\u5468\u5185\u83b7\u5f97\u7ea620%\u7684\u5e73\u5747\u56de\u62a5\u3002", "conclusion": "\u667a\u80fdAI\u548c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u53d1\u73b0\u9884\u6d4b\u5e02\u573a\u4e2d\u6f5c\u5728\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u8fd9\u4e9b\u53d1\u73b0\u7684\u5173\u7cfb\u53ef\u4ee5\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u5e02\u573a\u4fe1\u53f7\u3002"}}
{"id": "2512.02472", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02472", "abs": "https://arxiv.org/abs/2512.02472", "authors": ["Wenhao Yu", "Zhenwen Liang", "Chengsong Huang", "Kishan Panaganti", "Tianqing Fang", "Haitao Mi", "Dong Yu"], "title": "Guided Self-Evolving LLMs with Minimal Human Supervision", "comment": null, "summary": "AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.", "AI": {"tldr": "R-Few\u662f\u4e00\u4e2a\u5f15\u5bfc\u5f0f\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4eba\u7c7b\u76d1\u7763\u5b9e\u73b0AI\u6a21\u578b\u7684\u7a33\u5b9a\u53ef\u63a7\u81ea\u6211\u8fdb\u5316\uff0c\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u6301\u7eed\u6539\u8fdb\u3002", "motivation": "AI\u81ea\u8fdb\u5316\u88ab\u8ba4\u4e3a\u662f\u901a\u5f80\u8d85\u667a\u80fd\u7684\u8def\u5f84\uff0c\u4f46\u5b9e\u8df5\u4e2d\u65e0\u5f15\u5bfc\u7684\u81ea\u8fdb\u5316\u7cfb\u7edf\u5f80\u5f80\u5f88\u5feb\u8fbe\u5230\u5e73\u53f0\u671f\u751a\u81f3\u9000\u5316\uff0c\u8fd9\u662f\u7531\u4e8e\u6982\u5ff5\u6f02\u79fb\u3001\u591a\u6837\u6027\u5d29\u6e83\u548c\u9519\u8bef\u8fdb\u5316\u7b49\u95ee\u9898\u5bfc\u81f4\u7684\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7a33\u5b9a\u53ef\u63a7\u5730\u81ea\u6211\u8fdb\u5316\u540c\u65f6\u6700\u5c0f\u5316\u4eba\u7c7b\u76d1\u7763\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faR-Few\u6846\u67b6\uff0c\u91c7\u7528\u5f15\u5bfc\u5f0f\u81ea\u5bf9\u5f08\u7684\u6311\u6218\u8005-\u6c42\u89e3\u5668\u67b6\u6784\u3002\u6311\u6218\u8005\u901a\u8fc7\u5c11\u91cf\u4eba\u7c7b\u6807\u6ce8\u793a\u4f8b\u6307\u5bfc\u5408\u6210\u95ee\u9898\u751f\u6210\uff0c\u6c42\u89e3\u5668\u5728\u5728\u7ebf\u96be\u5ea6\u8bfe\u7a0b\u4e0b\u8054\u5408\u8bad\u7ec3\u4eba\u7c7b\u548c\u5408\u6210\u793a\u4f8b\u3002\u7ed3\u5408\u4e0a\u4e0b\u6587\u57fa\u7840\u8bad\u7ec3\u548c\u6df7\u5408\u8bad\u7ec3\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u4eba\u7c7b\u76d1\u7763\u3002", "result": "\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cR-Few\u5b9e\u73b0\u4e86\u4e00\u81f4\u4e14\u8fed\u4ee3\u7684\u6539\u8fdb\u3002\u4f8b\u5982\uff0cQwen3-8B-Base\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u6bd4R-Zero\u63d0\u9ad8\u4e863.0\u5206\uff0c\u6027\u80fd\u4e0eGeneral-Reasoner\u76f8\u5f53\uff0c\u5c3d\u7ba1\u540e\u8005\u4f7f\u7528\u4e8620\u500d\u7684\u4eba\u7c7b\u6570\u636e\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u57fa\u7840\u6311\u6218\u8005\u8bad\u7ec3\u548c\u8bfe\u7a0b\u6c42\u89e3\u5668\u8bad\u7ec3\u7684\u4e92\u8865\u8d21\u732e\u3002", "conclusion": "R-Few\u901a\u8fc7\u5f15\u5bfc\u5f0f\u81ea\u5bf9\u5f08\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u4ea7\u751f\u4e86\u66f4\u7a33\u5b9a\u53ef\u63a7\u7684\u534f\u540c\u8fdb\u5316\u52a8\u6001\uff0c\u4e3aAI\u81ea\u8fdb\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.02499", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02499", "abs": "https://arxiv.org/abs/2512.02499", "authors": ["Yongkai Liu", "Helena Feng", "Bin Jiang", "Yixin Wang", "Max Wintermark", "David S. Liebeskind", "Michael Moseley", "Maarten Lansberg", "Gregory Albers", "Jeremy Heit", "Greg Zaharchuk"], "title": "COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes", "comment": null, "summary": "Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.", "AI": {"tldr": "COPE\u662f\u4e00\u4e2a\u57fa\u4e8e\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u9884\u6d4b\u6025\u6027\u7f3a\u8840\u6027\u5352\u4e2d90\u5929\u529f\u80fd\u7ed3\u5c40\uff0c\u6027\u80fd\u4e0eGPT-4.1\u76f8\u5f53\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u7279\u6027\u9650\u5236\u4e86\u5728\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u4e2d\u7684\u4f7f\u7528\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86COPE\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eLLaMA-3-8B\u7684\u4e24\u6b65\u601d\u7ef4\u94fe\u67b6\u6784\uff1a\u7b2c\u4e00\u6b65\u751f\u6210\u4e34\u5e8a\u63a8\u7406\uff0c\u7b2c\u4e8c\u6b65\u8f93\u51famRS\u9884\u6d4b\u3002\u5728464\u540dAIS\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "COPE\u7684MAE\u4e3a1.01\uff0c\u00b11\u51c6\u786e\u7387\u4e3a74.4%\uff0c\u7cbe\u786e\u51c6\u786e\u7387\u4e3a32.8%\uff0c\u6027\u80fd\u4e0eGPT-4.1\u76f8\u5f53\uff0c\u4f18\u4e8eClinicalBERT\u3001\u57fa\u4e8e\u7ed3\u6784\u5316\u53d8\u91cf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5355\u6b65LLM\u3002", "conclusion": "COPE\u4f5c\u4e3a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u4e3a\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\u8fdb\u884c\u7ed3\u5c40\u9884\u6d4b\u63d0\u4f9b\u4e86\u51c6\u786e\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02530", "abs": "https://arxiv.org/abs/2512.02530", "authors": ["Yuxiang He", "Jian Zhao", "Yuchen Yuan", "Tianle Zhang", "Wei Cai", "Haojie Cheng", "Ziyan Shi", "Ming Zhu", "Haichuan Tang", "Chi Zhang", "Xuelong Li"], "title": "Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration", "comment": "https://github.com/Herrieson/Aetheria", "summary": "The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.", "AI": {"tldr": "Aetheria\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u534f\u4f5c\u7684\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u5185\u5bb9\u5b89\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8fa9\u8bba\u673a\u5236\u548cRAG\u77e5\u8bc6\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u9690\u5f0f\u98ce\u9669\u8bc6\u522b\u80fd\u529b\u5e76\u751f\u6210\u53ef\u8ffd\u6eaf\u7684\u5ba1\u8ba1\u62a5\u544a\u3002", "motivation": "\u6570\u5b57\u5185\u5bb9\u7206\u70b8\u5f0f\u589e\u957f\u7ed9\u5185\u5bb9\u5b89\u5168\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u5ba1\u6838\u7cfb\u7edf\u901a\u5e38\u57fa\u4e8e\u5355\u4e00\u6a21\u578b\u6216\u56fa\u5b9a\u6d41\u7a0b\uff0c\u5728\u8bc6\u522b\u9690\u5f0f\u98ce\u9669\u548c\u63d0\u4f9b\u53ef\u89e3\u91ca\u5224\u65ad\u8fc7\u7a0b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faAetheria\u6846\u67b6\uff0c\u91c7\u7528\u4e94\u4e2a\u6838\u5fc3\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u67b6\u6784\uff0c\u901a\u8fc7\u57fa\u4e8eRAG\u77e5\u8bc6\u68c0\u7d22\u7684\u52a8\u6001\u76f8\u4e92\u8bf4\u670d\u8fa9\u8bba\u673a\u5236\uff0c\u5bf9\u591a\u6a21\u6001\u5185\u5bb9\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\u548c\u88c1\u51b3\u3002", "result": "\u5728\u63d0\u51fa\u7684AIR-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAetheria\u4e0d\u4ec5\u751f\u6210\u8be6\u7ec6\u53ef\u8ffd\u6eaf\u7684\u5ba1\u8ba1\u62a5\u544a\uff0c\u800c\u4e14\u5728\u6574\u4f53\u5185\u5bb9\u5b89\u5168\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9690\u5f0f\u98ce\u9669\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u900f\u660e\u53ef\u89e3\u91ca\u7684\u8303\u5f0f\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u53ef\u4fe1AI\u5185\u5bb9\u5ba1\u6838\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.02610", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02610", "abs": "https://arxiv.org/abs/2512.02610", "authors": ["Yubo Hou", "Mohamed Ragab", "Min Wu", "Chee-Keong Kwoh", "Xiaoli Li", "Zhenghua Chen"], "title": "Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction", "comment": null, "summary": "Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.", "AI": {"tldr": "TACDA\u662f\u4e00\u79cd\u7528\u4e8e\u8de8\u57df\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u9884\u6d4b\u7684\u65b0\u578b\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u57df\u91cd\u6784\u7b56\u7565\u548c\u805a\u7c7b\u914d\u5bf9\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u673a\u68b0\u8bbe\u5907\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u3001\u63d0\u9ad8\u8bbe\u5907\u8fd0\u884c\u65f6\u95f4\u5e76\u51cf\u5c11\u4e0d\u826f\u540e\u679c\u3002\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8RUL\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6765\u81ea\u76f8\u540c\u5206\u5e03\uff0c\u4f46\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u8fd9\u4e00\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u5b58\u5728\u57df\u5dee\u5f02\u95ee\u9898\u3002\u73b0\u6709\u7684\u5bf9\u6297\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u867d\u7136\u5173\u6ce8\u83b7\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u4f46\u5ffd\u7565\u4e86\u76ee\u6807\u7279\u5b9a\u4fe1\u606f\u548c\u9000\u5316\u9636\u6bb5\u7684\u4e00\u81f4\u6027\u7279\u5f81\u3002", "method": "\u63d0\u51faTACDA\u65b9\u6cd5\uff1a1\uff09\u5728\u5bf9\u6297\u81ea\u9002\u5e94\u8fc7\u7a0b\u4e2d\u5f15\u5165\u76ee\u6807\u57df\u91cd\u6784\u7b56\u7565\uff0c\u5728\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\u7684\u540c\u65f6\u4fdd\u7559\u76ee\u6807\u7279\u5b9a\u4fe1\u606f\uff1b2\uff09\u5f00\u53d1\u65b0\u9896\u7684\u805a\u7c7b\u548c\u914d\u5bf9\u7b56\u7565\uff0c\u5b9e\u73b0\u76f8\u4f3c\u9000\u5316\u9636\u6bb5\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u5bf9\u9f50\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cTACDA\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "TACDA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57dfRUL\u9884\u6d4b\u4e2d\u7684\u57df\u5dee\u5f02\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u57df\u91cd\u6784\u548c\u9000\u5316\u9636\u6bb5\u4e00\u81f4\u6027\u5bf9\u9f50\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.02677", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02677", "abs": "https://arxiv.org/abs/2512.02677", "authors": ["Zhiyuan He"], "title": "Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks", "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9012\u5f52\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u6df1\u5ea6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5faa\u73af\u5b9a\u4f4d\u66ff\u6362\u7ba1\u9053\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9012\u5f52\u63a8\u7406\u95ee\u9898\u65f6\u9762\u4e34\u6df1\u5ea6\u6cdb\u5316\u6311\u6218\uff0c\u5373\u65e0\u6cd5\u5904\u7406\u6bd4\u8bad\u7ec3\u65f6\u66f4\u6df1\u7684\u5d4c\u5957\u5c42\u6b21\u7ed3\u6784\uff0c\u5373\u4f7f\u5b83\u4eec\u80fd\u5904\u7406\u66f4\u957f\u7684\u975e\u5d4c\u5957\u5e8f\u5217\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5faa\u73af\u5b9a\u4f4d\u66ff\u6362\u7ba1\u9053\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\uff1a\u5b9a\u4f4d\u5668\u8bc6\u522b\u53ef\u89e3\u5b50\u8868\u8fbe\u5f0f\uff0c\u66ff\u6362\u5668\u8bc4\u4f30\u8fd9\u4e9b\u7ec4\u4ef6\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u7ed3\u6784\u3002", "result": "\u5728\u5e03\u5c14\u4ee3\u6570\u3001\u9012\u5f52\u7b97\u672f\u548c\u547d\u9898\u903b\u8f91\u4e09\u4e2a\u53ef\u63a7\u9012\u5f52\u6df1\u5ea6\u7684\u9886\u57df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5728\u5206\u5e03\u5916\u9012\u5f52\u6df1\u5ea6\u6d4b\u8bd5\u65f6\u7684\u6027\u80fd\u8870\u51cf\u3002", "conclusion": "\u6807\u51c6Transformer\u67b6\u6784\u5728\u6df1\u5ea6\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u63d0\u51fa\u7684\u5faa\u73af\u5b9a\u4f4d\u66ff\u6362\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9012\u5f52\u63a8\u7406\u4e2d\u7684\u6df1\u5ea6\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2512.02699", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02699", "abs": "https://arxiv.org/abs/2512.02699", "authors": ["Hyeongseop Rha", "Jeong Hun Yeo", "Junil Won", "Se Jin Park", "Yong Man Ro"], "title": "Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding", "comment": "16 pages, 8 figures", "summary": "In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.", "AI": {"tldr": "MIGR\u6846\u67b6\u901a\u8fc7\u6a21\u6001\u91cd\u8981\u6027\u5f15\u5bfc\u63a8\u7406\uff0c\u6539\u5584\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u7406\u89e3\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u63a8\u7406\u6f02\u79fb\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6f02\u79fb\u95ee\u9898\uff1a\u6a21\u578b\u9010\u6e10\u4f9d\u8d56\u81ea\u8eab\u751f\u6210\u7684\u6587\u672c\u800c\u975e\u591a\u6a21\u6001\u8bc1\u636e\uff0c\u4e14\u89e3\u91ca\u8fc7\u5ea6\u53d7\u89c6\u89c9\u4e3b\u5bfc\u7684\u63a8\u7406\u8def\u5f84\u5f71\u54cd\uff0c\u5bfc\u81f4\u60c5\u611f\u7406\u89e3\u4e0d\u53ef\u9760", "method": "\u63d0\u51fa\u6a21\u6001\u91cd\u8981\u6027\u673a\u5236\u8bc6\u522b\u60c5\u611f\u4e3b\u5bfc\u6a21\u6001\uff0c\u57fa\u4e8e\u6b64\u91cd\u7ec4\u63a8\u7406\u5e8f\u5217\uff0c\u4f7f\u89e3\u91ca\u4ece\u6700\u5173\u952e\u7684\u60c5\u611f\u6a21\u6001\u5f00\u59cb\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u6a21\u6001\u5bf9\u9f50\u76d1\u7763\u5fae\u8c03\u548c\u6a21\u6001\u611f\u77e5\u5956\u52b1\u4f18\u5316", "result": "\u5728DFEW\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMIGR\u663e\u8457\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\uff0c\u5c06\u6b63\u786e\u9884\u6d4b\u4f46\u60c5\u611f\u4e0d\u4e00\u81f4\u89e3\u91ca\u7684\u6bd4\u4f8b\u4ece18.10%\u964d\u81f37.37%", "conclusion": "\u4ece\u60c5\u611f\u4e3b\u5bfc\u6a21\u6001\u5f00\u59cb\u63a8\u7406\u80fd\u6709\u6548\u6539\u5584\u591a\u6a21\u6001\u60c5\u611f\u7406\u89e3\u7684\u53ef\u9760\u6027\uff0cMIGR\u6846\u67b6\u4e3a\u89e3\u51b3\u63a8\u7406\u6f02\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2512.02713", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02713", "abs": "https://arxiv.org/abs/2512.02713", "authors": ["Theodoros Aivalis", "Iraklis A. Klampanos", "Antonis Troumpoukis", "Joemon M. Jose"], "title": "Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs", "comment": null, "summary": "As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u751f\u6210\u6a21\u578b\u6eaf\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001LLM\u4ece\u56fe\u50cf\u63d0\u53d6\u7ed3\u6784\u5316\u4e09\u5143\u7ec4\uff0c\u5bf9\u6bd4\u751f\u6210\u56fe\u50cf\u4e0e\u8bad\u7ec3\u56fe\u50cf\u7684KG\u6765\u8ffd\u8e2a\u6f5c\u5728\u5f71\u54cd\uff0c\u652f\u6301\u7248\u6743\u5206\u6790\u548cAI\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u7248\u6743\u4fb5\u6743\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u9700\u8981\u7406\u89e3\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff0c\u4ee5\u89e3\u51b3\u7248\u6743\u5206\u6790\u3001\u6570\u636e\u96c6\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91caAI\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4e09\u5143\u7ec4\uff0c\u6784\u5efa\u4e0e\u9886\u57df\u7279\u5b9a\u672c\u4f53\u5bf9\u9f50\u7684\u77e5\u8bc6\u56fe\u8c31\u3002\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u56fe\u50cf\u548c\u8bad\u7ec3\u56fe\u50cf\u7684KG\u6765\u8ffd\u8e2a\u6f5c\u5728\u5f71\u54cd\uff0c\u652f\u6301\u7248\u6743\u5206\u6790\u548c\u900f\u660e\u5ea6\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5c40\u90e8\u8bad\u7ec3\u6a21\u578b\u7684\u9057\u5fd8\u5b9e\u9a8c\u548c\u5927\u89c4\u6a21\u6a21\u578b\u7684\u98ce\u683c\u7279\u5b9a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6846\u67b6\u652f\u6301\u5f00\u53d1\u4fc3\u8fdb\u4eba\u7c7b\u534f\u4f5c\u3001\u521b\u9020\u529b\u548c\u6fc0\u53d1\u597d\u5947\u5fc3\u7684AI\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6eaf\u6e90\u673a\u5236\uff0c\u80fd\u591f\u8ffd\u8e2a\u8bad\u7ec3\u6570\u636e\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\uff0c\u652f\u6301\u7248\u6743\u5206\u6790\u3001\u6570\u636e\u96c6\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91caAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.02716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02716", "abs": "https://arxiv.org/abs/2512.02716", "authors": ["Tianyi Zhang", "Xiangyuan Xue", "Lingyan Ruan", "Shiya Fu", "Feng Xia", "Simon D'Alfonso", "Vassilis Kostakos", "Hong Jia"], "title": "Menta: A Small Language Model for On-Device Mental Health Prediction", "comment": null, "summary": "Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/", "AI": {"tldr": "Menta\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u8fdb\u884c\u591a\u4efb\u52a1\u5fc3\u7406\u5065\u5eb7\u9884\u6d4b\u7684\u4f18\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u5fc3\u7406\u5065\u5eb7\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SLM\u548c\u90e8\u5206LLM\uff0c\u5e76\u80fd\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u5f71\u54cd\u5168\u7403\u6570\u4ebf\u4eba\uff0c\u4f46\u65e9\u671f\u68c0\u6d4b\u4ecd\u7136\u6709\u9650\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u89c4\u6a21\u548c\u8ba1\u7b97\u9700\u6c42\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u7684\u5fc3\u7406\u5065\u5eb7\u9884\u6d4b\u65b9\u9762\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "Menta\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u591a\u4efb\u52a1\u5fc3\u7406\u5065\u5eb7\u9884\u6d4b\u4f18\u5316\u7684SLM\uff0c\u91c7\u7528LoRA-based\u6846\u67b6\u3001\u8de8\u6570\u636e\u96c6\u7b56\u7565\u548c\u5e73\u8861\u51c6\u786e\u7387\u5bfc\u5411\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u516d\u4e2a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u76f8\u6bd49\u4e2a\u6700\u5148\u8fdb\u7684SLM\u57fa\u7ebf\uff0cMenta\u5728\u6db5\u76d6\u6291\u90c1\u3001\u538b\u529b\u548c\u81ea\u6740\u503e\u5411\u7684\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u534715.2%\uff1b\u5728\u6291\u90c1\u548c\u538b\u529b\u5206\u7c7b\u4efb\u52a1\u4e0a\u6bd4130\u4ebf\u53c2\u6570LLM\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u6a21\u578b\u5927\u5c0f\u7ea6\u5c0f3.25\u500d\uff1b\u53ef\u5728iPhone 15 Pro Max\u4e0a\u5b9e\u65f6\u90e8\u7f72\uff0c\u4ec5\u9700\u7ea63GB\u5185\u5b58\u3002", "conclusion": "Menta\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u5fc3\u7406\u5065\u5eb7\u76d1\u6d4b\u6f5c\u529b\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u5fc3\u7406\u5065\u5eb7\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02720", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02720", "abs": "https://arxiv.org/abs/2512.02720", "authors": ["He Wang", "Wenyilin Xiao", "Songqiao Han", "Hailiang Huang"], "title": "StockMem: An Event-Reflection Memory Framework for Stock Forecasting", "comment": null, "summary": "Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.", "AI": {"tldr": "StockMem\uff1a\u57fa\u4e8e\u4e8b\u4ef6-\u53cd\u601d\u53cc\u5c42\u8bb0\u5fc6\u6846\u67b6\u7684\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65b0\u95fb\u4e8b\u4ef6\u548c\u6316\u6398\u589e\u91cf\u4fe1\u606f\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u9762\u4e34\u5e02\u573a\u6ce2\u52a8\u6027\u548c\u5b9e\u65f6\u4e8b\u4ef6\u654f\u611f\u6027\u7684\u6311\u6218\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u57fa\u4e8e\u6587\u672c\u7684\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u566a\u58f0\u65b0\u95fb\u6570\u636e\u548c\u6587\u672c\u4e2d\u7f3a\u4e4f\u660e\u786e\u7b54\u6848\u7684\u9650\u5236\u3002\u901a\u7528\u8bb0\u5fc6\u67b6\u6784\u96be\u4ee5\u8bc6\u522b\u4ef7\u683c\u53d8\u52a8\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "method": "\u63d0\u51faStockMem\u4e8b\u4ef6-\u53cd\u601d\u53cc\u5c42\u8bb0\u5fc6\u6846\u67b6\uff1a1\uff09\u5c06\u65b0\u95fb\u7ed3\u6784\u5316\u4e3a\u4e8b\u4ef6\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u6316\u6398\uff1a\u6a2a\u5411\u6574\u5408\u6574\u5408\u6bcf\u65e5\u4e8b\u4ef6\uff0c\u7eb5\u5411\u8ddf\u8e2a\u6355\u6349\u4e8b\u4ef6\u6f14\u5316\u4ee5\u63d0\u53d6\u53cd\u6620\u5e02\u573a\u9884\u671f\u5dee\u5f02\u7684\u589e\u91cf\u4fe1\u606f\uff0c\u6784\u5efa\u65f6\u5e8f\u4e8b\u4ef6\u77e5\u8bc6\u5e93\uff1b2\uff09\u901a\u8fc7\u5206\u6790\u4e8b\u4ef6-\u4ef7\u683c\u52a8\u6001\uff0c\u5f62\u6210\u56e0\u679c\u7ecf\u9a8c\u7684\u53cd\u601d\u77e5\u8bc6\u5e93\uff1b3\uff09\u9884\u6d4b\u65f6\u68c0\u7d22\u7c7b\u4f3c\u5386\u53f2\u573a\u666f\uff0c\u7ed3\u5408\u5f53\u524d\u4e8b\u4ef6\u3001\u589e\u91cf\u6570\u636e\u548c\u8fc7\u53bb\u7ecf\u9a8c\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eStockMem\u4f18\u4e8e\u73b0\u6709\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u8ffd\u8e2a\u5f71\u54cd\u4ef7\u683c\u7684\u4fe1\u606f\u94fe\u63d0\u4f9b\u66f4\u4f18\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u51b3\u7b56\u900f\u660e\u5ea6\u3002", "conclusion": "StockMem\u901a\u8fc7\u4e8b\u4ef6-\u53cd\u601d\u53cc\u5c42\u8bb0\u5fc6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u80a1\u7968\u4ef7\u683c\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u91d1\u878d\u9884\u6d4b\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2512.02726", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02726", "abs": "https://arxiv.org/abs/2512.02726", "authors": ["Md Abdul Kadir", "Sai Suresh Macharla Vasu", "Sidharth S. Nair", "Daniel Sonntag"], "title": "AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping", "comment": null, "summary": "Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \\textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.", "AI": {"tldr": "LLMs\u4f5c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u5668\u5728\u590d\u5f0f\u8bb0\u8d26\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u652f\u6301AI\u589e\u5f3a\u5ba1\u8ba1", "motivation": "\u4f20\u7edf\u65e5\u8bb0\u8d26\u6d4b\u8bd5\uff08JETs\uff09\u4ea7\u751f\u5927\u91cf\u8bef\u62a5\u4e14\u96be\u4ee5\u68c0\u6d4b\u7ec6\u5fae\u5f02\u5e38\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5ba1\u8ba1\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5", "method": "\u4f7f\u7528LLaMA\u548cGemma\u7b49\u6700\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u5668\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u533f\u540d\u8d26\u672c\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e0e\u4f20\u7edfJETs\u548c\u673a\u5668\u5b66\u4e60\u57fa\u51c6\u6bd4\u8f83", "result": "LLMs\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u578bJETs\u548c\u7ecf\u5178ML\u57fa\u51c6\uff0c\u540c\u65f6\u63d0\u4f9b\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u5f0f\u8bb0\u8d26\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301AI\u589e\u5f3a\u5ba1\u8ba1\uff0c\u4eba\u7c7b\u5ba1\u8ba1\u5e08\u4e0e\u57fa\u7840\u6a21\u578b\u534f\u4f5c\u53ef\u589e\u5f3a\u8d22\u52a1\u5b8c\u6574\u6027"}}
{"id": "2512.02731", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02731", "abs": "https://arxiv.org/abs/2512.02731", "authors": ["Przemyslaw Chojecki"], "title": "Self-Improving AI Agents through Self-Play", "comment": null, "summary": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $\u03bd_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $\u0398$, and we identify the coefficient of self-improvement $\u03ba$ as the Lie derivative of the capability functional along this flow.\n  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $\u03ba> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.\n  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5fc3\u7406\u6d4b\u91cf\u7535\u6c60\u7684\u6a21\u6570\u7406\u8bba\u6846\u67b6\u6269\u5c55\u5230\u52a8\u529b\u7cfb\u7edf\u9886\u57df\uff0c\u5c06\u667a\u80fd\u4f53\u5f62\u5f0f\u5316\u4e3a\u7531\u8ba1\u7b97\u8d44\u6e90\u53c2\u6570\u5316\u7684\u6d41\uff0c\u63d0\u51fa\u4e86\u751f\u6210\u5668-\u9a8c\u8bc1\u5668-\u66f4\u65b0\u5668(GVU)\u7b97\u5b50\uff0c\u5e76\u63a8\u5bfc\u51fa\u4fdd\u8bc1\u81ea\u6539\u8fdb\u7a33\u5b9a\u6027\u7684\u65b9\u5dee\u4e0d\u7b49\u5f0f\u6761\u4ef6\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5efa\u7acb\u4e86AAI\u80fd\u529b\u5206\u6570\u4f5c\u4e3a\u667a\u80fd\u4f53\u8868\u793a\u7a7a\u95f4\u4e0a\u7684\u9759\u6001\u6cdb\u51fd\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u4f5c\u4e3a\u52a8\u529b\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u63cf\u8ff0\u3002\u672c\u6587\u65e8\u5728\u5c06\u5fc3\u7406\u6d4b\u91cf\u7535\u6c60\u7684\u6a21\u6570\u7406\u8bba\u6846\u67b6\u6269\u5c55\u5230\u52a8\u529b\u7cfb\u7edf\u9886\u57df\uff0c\u4e3a\u81ea\u6539\u8fdb\u8fc7\u7a0b\u63d0\u4f9b\u4e25\u683c\u7684\u6570\u5b66\u57fa\u7840\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u5f62\u5f0f\u5316\u4e3a\u7531\u8ba1\u7b97\u8d44\u6e90r\u53c2\u6570\u5316\u7684\u6d41\u03bd_r\uff0c\u53d7\u9012\u5f52\u751f\u6210\u5668-\u9a8c\u8bc1\u5668-\u66f4\u65b0\u5668(GVU)\u7b97\u5b50\u63a7\u5236\u3002\u8bc1\u660e\u8be5\u7b97\u5b50\u5728\u53c2\u6570\u6d41\u5f62\u0398\u4e0a\u751f\u6210\u5411\u91cf\u573a\uff0c\u5e76\u5c06\u81ea\u6539\u8fdb\u7cfb\u6570\u03ba\u8bc6\u522b\u4e3a\u80fd\u529b\u6cdb\u51fd\u6cbf\u8be5\u6d41\u7684\u674e\u5bfc\u6570\u3002\u63a8\u5bfc\u51fa\u65b9\u5dee\u4e0d\u7b49\u5f0f\u8fd9\u4e00\u8c31\u6761\u4ef6\uff0c\u4f5c\u4e3a\u81ea\u6539\u8fdb\u7a33\u5b9a\u6027\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u81ea\u6539\u8fdb\u7cfb\u6570\u03ba>0\u7684\u5145\u5206\u6761\u4ef6\u662f\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u7ec4\u5408\u566a\u58f0\u8db3\u591f\u5c0f\uff08\u5728\u66f2\u7387\u548c\u6b65\u957f\u6548\u5e94\u8303\u56f4\u5185\uff09\u3002\u5e94\u7528\u8be5\u5f62\u5f0f\u5316\u6846\u67b6\u7edf\u4e00\u4e86\u8bed\u8a00\u81ea\u73a9(LSP)\u3001\u81ea\u6821\u6b63\u548c\u5408\u6210\u6570\u636e\u5f15\u5bfc\u7b49\u6700\u65b0\u6587\u732e\uff0c\u5c55\u793a\u4e86STaR\u3001SPIN\u3001Reflexion\u3001GANs\u548cAlphaZero\u7b49\u67b6\u6784\u90fd\u662f\u6ee1\u8db3\u65b9\u5dee\u4e0d\u7b49\u5f0f\u7684GVU\u7b97\u5b50\u7684\u5177\u4f53\u62d3\u6251\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u4f53\u81ea\u6539\u8fdb\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u5dee\u4e0d\u7b49\u5f0f\u4e3a\u81ea\u6539\u8fdb\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u6570\u5b66\u4fdd\u8bc1\uff0c\u5e76\u5c06\u591a\u79cd\u73b0\u6709\u67b6\u6784\u7edf\u4e00\u4e3aGVU\u7b97\u5b50\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u81ea\u6539\u8fdb\u7cfb\u7edf\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.02735", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02735", "abs": "https://arxiv.org/abs/2512.02735", "authors": ["Anna Rodum Bj\u00f8ru", "Jacob Lysn\u00e6s-Larsen", "Oskar J\u00f8rgensen", "Inga Str\u00fcmke", "Helge Langseth"], "title": "A Framework for Causal Concept-based Model Explanations", "comment": null, "summary": "This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u6982\u5ff5\u7684\u4e8b\u540e\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u5ff5\u5e72\u9884\u7684\u5145\u5206\u6982\u7387\u751f\u6210\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\uff0c\u5728CelebA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u7406\u89e3\u6027\u548c\u5fe0\u5b9e\u6027", "motivation": "\u4e3a\u4e0d\u53ef\u89e3\u91ca\u6a21\u578b\u63d0\u4f9b\u65e2\u6613\u4e8e\u7406\u89e3\u53c8\u5fe0\u5b9e\u4e8e\u539f\u6a21\u578b\u7684\u89e3\u91ca\uff0c\u89e3\u51b3\u73b0\u6709\u53ef\u89e3\u91caAI\u65b9\u6cd5\u5728\u53ef\u7406\u89e3\u6027\u548c\u5fe0\u5b9e\u6027\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u57fa\u4e8e\u56e0\u679c\u6982\u5ff5\u7684\u4e8b\u540e\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u6982\u5ff5\u5e72\u9884\u7684\u5145\u5206\u6982\u7387\u6765\u751f\u6210\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\uff0c\u4f7f\u7528CelebA\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u5668\u8fdb\u884c\u6982\u5ff5\u9a8c\u8bc1", "result": "\u5c55\u793a\u4e86\u57fa\u4e8e\u6e05\u6670\u6982\u5ff5\u8bcd\u6c47\u7684\u89e3\u91ca\u793a\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u7406\u89e3\u6027\uff1b\u5f3a\u8c03\u4e86\u91cd\u8981\u5047\u8bbe\uff0c\u786e\u4fdd\u89e3\u91ca\u751f\u6210\u4e0e\u89e3\u91ca\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\uff0c\u4ece\u800c\u4fdd\u8bc1\u5fe0\u5b9e\u6027", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u56e0\u679c\u6982\u5ff5\u7684\u4e8b\u540e\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u4e0d\u53ef\u89e3\u91ca\u6a21\u578b\u63d0\u4f9b\u65e2\u6613\u4e8e\u7406\u89e3\u53c8\u5fe0\u5b9e\u4e8e\u539f\u6a21\u578b\u7684\u89e3\u91ca\uff0c\u4e3a\u53ef\u89e3\u91caAI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2512.02812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02812", "abs": "https://arxiv.org/abs/2512.02812", "authors": ["Zijie Lin", "Qilin Cai", "Liang Shen", "Mingjun Xiao"], "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents", "comment": null, "summary": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u63d0\u793a\u7684\u534f\u4f5c\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u63d0\u5347\u8bba\u6587\u5230\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u7ea615%\u548c13%\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u8bba\u6587\u590d\u73b0\u6846\u67b6\u7f3a\u4e4f\u5bf9\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u8f93\u51fa\u7684\u9a8c\u8bc1\u548c\u7cbe\u70bc\u673a\u5236\uff0c\u6216\u8005\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u8fdb\u884c\u81ea\u6211\u7cbe\u70bc\uff0c\u8fd9\u9650\u5236\u4e86\u6846\u67b6\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u63d0\u793a\u7684\u534f\u4f5c\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1a\u9a8c\u8bc1\u667a\u80fd\u4f53\u68c0\u67e5\u6bcf\u4e2a\u6b65\u9aa4\u7684\u8f93\u51fa\u662f\u5426\u6ee1\u8db3\u5bf9\u5e94\u7cfb\u7edf\u63d0\u793a\u7684\u8981\u6c42\uff0c\u7cbe\u70bc\u667a\u80fd\u4f53\u6839\u636e\u8bc6\u522b\u51fa\u7684\u95ee\u9898\u4fee\u8ba2\u8f93\u51fa\u3002\u8be5\u65b9\u6cd5\u4ec5\u5229\u7528\u539f\u59cb\u7cfb\u7edf\u63d0\u793a\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u548c\u6539\u8fdb\uff0c\u65e0\u9700\u4eba\u5de5\u4e3a\u6bcf\u4e2a\u6b65\u9aa4\u8bbe\u8ba1\u7279\u5b9a\u7684\u7cbe\u70bc\u63d0\u793a\u3002", "result": "\u5728PaperBench Code-Dev\u548cPaper2CodeBench\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u73b0\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u76f8\u6bd4\u6ca1\u6709\u8be5\u667a\u80fd\u4f53\u7684\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u83b7\u5f97\u4e86\u7ea615%\u548c13%\u7684\u6027\u80fd\u63d0\u5347\u3002\u4e0eSelf-Refine\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65e0\u63d0\u793a\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u9700\u63d0\u793a\u7684\u534f\u4f5c\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u81ea\u52a8\u5316\u8bba\u6587\u5230\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9a8c\u8bc1\u548c\u7cbe\u70bc\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.02879", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02879", "abs": "https://arxiv.org/abs/2512.02879", "authors": ["Jef Caers"], "title": "The future of AI in critical mineral exploration", "comment": null, "summary": "The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u4e3b\u4e49\u548c\u8bc1\u4f2a\u539f\u5219\u7684\u65b0\u79d1\u5b66\u65b9\u6cd5\uff0c\u5229\u7528AI\u51cf\u5c11\u8ba4\u77e5\u504f\u5dee\u548c\u5047\u9633\u6027\uff0c\u964d\u4f4e\u77ff\u4ea7\u52d8\u63a2\u6210\u672c", "motivation": "\u5c3d\u7ba1\u6295\u8d44\u589e\u52a0\uff0c\u4f46\u8fc7\u53bb20\u5e74\u65b0\u77ff\u4ea7\u53d1\u73b0\u51cf\u5c11\uff0c\u9700\u8981\u89e3\u51b3\u80fd\u6e90\u8f6c\u578b\u4e2d\u5173\u952e\u77ff\u4ea7\u52d8\u63a2\u7684\u6311\u6218", "method": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u4e3b\u4e49\u548c\u8bc1\u4f2a\u539f\u5219\u7684\u54f2\u5b66\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u91c7\u96c6\u89c6\u4e3a\u8bc1\u4f2a\u4eba\u7c7b\u5047\u8bbe\u7684\u624b\u6bb5\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u6307\u6807\u548c\u7406\u6027\u51b3\u7b56\u786e\u5b9a\u4e0b\u4e00\u6b65\u6570\u636e\u91c7\u96c6", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684\u52d8\u63a2\u534f\u8bae\u6a21\u677f\uff0c\u9700\u8981\u4e24\u79cdAI\u5f62\u5f0f\uff1a1)\u4e0e\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u751f\u6210\u7ade\u4e89\u6027\u5730\u8d28\u5047\u8bbe\uff1b2)\u4eba\u673a\u4ea4\u4e92AI\u7b97\u6cd5\u4f18\u5316\u5730\u8d28\u3001\u5730\u7403\u7269\u7406\u3001\u5730\u7403\u5316\u5b66\u548c\u94bb\u63a2\u6570\u636e\u91c7\u96c6\u89c4\u5212", "conclusion": "AI\u80fd\u591f\u5b9e\u73b0\u4e25\u8c28\u7684\u77ff\u4ea7\u52d8\u63a2\u79d1\u5b66\u65b9\u6cd5\uff0c\u51cf\u5c11\u8ba4\u77e5\u504f\u5dee\u548c\u5047\u9633\u6027\uff0c\u964d\u4f4e\u52d8\u63a2\u6210\u672c\uff0c\u63a8\u52a8\u5173\u952e\u77ff\u4ea7\u53d1\u73b0"}}
