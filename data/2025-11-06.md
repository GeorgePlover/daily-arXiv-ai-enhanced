<div id=toc></div>

# Table of Contents

- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [1] [ALAS: Transactional and Dynamic Multi-Agent LLM Planning](https://arxiv.org/abs/2511.03094)
*Longling Geng,Edward Y. Chang*

Main category: cs.MA

TL;DR: ALAS是一个状态感知、抗干扰的多智能体规划框架，通过分离规划与验证、记录版本化执行日志、执行局部修复来解决LLM规划中的脆弱性问题，显著提高效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多智能体规划中表现灵活但实际应用脆弱：验证经常循环、状态变化未跟踪修复、小故障触发昂贵的全局重计算。

Method: ALAS框架将规划与非循环验证分离，记录版本化执行日志用于基础检查和恢复点，执行局部修复以保留进行中的工作。验证器独立于规划LLM运行，修复协议仅编辑最小受影响区域。

Result: 在作业车间调度测试中，ALAS达到83.7%成功率，减少60%token使用，运行速度快1.82倍。验证器能检测注入的结构故障，局部修复能限制运行时扰动。

Conclusion: 验证器隔离、版本化执行日志和局部修复的结合为多智能体LLM规划提供了可衡量的效率、可行性和可扩展性。

Abstract: Large language models enable flexible multi-agent planning but remain fragile
in practice: verification is often circular, state changes are not tracked for
repair, and small faults trigger costly global recomputation. We present ALAS,
a stateful, disruption-aware framework that separates planning from
non-circular validation, records a versioned execution log for grounded checks
and restore points, and performs localized repair that preserves work in
progress. The validator operates independently of the planning LLM with fresh,
bounded context, avoiding self-check loops and mid-context attrition. The
repair protocol edits only the minimal affected region under explicit policies
(retry, catch, timeout, backoff, idempotency keys, compensation, loop guards)
defined in a canonical workflow IR that maps to Amazon States Language and Argo
Workflows. On job-shop scheduling suites (DMU, TA) across five classical
benchmarks, ALAS matches or exceeds strong single-LLM and multi-agent
baselines, achieving 83.7% success, reducing token usage by 60%, and running
1.82times faster under comparable settings. A minimal reliability study shows
that the validator detects injected structural faults with low overhead, and
that localized repair contains runtime perturbations with a bounded edit radius
and less makespan degradation than global recompute. Results indicate that the
combination of validator isolation, versioned execution logs, and localized
repair provides measurable efficiency, feasibility, and scalability for
multi-agent LLM planning. Code and seeds will be released.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 本文总结了EuroHPC JU CEEC项目在欧洲主要HPC系统上测量、分析和优化能耗的集体经验，通过CFD应用案例研究评估了不同架构的能效表现。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算系统面临能效挑战，计算需求增长和架构复杂性导致显著的能源足迹，需要系统性地测量和优化能耗。

Method: 使用代表性CFD应用（waLBerla、FLEXI/GALÆXI、Neko、NekRS）在多种架构上进行案例研究，评估能耗与时间指标，包括CPU和GPU分区。

Result: 结果表明加速器和混合精度技术在保持计算精度的同时能显著降低能耗，凸显了这些技术的能效优势。

Conclusion: 需要在HPC系统上促进能耗测量，以提高意识、教育社区并采取行动实现更可持续的百亿亿次计算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [3] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: UMDAM是一种针对NPU-PIM协同执行优化的统一内存亲和性数据布局和DRAM地址映射方案，通过列主序、基于瓦片的布局和可配置的DRAM映射策略，在不增加内存开销或带宽损失的情况下，显著提升边缘设备上LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在配备NPU的边缘设备上部署，解码阶段仍然存在内存密集型问题，限制了性能。NPU-PIM协同执行面临数据布局不匹配、带宽损失和冗余存储等挑战。

Method: 提出UMDAM方案，采用列主序、基于瓦片的布局和可配置的DRAM映射策略，确保与NPU计算的兼容性，同时最大化PIM效率。

Result: 在OPT模型上的综合评估显示，UMDAM将首令牌时间(TTFT)减少高达3.0倍，末令牌时间(TTLT)减少2.18倍。

Conclusion: UMDAM显著提升了边缘设备上端到端LLM推理效率，解决了NPU-PIM协同执行的关键挑战。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [4] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: 本文评估了三种隔离策略（cgroups和CPU固定、Docker容器、Firecracker微虚拟机）在云环境双基准测试中的效果，发现除了Docker容器外，进程隔离能降低假阳性率。


<details>
  <summary>Details</summary>
Motivation: 云环境中多租户资源竞争导致性能变异性，双基准测试通过在同一VM上并发运行两个工作负载版本来缓解此问题，但需要额外的隔离机制来处理同步工作负载间的内部VM竞争。

Method: 通过运行双基准测试设置与噪声生成器，比较三种隔离策略（cgroups和CPU固定、Docker容器、Firecracker微虚拟机）与无隔离基线实验的效果。

Result: 所有实验在噪声影响下都显示出不同的延迟分布，但进程隔离通常能降低假阳性率，除了Docker容器实验。Docker容器尽管内部依赖cgroups和CPU固定，但对噪声导致的性能退化更敏感。

Conclusion: 建议在同步工作负载中使用进程隔离，但应避免使用Docker容器。

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [5] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文提出了一种新的拓扑编码方法，通过谱空间来建模基于轮次的全信息对手的执行过程，为分布式计算中的无色任务可解性提供了通用特征化。


<details>
  <summary>Details</summary>
Motivation: 统一分布式计算中的拓扑方法，为消息对手模型下的无色任务可解性提供通用理论框架。

Method: 使用谱空间和Alexandrov拓扑来建模分布式协议执行后的全局状态，通过射影极限构造极限对象，并应用Stone对偶性。

Result: 得到了无色任务可解性的通用特征化定理：存在算法解决无色任务当且仅当存在与Δ兼容的谱映射f。

Conclusion: 该工作为分布式计算拓扑方法提供了统一框架，揭示了彩色和非彩色模型具有相同的计算能力，并为这种等价性提供了拓扑解释。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [6] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文扩展了分布式计算中无色任务的可计算性理论，将消息对手模型推广到输入依赖型，并研究了条件基对手下的k-集一致性问题，提供了完整的可解性特征描述。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定消息对手模型，但在实际分布式系统中，对手行为可能依赖于输入配置。本文旨在扩展可计算性理论，处理输入依赖型对手和条件基对手场景。

Method: 使用拓扑框架和几何构造方法，将CG-24的特征描述推广到输入依赖型对手，分析核心弹性对手的计算能力等价性，并建立条件基核心依赖对手下k-集一致问题的充要条件。

Result: 证明了IIS_n的核心弹性对手与仅在开始时发生崩溃的核心弹性对手具有相同的计算能力；为条件基核心依赖对手下的k-集一致问题提供了完整的可解性特征描述。

Conclusion: 通过拓扑方法成功扩展了分布式任务的可计算性理论，为输入依赖型和条件基对手场景提供了统一的理论框架，简化了证明过程而不改变任务的计算能力。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 本文提出了一个多智能体AI框架，通过结构化设计和评审循环来优化工程设讴过程，并以NACA翼型气动优化为例展示了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统工程设讴方法资源密集且效率低下，需要多领域专家协作，导致复杂的合作和迭代优化。

Method: 构建包含三个AI智能体的框架：图本体学家使用LLM构建领域知识图谱，系统工程师制定技术需求，设计工程师生成候选设计并形成迭代反馈循环。

Result: 该框架成功应用于4位数NACA翼型的气动优化，通过智能体协作生成并优化设计，最终提升了升阻比等性能指标。

Conclusion: 研究表明，配备结构化知识表示的协作AI智能体能够显著提高工程设讴过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [8] [Evaluating Control Protocols for Untrusted AI Agents](https://arxiv.org/abs/2511.02997)
*Jon Kutasov,Chloe Loughridge,Yuqi Sun,Henry Sleight,Buck Shlegeris,Tyler Tracy,Joe Benton*

Main category: cs.AI

TL;DR: 本文系统评估了AI控制协议在SHADE-Arena环境中的安全性，发现关键行动延迟协议对红队攻击具有高度鲁棒性，而重采样策略在红队获得协议内部信息时安全性大幅下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统作为智能体广泛部署，确保其安全运行变得至关重要。AI控制通过监控智能体行为并在必要时干预或审计来降低不可信AI的风险。

Method: 在SHADE-Arena多样化智能体环境中系统评估控制协议，包括可信模型延迟、重采样和关键行动延迟等蓝队协议，并迭代开发红队攻击策略。

Result: 重采样和关键行动延迟协议将安全性从50%提升至96%；但当红队获得重采样发生时机或模拟监控器能力时，攻击成功率显著提高，安全性降至17%；关键行动延迟协议对最强红队策略仍保持高度鲁棒性。

Conclusion: 关键行动延迟协议对自适应攻击具有高度鲁棒性，表明拒绝攻击策略访问协议内部信息对于AI控制安全至关重要。

Abstract: As AI systems become more capable and widely deployed as agents, ensuring
their safe operation becomes critical. AI control offers one approach to
mitigating the risk from untrusted AI agents by monitoring their actions and
intervening or auditing when necessary. Evaluating the safety of these
protocols requires understanding both their effectiveness against current
attacks and their robustness to adaptive adversaries. In this work, we
systematically evaluate a range of control protocols in SHADE-Arena, a dataset
of diverse agentic environments. First, we evaluate blue team protocols,
including deferral to trusted models, resampling, and deferring on critical
actions, against a default attack policy. We find that resampling for
incrimination and deferring on critical actions perform best, increasing safety
from 50% to 96%. We then iterate on red team strategies against these protocols
and find that attack policies with additional affordances, such as knowledge of
when resampling occurs or the ability to simulate monitors, can substantially
improve attack success rates against our resampling strategy, decreasing safety
to 17%. However, deferring on critical actions is highly robust to even our
strongest red team strategies, demonstrating the importance of denying attack
policies access to protocol internals.

</details>


### [9] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: Solly是首个在简化版Liar's Poker中达到精英人类水平的AI智能体，通过自对弈和深度强化学习训练，在单挑和多玩家游戏中表现优异，胜率超过50%，并超越了大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 虽然AI在德州扑克等游戏中已取得突破，但多人动态较弱。Liar's Poker具有广泛的多人参与特性，是测试多玩家动态、不完全信息和不确定性推理的理想环境。

Method: 使用模型无关的演员-评论家深度强化学习算法，通过自对弈训练Solly智能体。

Result: Solly在单挑和多玩家Liar's Poker中达到精英人类水平，胜率超过50%，在资金收益上表现优异，超越了具有推理能力的大型语言模型，开发了新颖的竞价策略并有效随机化游戏。

Conclusion: Solly证明了深度强化学习在复杂多人不完全信息游戏中的有效性，能够达到精英人类水平并超越现有AI方法。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


### [10] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent是一个多智能体框架，通过将端到端数据分析工作流分解为专门的智能体（意图澄清、数据集发现、分析和报告），解决了LLM在复杂分析任务中的注意力稀释、专业推理模式冲突和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库具有基于证据决策的潜力，但缺乏数据集发现、模式映射和统计分析专业知识的非专家难以访问。虽然大型语言模型在单个任务上表现出潜力，但在端到端分析工作流中存在根本性限制。

Method: 设计多智能体框架，将分析工作流分解为专门的智能体：意图澄清、数据集发现、分析和报告。这种架构在智能体上下文中保持专注注意力，并在每个阶段实现验证。

Result: 评估了5个模型和50个查询，得出多智能体LLM系统的五个设计原则：专业化独立于模型强度提供价值；智能体分为通用型和条件型；智能体缓解不同的失败模式；架构优势在不同任务复杂度下持续存在；智能体有效性在不同模型间差异显著。

Conclusion: 这些原则指导了何时以及为什么在复杂分析工作流中需要专业化，同时通过自然语言接口实现更广泛的公共数据访问。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [11] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: ScalingEval是一个大规模基准测试研究，系统比较了36个大型语言模型作为评估者的性能，发现Claude 3.5 Sonnet决策置信度最高，Gemini 1.5 Pro整体表现最佳，GPT-4o在延迟-准确性-成本方面最优，GPT-OSS 20B在开源模型中领先。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型作为评估者的重要性日益增加，需要建立可扩展且可信赖的评估流程，因此开发了无需人工标注的可复现比较框架。

Method: 采用多智能体框架，通过可扩展的多数投票将模式审计和问题代码聚合为真实标签，使用共识驱动的评估协议在多个产品类别中系统比较36个LLM。

Result: 在互补商品推荐的大规模应用中，发现结构化领域（电子、体育）达成强共识，而生活方式类别（服装、食品）存在持续分歧。

Conclusion: ScalingEval建立了可复现的基准测试和评估协议，为LLM作为评估者提供了关于扩展性、可靠性和模型家族权衡的可操作指导。

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [12] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文构建了首个基准测试来评估LLMs对现实世界概率分布知识的掌握程度，发现LLMs在理解现实世界统计分布方面表现不佳，未能自然内化真实世界统计数据。


<details>
  <summary>Details</summary>
Motivation: AI系统在科学领域具有巨大潜力，但需要更强的能力来实现更通用的智能。关键区别在于事实性知识和概率性知识，而LLMs是否能够内化现实世界分布是一个重要问题。

Method: 开发首个基准测试，直接测试LLMs是否能够获取描述现实世界人口的经验分布，涵盖经济、健康、教育和社会行为等领域。

Result: LLMs整体表现不佳，似乎无法自然内化现实世界统计数据。在Pearl因果层次框架下，语言模型缺乏对观测分布的知识。

Conclusion: LLMs在理解现实世界概率分布方面存在根本性限制，根据因果层次定理，这意味着它们在干预性和反事实性知识方面也受到限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [13] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: SnapStream是一种KV缓存压缩方法，可在保持模型精度的同时实现4倍片上内存使用改进，首次在具有静态图和连续批处理的生产推理系统中部署稀疏KV注意力技术。


<details>
  <summary>Details</summary>
Motivation: 随着100B+参数大语言模型和100k+上下文长度的普及，对片上内存支持大KV缓存的需求增加，但现有技术如StreamingLLM和SnapKV在工业部署中应用有限，因为静态图和连续批处理框架难以修改标准多头注意力算法，且这些技术对现代指令跟随和推理模型的精度影响不明确。

Method: 开发SnapStream KV缓存压缩方法，在16路张量并行部署的DeepSeek-671B模型上进行测试，使用SambaNova SN40L加速器，支持128k上下文长度和高达1832 tokens/秒的推理速度。

Result: SnapStream实现了4倍片上内存使用改进，在LongBench-v2、AIME24和LiveCodeBench基准测试中引入最小精度下降，在真实生产环境中验证了有效性。

Conclusion: SnapStream是首个在具有静态图和连续批处理的生产推理系统中成功部署的稀疏KV注意力技术，为大规模LLM部署提供了高效的内存优化解决方案。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [14] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 提出基于能力的监控方法，作为传统任务型监控的替代方案，用于医疗领域大语言模型的规模化监控。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习监控方法基于任务性能退化和数据集漂移假设，但大语言模型并非针对特定任务训练，这种假设不成立。需要新的监控原则来适应通用型AI模型的特点。

Method: 基于能力的监控方法，围绕共享模型能力（如摘要、推理、翻译、安全防护）组织监控，而非独立评估每个下游任务。

Result: 该方法能够实现跨任务检测系统性弱点、长尾错误和涌现行为，这些是任务型监控可能遗漏的。

Conclusion: 基于能力的监控为医疗领域大语言模型和未来通用AI模型提供了可扩展的安全、自适应和协作监控基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [15] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 本文对miniF2F基准测试中的形式化和非形式化陈述进行了深入分析，发现原始基准存在大量不一致问题，导致AI系统在数学奥林匹克竞赛任务中的准确率仅为36%。通过修正所有错误，创建了miniF2F-v2，将准确率提升至70%。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在数学奥林匹克竞赛任务中的表现，包括自然语言理解、形式化表达和定理证明的完整流程，发现原始基准测试存在严重的数据质量问题。

Method: 对miniF2F基准进行系统分析，识别形式化和非陈述之间的差异，修正所有错误和简化，创建经过验证的miniF2F-v2数据集，并重新评估完整的定理证明流程。

Result: 原始miniF2F中超过一半问题存在形式化和非形式化陈述不一致的问题，导致最佳准确率仅为36%。修正后的miniF2F-v2将准确率提升至70%，但仍显示自动形式化模型与定理证明器之间存在显著不对齐。

Conclusion: 高质量基准测试对于评估形式推理领域进展至关重要，能够更好地诊断自动形式化和定理证明模型的失败和成功模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [16] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 本文提出了一种利用多模态大语言模型辅助设计烟花算法的新方法，通过引入关键部分概念扩展FWA处理复杂高维任务，在旅行商问题和电子设计自动化问题上取得了优异结果。


<details>
  <summary>Details</summary>
Motivation: 随着优化问题日益复杂多样，传统优化方法在处理非凸性、高维性、黑盒特性等问题时效率低下。大语言模型在语言理解和代码生成方面的进步为优化算法设计提供了新思路。

Method: 以烟花算法为基础优化器，结合多模态大语言模型，提出关键部分概念来扩展FWA处理复杂高维任务，并利用大语言模型的多模态特性充分挖掘优化过程中的信息。

Result: 实验结果表明，在新框架下生成的烟花算法在多个问题实例上达到或超越了当前最优结果。

Conclusion: 大语言模型辅助的优化算法设计框架能够有效提升传统优化方法的性能，特别是在复杂高维任务上表现出色。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [17] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的安全响应框架，通过输入级安全分类模型和输出级检索增强生成技术，系统保护大语言模型的安全部署。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其安全问题日益突出，严重制约了在关键领域的可信部署。

Method: 在输入级采用基于监督微调的安全分类模型，通过四层级分类法进行精确风险识别；在输出级集成检索增强生成与专门微调的解释模型，确保响应基于可信知识库。

Result: 风险召回率达到99.3%，在公共安全评估基准上显著优于基线模型，在专有高风险测试集上获得100%安全分数。

Conclusion: 该研究为构建高安全性、高可信度的大语言模型应用提供了有效的工程路径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [18] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了一种验证形式化解释器的新方法，并评估了公开可用的PyXAI解释器，发现其在大多数数据集上产生了错误的解释。


<details>
  <summary>Details</summary>
Motivation: 形式化可解释人工智能（XAI）相比非形式化方法具有理论严谨性保证，但对形式化解释器实际实现的验证关注不足。

Method: 开发了一种新颖的验证形式化解释器的方法论，并对PyXAI解释器进行了评估。

Result: 实验发现PyXAI在大多数分析的数据集上计算出了错误的解释，验证了所提验证方法的重要性。

Conclusion: 形式化解释器的实际实现需要严格的验证，本文提出的验证方法对于确保解释的正确性至关重要。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [19] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文介绍了为Adobe Summit开发的领域特定AI助手Summit Concierge，该助手能够处理各种活动相关查询，并在数据稀疏、质量保证和快速部署等现实约束下运行。


<details>
  <summary>Details</summary>
Motivation: 生成式AI助手在企业环境中具有提升生产力、简化信息访问和改善用户体验的潜力，但面临数据稀疏、质量保证和快速部署等现实挑战。

Method: 采用人在回路开发工作流，结合提示工程、检索基础和轻量级人工验证，构建了系统架构并实施了敏捷的反馈驱动开发流程。

Result: 成功开发并部署了Summit Concierge助手，能够处理广泛的活动相关查询，在现实约束下实现了可扩展和可靠的AI助手。

Conclusion: 即使在冷启动场景下，敏捷的反馈驱动开发方法也能实现可扩展且可靠的AI助手，为类似企业应用提供了可行的开发路径。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [20] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）能够仅基于大五人格量表数据，通过两阶段推理过程准确预测人类心理特质的相关结构，其表现接近直接训练的机器学习算法。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够从最小化的定量输入中建模人类心理特质的相关结构，探索其推理能力和心理模拟潜力。

Method: 使用816名人类的大五人格量表数据提示各种LLMs，让它们模拟在其他9个心理量表上的回答，并分析推理过程。

Result: LLMs在捕捉人类心理结构方面表现出色，量表间相关模式与人类数据高度一致（R² > 0.89），零样本表现显著优于基于语义相似度的预测。

Conclusion: LLMs通过抽象和推理过程能够精确预测个体心理特质，这既为心理模拟提供了强大工具，也揭示了其新兴推理能力。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [21] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 本文提出了一个名为AAA的自动化Web可访问性审计框架，通过人机协作模式实现WCAG-EM标准的规模化执行，包含GRASP图采样方法和MaC多模态助手两个核心创新。


<details>
  <summary>Details</summary>
Motivation: 当前网站可访问性审计资源密集且难以规模化，大多数网站界面不符合标准，阻碍了数字空间的社会福利、正义和平等发展。

Method: AAA框架包含GRASP（基于图的多模态采样方法，利用视觉、文本和关系线索的嵌入确保页面覆盖代表性）和MaC（基于多模态大语言模型的助手，通过跨模态推理支持审计员完成高难度任务）。

Result: 实验证明该方法的有效性，发现经过微调的小型语言模型可以作为能力强的专家使用，并贡献了四个用于审计流程基准测试的新数据集。

Conclusion: 该框架实现了可扩展的端到端Web可访问性审计，通过AI增强辅助为现实世界影响赋能人类审计员。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [22] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 该论文对机器学习模型中解释问题的参数化复杂度进行了理论分析，重点关注具有透明内部机制的模型，包括决策树、决策集、决策列表、布尔电路及其集成模型。


<details>
  <summary>Details</summary>
Motivation: 填补可解释AI领域的理论空白，为理解生成机器学习模型解释的复杂性提供基础，促进AI系统的透明度和问责制。

Method: 对多种机器学习模型中的溯因和对比解释问题进行参数化复杂度分析，涵盖局部和全局变体。

Result: 为不同ML模型提供了解释问题复杂度的理论框架，揭示了各种模型在解释生成方面的计算特性。

Conclusion: 这项工作为可解释AI领域的进一步研究提供了重要见解，强调了AI系统透明度和问责制的必要性。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>
