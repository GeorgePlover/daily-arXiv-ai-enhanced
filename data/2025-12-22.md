<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 本文提出了一种新的ROS2调度方法，使用事件执行器实现固定作业级优先级调度器，支持任意有向无环图任务，填补了实时系统理论与ROS2调度分析之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前ROS2调度方法主要局限于简单的链式任务调度，缺乏对任意有向无环图任务的支持。现有研究多集中在链式调度和临时响应时间分析上，需要更通用的调度方案来支持复杂的ROS2应用。

Method: 提出使用事件执行器实现固定作业级优先级调度器，将ROS2应用抽象为树状森林结构，映射到传统的实时DAG任务模型。需要特殊实现事件队列和支持LIFO顺序消息传递的通信中间件。

Result: 该方法能够在单处理器系统上为任意ROS2图生成与传统固定优先级DAG任务调度器相同的调度结果，即使在没有通常所需的优先级信息的情况下也能实现。

Conclusion: 该研究填补了实时系统理论与ROS2调度分析之间的差距，为ROS2应用提供了更通用的调度解决方案，支持复杂的DAG任务结构，但需要ROS2平台支持LIFO消息传递等新特性。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [2] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成高性能计算代码方面的能力，特别是针对Mandelbrot集在不同并行范式下的C++实现。


<details>
  <summary>Details</summary>
Motivation: 并行编程仍然是高性能计算中最具挑战性的方面之一，需要深入了解同步、通信和内存模型。虽然现代C++标准和OpenMP、MPI等框架简化了并行性，但掌握这些范式仍然很复杂。大型语言模型在自动化代码生成方面显示出潜力，但它们在生成正确高效的高性能计算代码方面的效果尚不清楚。

Method: 系统评估包括ChatGPT 4和5、Claude和LLaMA在内的领先大型语言模型，在生成使用共享内存、基于指令和分布式内存范式的Mandelbrot集C++实现方面的能力。每个生成的程序都使用GCC 11.5.0编译和执行，以评估其正确性、鲁棒性和可扩展性。

Result: 结果显示，ChatGPT-4和ChatGPT-5在语法精度和可扩展性能方面表现强劲。

Conclusion: 大型语言模型特别是ChatGPT系列在生成高性能计算代码方面具有实际应用价值，能够产生语法正确且具有良好可扩展性的并行代码实现。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [3] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve是一个针对扩散大语言模型的高效服务系统，通过内存优化、计算调度和生成质量协同优化，解决了扩散模型特有的内存占用危机和资源振荡问题，在消费级和服务器级GPU上实现了显著的吞吐量提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型研究主要关注内核级优化，缺乏针对生产环境中扩散过程独特内存动态的整体服务框架。作者识别出dLLM特有的"内存占用危机"，由单一logit张量和计算密集型"刷新"阶段与带宽密集型"重用"阶段之间的严重资源振荡驱动。

Method: dLLM-Serve系统包含三个关键技术：1) Logit-Aware Activation Budgeting分解瞬态张量峰值；2) Phase-Multiplexed Scheduler交错处理异构请求阶段；3) Head-Centric Sparse Attention将逻辑稀疏性与物理存储解耦。

Result: 在多样化工作负载(LiveBench、Burst、OSC)和GPU(RTX 4090、L40S)上评估，相比最先进基线，dLLM-Serve在消费级RTX 4090上提升吞吐量1.61-1.81倍，在服务器级L40S上提升1.60-1.74倍，在重度争用下尾部延迟降低近4倍。

Conclusion: dLLM-Serve为可扩展的dLLM推理建立了首个蓝图，将理论算法稀疏性转化为跨异构硬件的实际时钟加速，解决了扩散模型服务的关键瓶颈问题。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [4] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE是一个可扩展的向量索引系统，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现了高准确率、低延迟和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分布式近似最近邻搜索（ANNS）索引设计在处理数十亿向量时难以平衡准确率、延迟和吞吐量之间的权衡，需要一种新的可扩展解决方案。

Method: SPIRE采用两个核心设计：1）识别平衡的分区粒度以避免读取成本爆炸；2）引入保持准确率的递归构建方法，构建具有可预测搜索成本和稳定准确率的多级索引。

Result: 在46个节点上处理高达80亿向量的实验中，SPIRE实现了高可扩展性，比最先进系统的吞吐量提高了9.64倍。

Conclusion: SPIRE通过创新的分区粒度和递归索引构建方法，成功解决了大规模向量搜索中的可扩展性问题，在准确率、延迟和吞吐量之间取得了更好的平衡。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [5] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云的联邦系统作为NIH HEAL计划数据的统一搜索、发现和分析平台


<details>
  <summary>Details</summary>
Motivation: HEAL计划产生的多样化数据分散在多个NIH和第三方数据存储库中，需要一个统一的发现平台来促进数据的二次利用

Method: 基于开源Gen3平台构建，使用框架服务（认证授权、持久标识符、元数据管理）和API与数据存储库互操作

Result: 平台已整合1000多项HEAL研究，每月数百用户使用，与19个数据存储库互操作，提供丰富元数据和安全的云计算环境

Conclusion: HEAL数据平台实现了对分散数据的统一发现和分析，通过确保FAIR原则最大化HEAL计划数据的价值

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [6] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec和UnifiedServe联合优化多模态大语言模型服务系统，通过协同多GPU视频解码加速预处理，并通过逻辑解耦但物理共享GPU资源的方式优化视觉编码和推理阶段，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的三阶段流水线（多模态预处理、视觉编码、LLM推理）存在显著系统瓶颈：1）CPU视频解码主导首token延迟；2）视觉编码器作为独立计算密集型阶段，无法与LLM预填充或解码协同批处理，导致阶段间阻塞和资源利用率低下。

Method: 提出FlashCodec和UnifiedServe两个互补设计：FlashCodec通过协同多GPU视频解码加速多模态预处理；UnifiedServe采用逻辑解耦但物理共享GPU资源的执行方式，优化视觉到文本和推理阶段，消除阶段间阻塞并最大化GPU利用率。

Result: 该框架能够服务最多3.0倍请求量或强制执行1.5倍更严格的SLOs，同时相比最先进系统实现最高4.4倍的吞吐量提升。

Conclusion: FlashCodec和UnifiedServe共同构成了端到端优化的MLLM服务堆栈，有效解决了多模态预处理延迟和视觉编码器与LLM推理阶段间的异构性瓶颈，显著提升了系统性能和资源利用率。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑框架的实体集扩展图方法，通过局部推理任务实现高效导航，避免完整图构建的计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统的线性实体集扩展方法无法揭示知识资源中丰富的分类结构，而完整的扩展图构建在现实场景中可能不切实际，需要更高效的局部导航方法。

Method: 采用基于逻辑的扩展图框架，其中节点表示由逻辑公式标记的语义泛化，边编码严格的语义包含关系。通过形式化推理任务来检查两个元组是否属于可比较、不可比较或相同的节点。

Result: 在现实假设下（如限制输入或实体描述），这些推理任务可以高效实现，支持对扩展图的局部增量导航，无需构建完整图结构。

Conclusion: 通过局部推理任务实现扩展图的高效导航，为实际应用提供了可行的解决方案，避免了完整图构建的计算负担。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [8] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能（SGI）的操作性定义，基于实用探究模型（PIM），并创建了包含1000多个跨学科样本的SGI-Bench基准，用于评估大语言模型在科学研究任务中的表现。研究发现现有模型在深度研究、实验设计和推理等方面存在显著差距，并提出了测试时强化学习（TTRL）方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI取得了进展，但缺乏一个连贯的科学通用智能（SGI）框架，即能够自主构思、研究和跨科学领域推理的能力。现有AI系统在参与真正的科学发现方面仍有局限。

Method: 基于实用探究模型（PIM：审议、构思、行动、感知）建立SGI的操作性定义，通过四个科学家对齐的任务（深度研究、想法生成、干/湿实验、实验推理）进行实现。创建SGI-Bench基准，包含1000多个专家策划的跨学科样本，评估最先进的LLMs。提出测试时强化学习（TTRL）方法，在推理时优化检索增强的新颖性奖励。

Result: 评估结果显示显著差距：深度研究的精确匹配率低（10-20%），想法缺乏可行性和细节，干实验代码可执行性高但执行结果准确性低，湿实验协议序列保真度低，多模态比较推理挑战持续存在。TTRL方法能够在不依赖参考答案的情况下提升假设新颖性。

Conclusion: 基于PIM的定义、以工作流程为中心的基准和实证见解为真正参与科学发现的AI系统奠定了基础。研究揭示了当前AI在科学任务中的局限性，并提出了改进方向，推动了科学通用智能的发展。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [9] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个针对LLM智能体工作流的计划感知自动上下文工程框架，通过上下文压缩优化多步推理，在保持性能的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多步工作流中会产生快速扩展的上下文，现有压缩方法忽视了计划感知特性，导致注意力稀释和推理成本增加。

Method: PAACE包含两个组件：PAACE-Syn生成带压缩监督的合成工作流数据，PAACE-FT训练蒸馏的计划感知压缩器。框架采用next-k-task相关性建模、计划结构分析、指令协同细化和函数保留压缩。

Result: 在AppWorld、OfficeBench和8-Objective QA等长视野基准测试中，PAACE在提高智能体正确性的同时显著降低上下文负载。蒸馏模型保留97%教师性能，推理成本降低一个数量级。

Conclusion: PAACE为LLM智能体的多步工作流提供了有效的计划感知上下文压缩方案，在保持性能的同时实现实用化部署，解决了智能体工作流中的上下文管理难题。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [10] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 本文提出关系中心的知识图谱问答新范式UniRel-R1，通过子图选择、多阶段图剪枝和强化学习微调LLM，解决传统实体中心问答无法处理关系查询的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答主要关注返回单个答案实体的实体中心查询，但现实世界中的查询通常是关系性的，需要理解实体之间的关联。现有方法无法有效处理这种关系中心的问题。

Method: 提出UniRel-R1统一框架，包含三个核心组件：子图选择、多阶段图剪枝、以及通过强化学习微调的大型语言模型。奖励函数设计鼓励选择紧凑且具体的子图，包含更多信息性关系和低度中间实体。

Result: 大量实验表明，UniRel-R1在连接性和奖励方面相比基准方法取得显著提升，并且能够有效泛化到未见过的实体和关系。

Conclusion: UniRel-R1成功解决了关系中心知识图谱问答的挑战，通过统一的框架处理候选子图过多的问题，为理解实体间语义连接提供了有效解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [11] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 研究使用LLM驱动的智能体模拟社会冲突，发现物质威胁直接增加敌意，而象征性威胁主要通过内群体偏见间接影响，且只在物质威胁缺失时增加敌意。


<details>
  <summary>Details</summary>
Motivation: 人类冲突通常归因于物质条件和象征性价值观受到的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究进展受到因果控制薄弱、伦理约束和时间数据稀缺的限制。

Method: 使用大型语言模型驱动的智能体在虚拟社会中模拟，独立变化现实威胁和象征性威胁，同时追踪行动、语言和态度。通过表征分析研究LLM如何编码这些状态，并通过操纵这些状态来因果性地改变行为。

Result: LLM将现实威胁、象征性威胁和敌意编码为不同的内部状态；现实威胁直接增加敌意，而象征性威胁效应较弱，完全通过内群体偏见中介，且只在现实威胁缺失时增加敌意；非敌意的群体间接触能缓冲冲突升级，结构性不对称使敌意集中在多数群体中。

Conclusion: 研究提供了威胁驱动冲突的因果解释框架，区分了现实威胁和象征性威胁的不同作用机制，为理解社会冲突动态提供了新的模拟方法学视角。

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [12] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 将AIXI强化学习智能体推广到更广泛的效用函数类别，通过处理信念分布中只能预测有限历史前缀的假设，探讨了半测度损失作为死亡概率或完全无知的不同解释，并研究了使用Choquet积分计算期望效用的计算性


<details>
  <summary>Details</summary>
Motivation: AIXI智能体通常假设效用函数定义在完整交互历史上，但实际中许多假设只能预测有限历史前缀，这导致半测度损失问题。需要解决如何为这些有限历史分配效用，并探讨半测度损失的不同解释（死亡概率vs完全无知）

Method: 将AIXI推广到更广泛的效用函数类别；使用不精确概率理论中的Choquet积分计算期望效用；分析不同解释下（死亡解释vs完全无知）的效用分配方法；研究这些方法的可计算性水平

Result: 标准递归值函数可以作为Choquet积分的特例恢复；在死亡解释下最一般的期望效用不能表征为Choquet积分；探讨了不同方法在可计算性方面的特性

Conclusion: 通过推广AIXI智能体以处理更广泛的效用函数，并利用不精确概率理论中的Choquet积分，为处理有限历史预测问题提供了新的理论框架，但死亡解释下的最一般期望效用需要其他数学工具

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [13] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文提出了一种ASP求解器在环的方法，通过求解器引导的指令微调来提升大语言模型生成答案集编程代码的能力，仅需自然语言问题描述和解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在通用编程语言代码生成方面表现良好，但在领域特定语言（如答案集编程ASP）的代码生成上仍面临挑战。ASP是一种解决组合搜索问题的有效方法，但LLMs在ASP代码生成上的效果受到预训练阶段所见示例数量有限的限制。

Method: 提出ASP求解器在环的方法进行求解器引导的指令微调。方法仅需自然语言问题描述及其解决方案：1）从LLMs采样ASP语句作为程序延续；2）利用ASP声明式编程的特性（部分编码逐步缩小解空间），基于求解器反馈将采样分为接受和拒绝实例；3）对筛选数据进行监督微调；4）使用求解器引导的搜索（包括最佳N采样）进一步提高鲁棒性。

Result: 实验表明，该方法在两种不同的提示设置和两个数据集上均取得了持续改进。

Conclusion: 通过ASP求解器在环的指令微调方法，可以有效提升LLMs在ASP代码生成任务上的性能，解决了领域特定语言代码生成的挑战。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [14] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: SAGE是一个基于强化学习的框架，通过技能库增强LLM智能体的自我进化能力，在AppWorld任务中显著提升了目标完成率并减少了交互步骤和token消耗。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂推理和多轮交互方面表现出色，但在新环境中部署时难以持续改进和适应。现有的技能库方法主要依赖LLM提示，难以实现一致的技能库实施。

Method: 提出SAGE（Skill Augmented GRPO for self-Evolution）强化学习框架，通过Sequential Rollout机制在相似任务链中迭代部署智能体，让先前任务生成的技能积累到库中供后续任务使用，并通过Skill-integrated Reward增强技能生成和利用。

Result: 在AppWorld实验中，SAGE应用于有专家经验的监督微调模型，实现了8.9%更高的场景目标完成率，同时需要减少26%的交互步骤和生成59%更少的token，在准确性和效率上都显著优于现有方法。

Conclusion: SAGE框架通过强化学习有效增强了LLM智能体的自我改进能力，通过技能库和顺序部署机制实现了更高效的任务完成，为智能体在动态环境中的持续适应提供了有前景的解决方案。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [15] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出基于所罗门诺夫理论的LLM假设加权方法，通过简洁性和预测拟合度评估多个候选解，在不确定性下实现更平衡的概率分布


<details>
  <summary>Details</summary>
Motivation: 现实世界AI任务中，稀疏数据下的不确定性推理是核心挑战，现有方法难以在准确性和简洁性之间平衡评估多个候选解决方案

Method: 提出所罗门诺夫启发的方法，对LLM生成的假设按简洁性和预测拟合度进行加权，应用于Mini-ARC基准任务，生成每个单元格预测的所罗门诺夫加权混合

Result: 相比贝叶斯模型平均，所罗门诺夫评分在竞争假设间更均匀地分布概率，而BMA集中在最可能但可能有缺陷的候选上，产生保守、不确定性感知的输出

Conclusion: 算法信息论先验对于可解释、可靠的不确定性下多假设推理具有重要价值

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [16] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的可解释多模态检索增强生成方法，通过两阶段强化微调框架提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法缺乏对检索和响应生成背后推理逻辑的解释，限制了结果的可解释性。为了解决这一缺陷，作者提出引入强化学习来增强多模态检索增强生成的推理能力。

Method: 采用两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度的点式排序，过滤显著不相关的文档；第二阶段使用基于推理的强化微调联合优化细粒度的列表式排序和答案生成，引导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 提出的基于强化学习的可解释MMRAG方法能够有效提升多模态大语言模型的推理能力，实现可解释的多模态检索增强生成，为复杂多模态场景提供了更可信的解决方案。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [17] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench是一个针对统一多模态模型（UMMs）的全维度评估基准，能够在单一评估过程中同时测试理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前对统一多模态模型的评估是分离的，分别评估其理解和生成能力，缺乏一个综合的评估框架。需要开发一个能够全面评估UMMs各方面能力的基准。

Method: UmniBench利用人类检查的提示和问答对，通过UMM自身的理解能力来评估其生成和编辑能力。这种简单有效的范式允许在单一评估过程中同时测试理解、生成和编辑三个维度。

Result: 基于UmniBench对24个流行模型进行了基准测试，包括UMMs和单能力大模型。该基准提供了对统一模型更全面客观的评估视角，并为社区模型性能改进提供了支持。

Conclusion: UmniBench为统一多模态模型提供了一个全面、客观的评估框架，能够同时评估理解、生成和编辑能力，填补了现有评估方法的空白，有助于推动社区模型的发展。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [18] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友配合下的适应性变化。足球俱乐部成功很大程度上取决于转会决策，但预测转会是否成功仍然困难，因为场上表现具有强烈的上下文依赖性。

Method: 提出EventGPT模型，基于GPT风格的自回归transformer架构，将比赛视为离散token序列，联合预测下一持球动作的类型、位置、时间及其估计的残差持球价值(rOBV)。模型学习球员嵌入表示，能够通过替换球员嵌入到新事件序列中进行反事实模拟。

Result: 在五个赛季的英超联赛事件数据上评估，EventGPT在下一事件预测准确性和空间精度方面优于现有基于序列的基线模型。通过案例研究展示了模型在转会分析中的实际效用，如比较不同体系下前锋表现，识别特定角色的风格替代者。

Conclusion: EventGPT提供了一个原则性的方法来评估转会匹配度，通过反事实模拟能够预测球员在不同球队或战术结构中的行为分布和价值变化，为转会决策提供更科学的依据。

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [19] [Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation](https://arxiv.org/abs/2512.17308)
*Daksh Jain,Aarya Jain,Ashutosh Desai,Avyakt Verma,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.AI

TL;DR: LLMs在宝可梦对战中被评估为战略决策者，既能做出战术决策又能生成平衡的游戏内容，无需领域特定训练即可作为动态游戏对手。


<details>
  <summary>Details</summary>
Motivation: 宝可梦对战需要类型匹配、统计权衡和风险评估等战略思维，为评估大语言模型的战略决策能力提供了独特测试平台。

Method: 开发了基于回合的宝可梦对战系统，LLMs根据战斗状态选择招式而非预编程逻辑，系统包含类型效果乘数、基于统计的伤害计算和多宝可梦队伍管理。

Result: 通过多个模型架构的系统评估，测量了胜率、决策延迟、类型对齐准确性和令牌效率，结果表明LLMs无需领域特定训练即可作为动态游戏对手。

Conclusion: LLMs兼具战术推理和内容创造的双重能力，既能作为玩家又能作为设计师，对交互娱乐中的程序生成和自适应难度系统有重要意义。

Abstract: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

</details>


### [20] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 该论文从算法信息论角度提出了一种概念定义框架，将概念视为与智能体整体经验相关的信息对象，通过可逆一致性关系和冗余信息度量来形式化概念发现与演化过程。


<details>
  <summary>Details</summary>
Motivation: 人类概念本身具有流动性（如冥王星不再被视为行星），传统基于字典标签的概念定义无法捕捉这种动态性。需要一种能够被修订、比较和在智能体间对齐的概念定义框架。

Method: 采用算法信息论视角，将概念定义为通过可逆一致性关系与智能体经验相关的信息对象。提出冗余信息度量来衡量概念分解的自然性，并形式化辩证法作为优化动态过程，让竞争概念通过更短的描述来解释新信息。

Result: 建立了一个形式化框架，使得概念存在性成为可检验的结构性主张，防止概念脱离经验基础。提出了低代价概念传输和多智能体对齐机制，通过共享协议下的种子实现概念重构。

Conclusion: 该研究为AI从原始经验中自主发现概念提供了理论基础，通过信息论约束确保概念的实证基础，并形式化了概念演化、传输和对齐的计算机制。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [21] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 该研究将Rashomon效应从分类任务扩展到序列决策领域，发现多个策略在行为表现相同的情况下内部结构存在差异，并展示了这种多样性集合在分布偏移下的鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未被探索。序列决策中策略的行为验证比分类任务更复杂，因为存在随机转移和单次轨迹的不确定性。

Method: 使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，验证策略是否表现出相同行为。通过构造Rashomon集合，研究其多样性和鲁棒性特性。

Result: 实验证明Rashomon效应存在于序列决策中。从Rashomon集合构建的集成策略对分布偏移表现出比单个策略更强的鲁棒性。从Rashomon集合派生的宽松策略在保持最优性能的同时减少了验证的计算需求。

Conclusion: Rashomon效应在序列决策中确实存在，这种多样性不仅是一个理论现象，还具有实际应用价值，如提高鲁棒性和降低验证成本。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [22] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（GPT-4o）的诊断聊天机器人，结合检索增强生成和可解释AI技术，在医疗诊断中实现90%准确率和100%的Top-3准确率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低、成本上升和专家资源有限等问题，导致治疗延误和不良健康结果。现有AI诊断系统缺乏交互性和透明度，难以在实际临床环境中有效应用。

Method: 采用基于GPT-4o的大语言模型诊断聊天机器人，结合检索增强生成和可解释AI技术。系统通过动态对话提取和标准化症状，使用相似性匹配和自适应提问优先考虑潜在诊断，并通过思维链提示提供透明推理。

Result: 与传统机器学习模型（朴素贝叶斯、逻辑回归、SVM、随机森林、KNN）相比，LLM系统表现优异，达到90%的准确率和100%的Top-3准确率。

Conclusion: 该研究为医疗领域提供了更透明、交互性强且临床相关的AI解决方案，展示了LLM在改善医疗诊断方面的巨大潜力。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [23] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 本文提出了定时奖励机（TRMs），扩展了传统奖励机以纳入时间约束，使强化学习中的奖励规范能表达精确的时间要求，并开发了相应的模型无关RL算法。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了在时间敏感应用中的使用。需要一种能够表达时间相关奖励规范的机制，例如对延迟施加成本、对及时行动给予奖励。

Method: 提出了定时奖励机（TRMs），将时间约束整合到奖励结构中。研究了模型无关RL框架（表格Q学习）在数字和实时语义下学习最优策略。算法通过定时自动机抽象将TRM整合到学习中，并利用反事实想象启发式方法利用TRM结构改进搜索。

Result: 实验证明，算法在流行的RL基准测试中学习到的策略能够获得高奖励同时满足TRM指定的时间约束。比较研究展示了不同TRM语义下的性能表现，消融实验突出了反事实想象的优势。

Conclusion: 定时奖励机为强化学习提供了更丰富的奖励规范表达能力，能够处理时间敏感任务，相应的学习算法能够有效学习满足时间约束的最优策略。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [24] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究通过跨国实验发现，AI拟人化设计对用户信任和参与度的影响具有文化特异性，挑战了普遍风险假设


<details>
  <summary>Details</summary>
Motivation: 当前AI系统日益模仿人类特征引发对拟人化风险的担忧，但现有安全框架主要基于西方理论假设，缺乏全球用户群体的实证研究，需要验证拟人化设计与用户信任、参与度的因果关系

Method: 在10个不同国家进行两项大规模跨国实验（N=3,500），涉及与AI系统的实时开放式互动，实验性地测试拟人化设计杠杆对用户感知和行为的影响

Result: 用户评估AI拟人化时更关注交互性线索而非理论特征；拟人化设计能增加用户拟人化感知，但不会普遍增加行为层面的参与度和信任；文化因素调节了拟人化与行为结果的关系，某些设计在某些文化中增加信任，在其他文化中可能产生相反效果

Conclusion: AI拟人化设计的影响是文化中介的复杂景观，挑战了普遍风险的叙事，需要超越一刀切的AI治理方法，考虑文化多样性

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [25] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 本文提出了推理定律（LoRe）框架，通过计算定律和准确率定律来形式化大型推理模型的理想推理行为，并开发了LoRe-Bench基准来评估模型的单调性和组合性，最后通过微调方法提升模型对计算定律的遵循度，从而改善推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）表现出色，但其推理行为常常违反直觉，导致推理能力不足。为了从理论上形式化理想的推理行为，需要建立一个统一的框架来表征LRMs的内在推理模式。

Method: 1. 提出推理定律（LoRe）框架，包括计算定律（推理计算量应与问题复杂度线性缩放）和补充的准确率定律。2. 由于问题复杂度难以量化，通过单调性和组合性这两个可处理属性来检验定律假设。3. 开发LoRe-Bench基准系统评估大型推理模型的这两个属性。4. 提出有效的微调方法，强制模型遵循计算定律的组合性。

Result: 评估显示大多数推理模型表现出合理的单调性但缺乏组合性。通过强制计算定律组合性的微调方法，模型在多个基准测试中表现出持续改进的推理性能，并揭示了不同属性和定律之间的协同效应。

Conclusion: 推理定律（LoRe）为形式化大型推理模型的理想行为提供了理论框架，通过LoRe-Bench评估和针对性微调，可以提升模型对计算定律的遵循度，从而显著改善推理性能，揭示了模型属性与推理定律之间的重要关系。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [26] [On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues](https://arxiv.org/abs/2512.17060)
*Monika Zamojska,Jarosław A. Chudziak*

Main category: cs.MA

TL;DR: 本文提出了一种基于交互分析理论的多智能体系统，通过将每个智能体划分为父母、成人和儿童三种自我状态，并结合信息检索机制，增强了LLM智能体的心理深度和行为真实性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在模拟人类行为时缺乏心理深度和一致性，只能提供直接或统计上可能的答案，而无法捕捉真实人类思维中的深层目标、情感冲突和动机。在社会科学、政治学和心理学研究中，需要更真实的智能体来模拟群体动态和社会行为。

Method: 提出基于交互分析理论的多智能体系统，将每个智能体划分为三种自我状态：父母、成人和儿童。这些自我状态作为独立的知识结构，拥有各自的视角和推理风格。同时，智能体配备信息检索机制，可以从向量存储中检索相关上下文信息来丰富响应过程。

Result: 通过在模拟对话场景中进行消融实验，比较了有信息检索和无信息检索的智能体表现。结果表明，该架构具有良好前景，为探索基于心理学的结构如何丰富智能体行为开辟了新方向。

Conclusion: 本文的贡献在于提出了一种将交互分析理论与上下文信息检索相结合的智能体架构，能够增强基于LLM的多智能体模拟的真实性，为创建更具心理深度的智能体系统提供了新思路。

Abstract: LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.

</details>


### [27] [MAPPO-LCR: Multi-Agent Policy Optimization with Local Cooperation Reward in Spatial Public Goods Games](https://arxiv.org/abs/2512.17187)
*Zhaoqilin Yang,Axin Xiang,Kedi Yang,Tianjun Liu,Youliang Tian*

Main category: cs.MA

TL;DR: 首次将MAPPO引入空间公共物品博弈，提出MAPPO-LCR方法，通过局部合作奖励促进合作涌现，相比PPO在耦合收益环境中表现更优


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖演化更新规则或基于价值的强化学习方法，难以处理大规模交互群体中的收益耦合和非平稳性问题。传统PPO将智能体视为独立学习者，忽略了空间公共物品博弈中通过重叠群体交互产生的内在收益耦合。

Method: 引入多智能体近端策略优化（MAPPO）到空间公共物品博弈中，通过集中式评论家评估联合策略配置。提出MAPPO-LCR方法，添加局部合作奖励使策略更新与周围合作密度对齐，不改变原始博弈结构，保持分散执行的同时在训练期间实现群体级价值估计。

Result: 大量仿真实验显示MAPPO-LCR能够稳定促进合作涌现并在不同增强因子下实现可靠收敛。统计分析进一步证实了MAPPO在空间公共物品博弈中相对于PPO的学习优势。

Conclusion: MAPPO框架通过集中式评论家有效解决了空间公共物品博弈中的收益耦合问题，MAPPO-LCR方法通过局部合作奖励成功促进了合作行为的涌现，为处理大规模交互群体中的集体困境提供了新的强化学习解决方案。

Abstract: Spatial public goods games model collective dilemmas where individual payoffs depend on population-level strategy configurations. Most existing studies rely on evolutionary update rules or value-based reinforcement learning methods. These approaches struggle to represent payoff coupling and non-stationarity in large interacting populations. This work introduces Multi-Agent Proximal Policy Optimization (MAPPO) into spatial public goods games for the first time. In these games, individual returns are intrinsically coupled through overlapping group interactions. Proximal Policy Optimization (PPO) treats agents as independent learners and ignores this coupling during value estimation. MAPPO addresses this limitation through a centralized critic that evaluates joint strategy configurations. To study neighborhood-level cooperation signals under this framework, we propose MAPPO with Local Cooperation Reward, termed MAPPO-LCR. The local cooperation reward aligns policy updates with surrounding cooperative density without altering the original game structure. MAPPO-LCR preserves decentralized execution while enabling population-level value estimation during training. Extensive simulations demonstrate stable cooperation emergence and reliable convergence across enhancement factors. Statistical analyses further confirm the learning advantage of MAPPO over PPO in spatial public goods games.

</details>
