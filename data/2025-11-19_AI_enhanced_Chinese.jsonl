{"id": "2511.14299", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.14299", "abs": "https://arxiv.org/abs/2511.14299", "authors": ["Xiaochuan Liu", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen"], "title": "DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning", "comment": null, "summary": "In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.", "AI": {"tldr": "DataSage\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u3001\u591a\u89d2\u8272\u8fa9\u8bba\u673a\u5236\u548c\u591a\u8def\u5f84\u63a8\u7406\u6765\u89e3\u51b3\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u667a\u80fd\u4f53\u5728\u9886\u57df\u77e5\u8bc6\u5229\u7528\u4e0d\u8db3\u3001\u5206\u6790\u6df1\u5ea6\u6d45\u548c\u4ee3\u7801\u751f\u6210\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u5728InsightBench\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6570\u636e\u9a71\u52a8\u65f6\u4ee3\uff0c\u5168\u81ea\u52a8\u7aef\u5230\u7aef\u6570\u636e\u5206\u6790\u7279\u522b\u662f\u6d1e\u5bdf\u53d1\u73b0\u5bf9\u4e8e\u7ec4\u7ec7\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u667a\u80fd\u4f53\u5b58\u5728\u9886\u57df\u77e5\u8bc6\u5229\u7528\u4e0d\u8db3\u3001\u5206\u6790\u6df1\u5ea6\u6d45\u548c\u4ee3\u7801\u751f\u6210\u6613\u51fa\u9519\u7b49\u9650\u5236\uff0c\u65e0\u6cd5\u63d0\u4f9b\u6ee1\u610f\u7ed3\u679c\u3002", "method": "\u63d0\u51faDataSage\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u7279\u6027\uff1a\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u4ee5\u4e30\u5bcc\u5206\u6790\u4e0a\u4e0b\u6587\uff0c\u591a\u89d2\u8272\u8fa9\u8bba\u673a\u5236\u6a21\u62df\u591a\u6837\u5316\u5206\u6790\u89c6\u89d2\u5e76\u52a0\u6df1\u5206\u6790\u6df1\u5ea6\uff0c\u591a\u8def\u5f84\u63a8\u7406\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u548c\u6d1e\u5bdf\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728InsightBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDataSage\u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u6d1e\u5bdf\u667a\u80fd\u4f53\u3002", "conclusion": "DataSage\u4e3a\u81ea\u52a8\u5316\u6570\u636e\u6d1e\u5bdf\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.13727", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13727", "abs": "https://arxiv.org/abs/2511.13727", "authors": ["Sophie Wenning"], "title": "Boosting performance: Gradient Clock Synchronisation with two-way measured links", "comment": "Master's thesis", "summary": "This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (F\u00fcgger et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\\% to 0,1\\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.", "AI": {"tldr": "\u672c\u8bba\u6587\u5c06GCS\u7b97\u6cd5\u7684\u5f62\u5f0f\u6a21\u578b\u6269\u5c55\u5230\u5b9e\u73b0\u8fd1\u4f3c\u7684\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u5c06\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\u6539\u4e3a\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u79fb\u9664\u4e86\u8bb8\u591a\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4e86GCS\u7684\u6838\u5fc3\u884c\u4e3a\u3002", "motivation": "\u6269\u5c55GCS\u7b97\u6cd5\u7684\u5f62\u5f0f\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u66f4\u63a5\u8fd1\u5b9e\u9645\u5b9e\u73b0\u7684\u5047\u8bbe\u4e0b\u8fd0\u884c\uff0c\u79fb\u9664\u5148\u524d\u5de5\u4f5c\u4e2d\u4e3a\u8bc1\u660e\u6027\u80fd\u800c\u65bd\u52a0\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u5c06\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\u66ff\u6362\u4e3a\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u53d6\u6d88\u7edf\u4e00\u94fe\u8def\u957f\u5ea6\u7684\u8981\u6c42\uff0c\u5f62\u5f0f\u5316\u5efa\u6a21\u9891\u7387\u6e90\uff0c\u5bf9\u7b97\u6cd5\u4f30\u8ba1\u8bef\u5dee\u7684\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\u8fdb\u884c\u7ec6\u7c92\u5ea6\u533a\u5206\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u786e\u5b9a\u6027\u5bf9\u7b97\u6cd5\u4f30\u8ba1\u8bef\u5dee\u7684\u8d21\u732e\uff0c\u4ece\u6bcf\u94fe\u8def\u5ef6\u8fdf\u7684\u6570\u91cf\u7ea7\u964d\u4f4e\u5230\u6bcf\u94fe\u8def\u5ef6\u8fdf\u768410%\u52300.1%\u8303\u56f4\u5185\uff0c\u5e76\u7ed9\u51fa\u4e86GCS\u5c40\u90e8\u548c\u5168\u5c40\u504f\u5dee\u7684\u5339\u914d\u4e0a\u754c\u3002", "conclusion": "\u901a\u8fc7\u6539\u53d8\u6d4b\u91cf\u8303\u5f0f\u548c\u79fb\u9664\u9650\u5236\uff0c\u6210\u529f\u6269\u5c55\u4e86GCS\u7b97\u6cd5\u7684\u5f62\u5f0f\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u66f4\u73b0\u5b9e\u7684\u5047\u8bbe\u4e0b\u8fd0\u884c\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u5e76\u4fdd\u6301\u4e86\u7b97\u6cd5\u7684\u6838\u5fc3\u884c\u4e3a\u3002"}}
{"id": "2511.13782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13782", "abs": "https://arxiv.org/abs/2511.13782", "authors": ["Xiaoxing Lian", "Aidong Yang", "Jun Zhu", "Peng Wang", "Yue Zhang"], "title": "Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models", "comment": "10 pages,a detail and effective benchmark for spatial reasoning", "summary": "Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SpatiaLite\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u5148\u8fdbVLMs\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u8868\u793a\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\uff0c\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u63d0\u51fa\u4e86Imagery Driven Framework\u6765\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u3001\u95ee\u9898\u89e3\u51b3\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7a7a\u95f4\u63a8\u7406\u8fd9\u4e00\u4eba\u7c7b\u8ba4\u77e5\u7684\u57fa\u672c\u7ec4\u6210\u90e8\u5206\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u7814\u7a76\u8005\u5047\u8bbe\u60f3\u8c61\u529b\u662f\u7a7a\u95f4\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u4e3b\u5bfc\u63a8\u7406\u673a\u5236\u3002", "method": "\u5f15\u5165SpatiaLite\u8fd9\u4e00\u5b8c\u5168\u5408\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8054\u5408\u6d4b\u91cf\u7a7a\u95f4\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u5206\u6790VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u673a\u5236\uff0c\u5e76\u63d0\u51faImagery Driven Framework\u7528\u4e8e\u6570\u636e\u5408\u6210\u548c\u8bad\u7ec3\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u7ed3\u679c\uff1a1\uff09\u5148\u8fdbVLMs\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u8868\u793a\u8fdb\u884c\u63a8\u7406\u548c\u60f3\u8c61\uff0c\u5728\u9700\u8981\u611f\u77e5\u7a7a\u95f4\u5173\u7cfb\u548c3D\u51e0\u4f55\u53d8\u6362\u7684\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1b2\uff09\u5f53\u524d\u7a7a\u95f4\u63a8\u7406\u673a\u5236\u6548\u7387\u4e25\u91cd\u4f4e\u4e0b\uff0c\u968f\u7740\u53d8\u6362\u590d\u6742\u5ea6\u589e\u52a0\uff0ctoken\u4f7f\u7528\u91cf\u5feb\u901f\u589e\u957f\uff1b3\uff09\u63d0\u51fa\u7684IDF\u6846\u67b6\u53ef\u4ee5\u9690\u5f0f\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u5bf9VLMs\u7684\u7a7a\u95f4\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u57fa\u4e8eSpatiaLite\uff0c\u8fd9\u9879\u5de5\u4f5c\u63cf\u7ed8\u4e86\u5148\u8fdbVLMs\u7684\u7a7a\u95f4\u63a8\u7406\u9650\u5236\u548c\u6a21\u5f0f\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u7f3a\u9677\uff0c\u5e76\u4e3a\u672a\u6765\u8fdb\u5c55\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002Imagery Driven Framework\u6709\u671b\u6539\u8fdbVLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.13728", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13728", "abs": "https://arxiv.org/abs/2511.13728", "authors": ["Maximilian Reisecker", "Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum", "comment": "In IEEE ACM 12th International Conference on Big Data Computing, Applications and Technologies (BDCAT 25), 2025, Nantes, France", "summary": "Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.", "AI": {"tldr": "Gaia\u662f\u4e00\u4e2aGPU\u5373\u670d\u52a1\u6a21\u578b\u548c\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u6267\u884c\u6a21\u5f0f\u8bc6\u522b\u548c\u8fd0\u884c\u65f6\u8bc4\u4f30\uff0c\u4e3a\u5f02\u6784\u73af\u5883\u4e2d\u7684\u65e0\u670d\u52a1\u5668AI\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9bSLO\u611f\u77e5\u3001\u6210\u672c\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u5e73\u53f0\u5728\u7ba1\u7406\u786c\u4ef6\u52a0\u901f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9759\u6001\u7528\u6237-\u8bbe\u5907\u5206\u914d\u65e0\u6cd5\u5728\u53d8\u5316\u8d1f\u8f7d\u6216\u653e\u7f6e\u4e0b\u786e\u4fddSLO\u5408\u89c4\u6027\uff0c\u4e00\u6b21\u6027\u52a8\u6001\u9009\u62e9\u5f80\u5f80\u5bfc\u81f4\u6b21\u4f18\u6216\u6210\u672c\u4f4e\u6548\u7684\u914d\u7f6e\u3002", "method": "Gaia\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u6267\u884c\u6a21\u5f0f\u6807\u8bc6\u5668\uff08\u5728\u90e8\u7f72\u65f6\u68c0\u67e5\u51fd\u6570\u4ee3\u7801\u5e76\u53d1\u51fa\u56db\u79cd\u6267\u884c\u6a21\u5f0f\u4e4b\u4e00\uff09\u548c\u52a8\u6001\u51fd\u6570\u8fd0\u884c\u65f6\uff08\u6301\u7eed\u91cd\u65b0\u8bc4\u4f30\u7528\u6237\u5b9a\u4e49\u7684SLO\u4ee5\u5728CPU\u548cGPU\u540e\u7aef\u4e4b\u95f4\u8fdb\u884c\u5347\u7ea7\u6216\u964d\u7ea7\uff09\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cGaia\u65e0\u7f1d\u9009\u62e9\u6700\u9002\u5408\u5de5\u4f5c\u8d1f\u8f7d\u7684\u786c\u4ef6\u52a0\u901f\uff0c\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe95%\u3002", "conclusion": "Gaia\u80fd\u591f\u4e3a\u5f02\u6784\u73af\u5883\u4e2d\u7684\u65e0\u670d\u52a1\u5668AI\u5b9e\u73b0SLO\u611f\u77e5\u3001\u6210\u672c\u9ad8\u6548\u7684\u52a0\u901f\u3002"}}
{"id": "2511.13738", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13738", "abs": "https://arxiv.org/abs/2511.13738", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Kyeongpil Min", "Chaebin Jung", "Woojoo Lee"], "title": "TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI", "comment": "8 pages, 6 figures, 4 Tables, DATE 2026 accepted paper", "summary": "The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.", "AI": {"tldr": "TT-Edge\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18AI\u5904\u7406\u5668\u4e0a\u4e13\u95e8\u4f18\u5316\u5f20\u91cf\u8bad\u7ec3\u5206\u89e3(TTD)\u7684\u8ba1\u7b97\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86TTD\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u95ee\u9898\u3002", "motivation": "\u5206\u5e03\u5f0f\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u6a21\u578b\u538b\u7f29\u3002TTD\u867d\u7136\u63d0\u4f9b\u9ad8\u538b\u7f29\u6bd4\u548c\u4f4e\u7cbe\u5ea6\u635f\u5931\uff0c\u4f46\u5176\u91cd\u590d\u7684SVD\u548c\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u5728\u4f4e\u529f\u8017\u5904\u7406\u5668\u4e0a\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u5f00\u9500\u3002", "method": "TT-Edge\u5c06SVD\u5206\u89e3\u4e3a\u53cc\u5bf9\u89d2\u5316\u548c\u5bf9\u89d2\u5316\u4e24\u4e2a\u9636\u6bb5\uff0c\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u5378\u8f7d\u5230\u4e13\u95e8\u7684TTD\u5f15\u64ce\u3002\u8be5\u5f15\u64ce\u4e0e\u73b0\u6709\u7684GEMM\u52a0\u901f\u5668\u7d27\u5bc6\u96c6\u6210\uff0c\u51cf\u5c11\u9891\u7e41\u7684\u77e9\u9635\u5411\u91cf\u4f20\u8f93\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u91cd\u7528GEMM\u8d44\u6e90\u5e76\u4f7f\u7528\u5171\u4eab\u6d6e\u70b9\u5355\u5143\u3002", "result": "\u5728\u57fa\u4e8eRISC-V\u7684\u8fb9\u7f18AI\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\uff0c\u538b\u7f29ResNet-32\u6a21\u578b\u65f6\u76f8\u6bd4\u4ec5\u4f7f\u7528GEMM\u7684\u57fa\u7ebf\u83b7\u5f971.7\u500d\u52a0\u901f\uff0c\u603b\u4f53\u80fd\u8017\u964d\u4f4e40.2%\u3002\u603b\u529f\u8017\u4ec5\u589e\u52a04%\uff0c\u786c\u4ef6\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "TT-Edge\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86TTD\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u57fa\u4e8e\u538b\u7f29\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.13751", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.13751", "abs": "https://arxiv.org/abs/2511.13751", "authors": ["Shinnung Jeong", "Chihyo Ahn", "Huanzhi Pu", "Jisheng Zhao", "Hyesoon Kim", "Blaise Tine"], "title": "Inside VOLT: Designing an Open-Source GPU Compiler", "comment": "11 pages, 10 figures, two tables, two algorithms", "summary": "Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.\n  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions", "AI": {"tldr": "VOLT\u662f\u4e00\u4e2a\u9488\u5bf9\u5f00\u6e90GPU\u7684\u8f7b\u91cf\u7ea7\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709GPU\u7a0b\u5e8f\u5728\u5f00\u6e90GPU\u67b6\u6784\u4e0a\u6267\u884c\u548c\u4f18\u5316\u7684\u6280\u672f\u6311\u6218\u3002", "motivation": "\u5f00\u6e90GPU\u67b6\u6784\u9700\u8981\u590d\u6742\u7684\u7f16\u8bd1\u5668\u6846\u67b6\u6765\u652f\u6301SIMT\u6267\u884c\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u94fe\u5728\u6280\u672f\u590d\u6742\u5ea6\u548c\u5f00\u53d1\u6210\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8bbe\u8ba1\uff0c\u5728\u4e2d\u95f4\u7aef\u96c6\u4e2d\u5904\u7406SIMT\u76f8\u5173\u5206\u6790\u548c\u4f18\u5316\uff0c\u652f\u6301\u591a\u79cd\u524d\u7aef\u8bed\u8a00\u548c\u5f00\u6e90GPU\u786c\u4ef6\uff0c\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u3002", "result": "VOLT\u80fd\u591f\u8de8\u591a\u4e2a\u62bd\u8c61\u5c42\u6b21\u751f\u6210\u548c\u4f18\u5316SIMT\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7ISA\u6269\u5c55\u548c\u4e3b\u673a\u8fd0\u884c\u65f6API\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "VOLT\u4e3a\u5f00\u6e90GPU\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684GPU\u67b6\u6784\u9700\u6c42\u3002"}}
{"id": "2511.13852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13852", "abs": "https://arxiv.org/abs/2511.13852", "authors": ["Anna Rodum Bj\u00f8ru", "Rafael Caba\u00f1as", "Helge Langseth", "Antonio Salmer\u00f3n"], "title": "Causal computations in Semi Markovian Structural Causal Models using divide and conquer", "comment": "36 pages, 7 figures, 1 appendix", "summary": "Recently, Bj\u00f8ru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \\textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5c06Bj\u00f8ru\u7b49\u4eba\u63d0\u51fa\u7684\u53cd\u4e8b\u5b9e\u6982\u7387\u8fb9\u754c\u8ba1\u7b97\u7b97\u6cd5\u4ece\u9a6c\u5c14\u53ef\u592b\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u6269\u5c55\u5230\u534a\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u4ee5\u5904\u7406\u66f4\u590d\u6742\u7684\u6df7\u6742\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u5176\u4e2d\u5916\u751f\u53d8\u91cf\u4ec5\u5f71\u54cd\u5355\u4e2a\u5185\u751f\u53d8\u91cf\u3002\u534a\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u5141\u8bb8\u5916\u751f\u53d8\u91cf\u5f71\u54cd\u591a\u4e2a\u5185\u751f\u53d8\u91cf\uff0c\u80fd\u8868\u793a\u66f4\u590d\u6742\u7684\u6df7\u6742\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u793a\u4f8b\u8bf4\u660e\u6269\u5c55\u6311\u6218\uff0c\u63d0\u51fa\u66ff\u4ee3\u89e3\u51b3\u65b9\u6848\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u548c\u8ba1\u7b97\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u5c06\u5206\u6cbb\u7b97\u6cd5\u6269\u5c55\u5230\u534a\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u7684\u6311\u6218\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u79cd\u66ff\u4ee3\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u6210\u529f\u5c06\u53cd\u4e8b\u5b9e\u6982\u7387\u8fb9\u754c\u8ba1\u7b97\u65b9\u6cd5\u6269\u5c55\u5230\u534a\u9a6c\u5c14\u53ef\u592b\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u4e3a\u5904\u7406\u66f4\u590d\u6742\u7684\u6df7\u6742\u5173\u7cfb\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.13892", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13892", "abs": "https://arxiv.org/abs/2511.13892", "authors": ["Badhan Chandra Das", "Md Tasnim Jawad", "Md Jueal Mia", "M. Hadi Amini", "Yanzhao Wu"], "title": "Jailbreaking Large Vision Language Models in Intelligent Transportation Systems", "comment": null, "summary": "Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8d8a\u72f1\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u50cf\u6392\u7248\u64cd\u7eb5\u548c\u591a\u8f6e\u63d0\u793a\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u5c42\u54cd\u5e94\u8fc7\u6ee4\u9632\u5fa1\u6280\u672f\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u6781\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u96c6\u6210LVLMs\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e0e\u4ea4\u901a\u76f8\u5173\u7684\u6709\u5bb3\u67e5\u8be2\u6570\u636e\u96c6\uff1b\u5176\u6b21\u63d0\u51fa\u5229\u7528\u56fe\u50cf\u6392\u7248\u64cd\u7eb5\u548c\u591a\u8f6e\u63d0\u793a\u7684\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff1b\u6700\u540e\u8bbe\u8ba1\u591a\u5c42\u54cd\u5e94\u8fc7\u6ee4\u9632\u5fa1\u6280\u672f\u9632\u6b62\u6a21\u578b\u751f\u6210\u4e0d\u5f53\u54cd\u5e94\u3002", "result": "\u5728\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u6700\u5148\u8fdbLVLMs\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u4f7f\u7528GPT-4\u5224\u65ad\u548c\u4eba\u5de5\u9a8c\u8bc1\u8bc4\u4f30\u653b\u51fb\u548c\u9632\u5fa1\u6548\u679c\u3002\u4e0e\u73b0\u6709\u8d8a\u72f1\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u56fe\u50cf\u6392\u7248\u64cd\u7eb5\u548c\u591a\u8f6e\u63d0\u793a\u7684\u8d8a\u72f1\u653b\u51fb\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684LVLMs\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u591a\u5c42\u9632\u5fa1\u673a\u5236\u6765\u4fdd\u62a4\u7cfb\u7edf\u5b89\u5168\u3002"}}
{"id": "2511.13778", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13778", "abs": "https://arxiv.org/abs/2511.13778", "authors": ["Angelika Schwarz", "Anton Anders", "Cole Brower", "Harun Bayraktar", "John Gunnels", "Kate Clark", "RuQing G. Xu", "Samuel Rodriguez", "Sebastien Cayrols", "Pawe\u0142 Tabaszewski", "Victor Podlozhnyuk"], "title": "Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme", "comment": null, "summary": "The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.", "AI": {"tldr": "ADP\u662f\u4e00\u4e2a\u5b8c\u5168\u5728GPU\u4e0a\u8fd0\u884c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u52a8\u6001\u7cbe\u5ea6\u6280\u672f\uff0c\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u5355\u5143\uff08\u5982FP4\uff09\u6a21\u62df\u53cc\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\uff0c\u5728\u4fdd\u8bc1FP64\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3GPU\u786c\u4ef6\u8f6c\u5411\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff08FP16\u3001FP8\u3001FP4\uff09\uff0c\u53cc\u7cbe\u5ea6\u8ba1\u7b97\u541e\u5410\u91cf\u76f8\u5bf9\u8f83\u4f4e\u3002\u9700\u8981\u5229\u7528\u4f4e\u7cbe\u5ea6\u5355\u5143\u6a21\u62df\u53cc\u7cbe\u5ea6\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6307\u6570\u8de8\u5ea6\u5bb9\u91cf\uff08ESC\uff09\u786c\u4ef6\u65e0\u5173\u4f30\u8ba1\u5668\u786e\u5b9a\u5206\u89e3\u53c2\u6570\uff0c\u96c6\u6210\u5f02\u5e38\u5904\u7406\u3001\u8fd0\u884c\u65f6\u542f\u53d1\u5f0f\u548c\u539f\u751fFP64\u56de\u9000\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u65e0\u7b26\u53f7\u6574\u6570\u5207\u7247\u65b9\u6848\u63d0\u9ad8\u8868\u793a\u6548\u7387\u3002", "result": "\u572855\u4f4d\u5c3e\u6570\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u539f\u751fFP64 GEMM\uff0c\u5728NVIDIA Blackwell GB200\u548cRTX Pro 6000 Blackwell Server Edition\u4e0a\u5206\u522b\u5b9e\u73b02.3\u500d\u548c13.2\u500d\u52a0\u901f\uff0c\u8fd0\u884c\u65f6\u95f4\u5f00\u9500\u4f4e\u4e8e10%\u3002", "conclusion": "\u4f4e\u7cbe\u5ea6\u52a0\u901f\u5668\u53ef\u4ee5\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u3001\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5b9e\u7528\u751f\u4ea7\u5c31\u7eea\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301FP64\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.13942", "categories": ["cs.AI", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13942", "abs": "https://arxiv.org/abs/2511.13942", "authors": ["Daniel Weitekamp"], "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees", "comment": null, "summary": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $\u03b2$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.", "AI": {"tldr": "CORGI\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u5f0f\u5339\u914d\u7b97\u6cd5\uff0c\u9488\u5bf9\u89c4\u5219\u7cfb\u7edf\u4e2d\u6307\u6570\u7ea7\u590d\u6742\u5ea6\u7684\u5339\u914d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e8c\u6b21\u65f6\u95f4\u7a7a\u95f4\u4fdd\u8bc1\uff0c\u80fd\u591f\u8fed\u4ee3\u6d41\u5f0f\u8f93\u51fa\u5339\u914d\u7ed3\u679c\uff0c\u907f\u514d\u5185\u5b58\u6ea2\u51fa\u3002", "motivation": "\u89e3\u51b3\u5b9e\u65f6AI\u7cfb\u7edf\u548c\u6570\u636e\u5e93\u67e5\u8be2\u4e2d\u89c4\u5219\u5339\u914d\u7684\u6307\u6570\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53AI\u7cfb\u7edf\u81ea\u52a8\u751f\u6210\u89c4\u5219\u65f6\u5bb9\u6613\u4ea7\u751f\u6700\u574f\u60c5\u51b5\u5339\u914d\u6a21\u5f0f\uff0c\u5bfc\u81f4\u7a0b\u5e8f\u6267\u884c\u7f13\u6162\u6216\u5185\u5b58\u8017\u5c3d\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u524d\u5411\u4f20\u9012\u6784\u5efa/\u7ef4\u62a4\u57fa\u7840\u5173\u7cfb\u56fe\uff0c\u540e\u5411\u8fed\u4ee3\u5668\u6309\u9700\u751f\u6210\u5339\u914d\uff0c\u4e0d\u540c\u4e8eRETE\u7684\u03b2\u5185\u5b58\u6536\u96c6\u90e8\u5206\u5339\u914d\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u6027\u80fd\u8bc4\u4f30\u4e2d\uff0cCORGI\u5728\u7b80\u5355\u7ec4\u5408\u5339\u914d\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eSOAR\u548cOPS5\u7684RETE\u5b9e\u73b0\u3002", "conclusion": "CORGI\u7b97\u6cd5\u901a\u8fc7\u6d88\u9664\u4f20\u7edf\u51b2\u7a81\u96c6\u7684\u5185\u5b58\u8d1f\u62c5\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u4f7f\u81ea\u52a8\u751f\u6210\u7684\u89c4\u5219\u7cfb\u7edf\u66f4\u52a0\u5b9e\u7528\u3002"}}
{"id": "2511.13779", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.13779", "abs": "https://arxiv.org/abs/2511.13779", "authors": ["Mohammad Abdi", "Francesca Meneghello", "Francesco Restuccia"], "title": "Semantic Multiplexing", "comment": null, "summary": "Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\\times$, 25$\\times$, and 54$\\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u591a\u8def\u590d\u7528\u65b0\u6982\u5ff5\uff0c\u5c06\u591a\u8def\u590d\u7528\u4ece\u6bd4\u7279\u7ea7\u63d0\u5347\u5230\u4efb\u52a1\u7ea7\uff0c\u901a\u8fc7\u5408\u5e76\u591a\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u538b\u7f29\u8868\u793a\u6765\u521b\u5efa\u5355\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u4ece\u800c\u5728\u8bed\u4e49\u5c42\u6269\u5c55\u6709\u6548\u81ea\u7531\u5ea6\uff0c\u5b9e\u73b0\u5728\u4e0d\u589e\u52a0\u5929\u7ebf\u6216\u5e26\u5bbd\u7684\u60c5\u51b5\u4e0b\u540c\u65f6\u5904\u7406\u66f4\u591a\u4efb\u52a1\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u9700\u8981\u5728\u65e0\u7ebf\u8fb9\u7f18\u5e76\u884c\u6267\u884c\u591a\u4e2a\u8ba1\u7b97\u4efb\u52a1\uff0c\u73b0\u6709\u901a\u4fe1\u7cfb\u7edf\u4ec5\u652f\u6301\u6bd4\u7279\u7ea7\u5e76\u884c\u4f20\u8f93\uff0c\u8fd9\u4ece\u6839\u672c\u4e0a\u9650\u5236\u4e86\u53ef\u5e76\u53d1\u5904\u7406\u7684\u4efb\u52a1\u6570\u91cf\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u591a\u8def\u590d\u7528\u65b9\u6cd5\uff0c\u5c06\u591a\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u538b\u7f29\u8868\u793a\u5408\u5e76\u4e3a\u5355\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u5728Jetson Orin Nano\u548c\u6beb\u7c73\u6ce2\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\u5b9e\u9a8c\u5e73\u53f0\u4e0a\u8fdb\u884c\u539f\u578b\u5b9e\u73b0\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8bed\u4e49\u591a\u8def\u590d\u7528\u53ef\u5728\u8bed\u4e49\u5c42\u8054\u5408\u5904\u7406\u591a\u4e2a\u4efb\u52a1\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u4efb\u52a1\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u57284\u00d74\u4fe1\u9053\u4e0a\u5c06\u591a\u8def\u590d\u7528\u4efb\u52a1\u6570\u4ece2\u589e\u52a0\u52308\u65f6\uff0c\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52304%\u3002\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u964d\u4f4e8\u500d\uff0c\u80fd\u8017\u964d\u4f4e25\u500d\uff0c\u901a\u4fe1\u8d1f\u8f7d\u964d\u4f4e54\u500d\u3002", "conclusion": "\u8bed\u4e49\u591a\u8def\u590d\u7528\u901a\u8fc7\u5c06\u591a\u8def\u590d\u7528\u4ece\u6bd4\u7279\u7ea7\u8f6c\u79fb\u5230\u4efb\u52a1\u7ea7\uff0c\u6709\u6548\u6269\u5c55\u4e86\u8bed\u4e49\u5c42\u7684\u81ea\u7531\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e76\u884c\u4efb\u52a1\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.13970", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13970", "abs": "https://arxiv.org/abs/2511.13970", "authors": ["Sanjay Acharjee", "Abir Khan Ratul", "Diego Patino", "Md Nazmus Sakib"], "title": "Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios", "comment": null, "summary": "Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u56fe\u5f15\u5bfc\u7684\u751f\u6210AI\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790OSHA\u4e8b\u6545\u62a5\u544a\u751f\u6210\u903c\u771f\u7684\u5de5\u4f5c\u573a\u6240\u5371\u9669\u573a\u666f\u56fe\u50cf\uff0c\u5e76\u5f15\u5165VQA\u6846\u67b6\u8bc4\u4f30\u751f\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "motivation": "\u83b7\u53d6\u771f\u5b9e\u7684\u5de5\u4f5c\u573a\u6240\u5371\u9669\u573a\u666f\u56fe\u50cf\u6570\u636e\u96c6\u975e\u5e38\u56f0\u96be\uff0c\u56e0\u4e3a\u6355\u6349\u4e8b\u6545\u89e6\u53d1\u573a\u666f\u51e0\u4e4e\u4e0d\u53ef\u80fd\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u751f\u6210\u903c\u771f\u7684\u5371\u9669\u573a\u666f\u56fe\u50cf\u4ee5\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u3002", "method": "\u4f7f\u7528GPT-4o\u5206\u6790OSHA\u4e8b\u6545\u62a5\u544a\u63d0\u53d6\u7ed3\u6784\u5316\u5371\u9669\u63a8\u7406\uff0c\u8f6c\u6362\u4e3a\u5bf9\u8c61\u7ea7\u573a\u666f\u56fe\uff0c\u7136\u540e\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u6784\u56fe\u51c6\u786e\u7684\u5371\u9669\u573a\u666f\uff0c\u5e76\u5f15\u5165VQA\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684VQA\u56fe\u5206\u6570\u5728\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u4e2d\u4f18\u4e8eCLIP\u548cBLIP\u6307\u6807\uff0c\u57fa\u4e8e\u71b5\u9a8c\u8bc1\u786e\u8ba4\u5176\u5177\u6709\u66f4\u9ad8\u7684\u5224\u522b\u654f\u611f\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u903c\u771f\u7684\u5de5\u4f5c\u573a\u6240\u5371\u9669\u573a\u666f\u56fe\u50cf\uff0c\u4e3a\u8bad\u7ec3\u5b89\u5168\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13804", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13804", "abs": "https://arxiv.org/abs/2511.13804", "authors": ["Temitayo Adefemi"], "title": "Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication", "comment": "9 pages, 6 figures", "summary": "MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.", "AI": {"tldr": "MPI\u6d3e\u751f\u6570\u636e\u7c7b\u578b(DDTs)\u5728\u5b9e\u9645\u6027\u80fd\u8868\u73b0\u4e0a\u5b58\u5728\u4e89\u8bae\uff0c\u672c\u6587\u901a\u8fc7\u4e09\u4e2a2D\u5e94\u7528\u5bf9\u56db\u79cdMPI\u5b9e\u73b0\u8fdb\u884c\u8de8\u5b9e\u73b0\u8bc4\u4f30\uff0c\u53d1\u73b0DDTs\u6027\u80fd\u8868\u73b0\u4e0d\u4e00\uff0c\u6ca1\u6709\u4e00\u79cd\u7b56\u7565\u5728\u6240\u6709\u7a0b\u5e8f\u3001\u8bed\u4e49\u548cMPI\u6808\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "MPI\u6d3e\u751f\u6570\u636e\u7c7b\u578b\u627f\u8bfa\u7b80\u5316\u975e\u8fde\u7eed\u6570\u636e\u7684\u65e0\u62f7\u8d1d\u901a\u4fe1\uff0c\u4f46\u5176\u5b9e\u9645\u6027\u80fd\u4ecd\u5b58\u5728\u4e89\u8bae\uff0c\u4e14\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00MPI\u6808\u8fdb\u884c\u62a5\u544a\uff0c\u9700\u8981\u8fdb\u884c\u8de8\u5b9e\u73b0\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a2D\u5e94\u7528\uff1aJacobi CFD\u6c42\u89e3\u5668\u3001\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u548c\u57fa\u4e8e\u683c\u70b9\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u6bcf\u4e2a\u5e94\u7528\u5206\u522b\u5b9e\u73b0\u57fa\u7840\u7248\u672c\uff08\u624b\u52a8\u6253\u5305\uff09\u548cDDT\u7248\u672c\u3002\u57281-4\u4e2a\u8fdb\u7a0b\u4e0a\u8fd0\u884c\u5f3a\u6269\u5c55\u548c\u5f31\u6269\u5c55\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u56db\u79cdMPI\u5b9e\u73b0\uff1aMPICH\u3001Open MPI\u3001Intel MPI\u548cMVAPICH2\u3002", "result": "\u7ed3\u679c\u6df7\u5408\uff1aDDTs\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6700\u5feb\uff08\u5982\u56fe\u50cf\u91cd\u5efa\u5728Intel MPI\u548cMPICH\u4e0a\uff09\uff0c\u4f46\u5728\u5176\u4ed6\u6808\u4e0a\u53ef\u80fd\u6700\u6162\uff08\u5982Open MPI\u548cMVAPICH2\uff09\u3002CFD\u6c42\u89e3\u5668\u4e2d\u57fa\u7840\u7248\u672c\u901a\u5e38\u4f18\u4e8eDDTs\uff0c\u800c\u751f\u547d\u6e38\u620f\u4e2d\u6392\u540d\u56e0MPI\u5e93\u800c\u5f02\u3002\u8fd8\u89c2\u5bdf\u5230\u6808\u7279\u5b9a\u5f02\u5e38\uff0c\u5982MPICH\u5728DDT\u90bb\u57df\u548c\u6301\u4e45\u6a21\u5f0f\u4e0b\u7684\u51cf\u901f\u3002", "conclusion": "\u6ca1\u6709\u4e00\u79cd\u7b56\u7565\u5728\u6240\u6709\u7a0b\u5e8f\u3001\u8bed\u4e49\u548cMPI\u6808\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0cDDTs\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u65e0\u6cd5\u4fdd\u8bc1\u3002\u5efa\u8bae\u5728\u76ee\u6807MPI\u5b9e\u73b0\u548c\u901a\u4fe1\u6a21\u5f0f\u4e0b\u540c\u65f6\u5206\u6790DDT\u548c\u624b\u52a8\u6253\u5305\u8bbe\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13940", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13940", "abs": "https://arxiv.org/abs/2511.13940", "authors": ["Stuart H. Sul", "Simran Arora", "Benjamin F. Spector", "Christopher R\u00e9"], "title": "ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels", "comment": null, "summary": "Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \\times$ speedup for data- and tensor-parallel workloads, $4.08 \\times$ for sequence-parallel workloads, and $1.22 \\times$ for expert-parallel workloads.", "AI": {"tldr": "ParallelKittens (PK) \u662f\u4e00\u4e2a\u7b80\u5316\u7684 CUDA \u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u6838\u5fc3\u539f\u8bed\u548c\u7edf\u4e00\u7f16\u7a0b\u6a21\u677f\uff0c\u7cfb\u7edf\u6027\u5730\u6307\u5bfc\u6700\u4f18\u591a GPU \u5185\u6838\u8bbe\u8ba1\uff0c\u5728 Hopper \u548c Blackwell \u67b6\u6784\u4e0a\u663e\u8457\u63d0\u5347\u5404\u79cd\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740 AI \u6a21\u578b\u89c4\u6a21\u6269\u5927\u548c\u786c\u4ef6\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u5347\u8d85\u8fc7\u4e92\u8fde\u5e26\u5bbd\u6539\u8fdb\uff0cGPU \u95f4\u901a\u4fe1\u5df2\u6210\u4e3a\u73b0\u4ee3 AI \u5de5\u4f5c\u8d1f\u8f7d\u7684\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u7cfb\u7edf\u901a\u8fc7\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u6765\u7f13\u89e3\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u5728\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u548c\u65b0\u52a0\u901f\u5668\u4e0a\u8fbe\u5230\u7406\u8bba\u5cf0\u503c\u6027\u80fd\u3002", "method": "PK \u6269\u5c55 ThunderKittens \u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u6838\u5fc3\u539f\u8bed\u548c\u7edf\u4e00\u7f16\u7a0b\u6a21\u677f\u4f53\u73b0\u591a GPU \u5185\u6838\u8bbe\u8ba1\u539f\u5219\uff0c\u8fd9\u4e9b\u539f\u5219\u57fa\u4e8e\u5bf9\u6570\u636e\u4f20\u8f93\u673a\u5236\u3001\u8d44\u6e90\u8c03\u5ea6\u548c\u8bbe\u8ba1\u5f00\u9500\u7b49\u5f71\u54cd\u591a GPU \u6027\u80fd\u56e0\u7d20\u7684\u7efc\u5408\u5206\u6790\u3002", "result": "\u5728 Hopper \u548c Blackwell \u67b6\u6784\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u7528\u4e0d\u5230 50 \u884c\u8bbe\u5907\u4ee3\u7801\uff0cPK \u5b9e\u73b0\u4e86\uff1a\u6570\u636e\u5e76\u884c\u548c tensor \u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u6700\u9ad8 2.33 \u500d\u52a0\u901f\uff0c\u5e8f\u5217\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d 4.08 \u500d\u52a0\u901f\uff0c\u4e13\u5bb6\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d 1.22 \u500d\u52a0\u901f\u3002", "conclusion": "PK \u6846\u67b6\u8bc1\u660e\u4e86\u4e00\u5c0f\u7ec4\u7b80\u5355\u3001\u53ef\u91cd\u7528\u7684\u539f\u5219\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u6307\u5bfc\u6700\u4f18\u591a GPU \u5185\u6838\u8bbe\u8ba1\uff0c\u663e\u8457\u7b80\u5316\u4e86\u91cd\u53e0\u591a GPU \u5185\u6838\u7684\u5f00\u53d1\u8fc7\u7a0b\uff0c\u5e76\u5728\u591a\u79cd\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.14018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14018", "abs": "https://arxiv.org/abs/2511.14018", "authors": ["Minghu Wang", "Shuliang Zhao", "Yuanyuan Zhao", "Hongxia Xu"], "title": "ALEX:A Light Editing-knowledge Extractor", "comment": null, "summary": "The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.", "AI": {"tldr": "ALEX\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u77e5\u8bc6\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5185\u5b58\u67b6\u6784\u5c06\u77e5\u8bc6\u66f4\u65b0\u7ec4\u7ec7\u6210\u8bed\u4e49\u7c07\uff0c\u5c06\u68c0\u7d22\u590d\u6742\u5ea6\u4eceO(N)\u964d\u4f4e\u5230O(K+N/C)\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8df3\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u8def\u5f84\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u77e5\u8bc6\u662f\u9759\u6001\u7684\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u590d\u6742\u591a\u8df3\u95ee\u9898\u65f6\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u68c0\u7d22\u6548\u7387\u7684\u6311\u6218\u3002", "method": "ALEX\u91c7\u7528\u5206\u5c42\u5185\u5b58\u67b6\u6784\u7ec4\u7ec7\u77e5\u8bc6\u66f4\u65b0\uff0c\u5305\u542b\u63a8\u7406\u67e5\u8be2\u5408\u6210\u6a21\u5757\u5f25\u5408\u67e5\u8be2\u4e0e\u4e8b\u5b9e\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u4ee5\u53ca\u52a8\u6001\u8bc1\u636e\u88c1\u51b3\u5f15\u64ce\u6267\u884c\u9ad8\u6548\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728MQUAKE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cALEX\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u8df3\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u8def\u5f84\u53ef\u9760\u6027\uff0c\u540c\u65f6\u5c06\u6240\u9700\u641c\u7d22\u7a7a\u95f4\u51cf\u5c11\u4e8680%\u4ee5\u4e0a\u3002", "conclusion": "ALEX\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u548c\u51c6\u786e\u7684\u77e5\u8bc6\u7f16\u8f91\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2511.14116", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14116", "abs": "https://arxiv.org/abs/2511.14116", "authors": ["Ziyi Xu", "Zhiqiang Xie", "Swapnil Gandhi", "Christos Kozyrakis"], "title": "FailSafe: High-performance Resilient Serving", "comment": null, "summary": "Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.", "AI": {"tldr": "FailSafe\u662f\u4e00\u4e2a\u5bb9\u9519\u7684\u5f20\u91cf\u5e76\u884c\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5faa\u73afKVCache\u653e\u7f6e\u3001\u6df7\u5408\u6ce8\u610f\u529b\u548c\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u611f\u77e5\u8def\u7531\u7b49\u6280\u672f\uff0c\u5728GPU\u6545\u969c\u65f6\u7ef4\u6301\u9ad8\u6027\u80fdLLM\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u5e76\u884c(TP)\u5728GPU\u6545\u969c\u65f6\u4f1a\u5bfc\u81f4\u6267\u884c\u4e2d\u65ad\u3001\u6602\u8d35\u7684KVCache\u91cd\u65b0\u8ba1\u7b97\u4ee5\u53ca\u957f\u671f\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u4e0d\u5e73\u8861\uff0c\u7cfb\u7edf\u8106\u5f31\u6027\u9ad8\u3002", "method": "\u91c7\u7528\u5faa\u73afKVCache\u653e\u7f6e\u5b9e\u73b0\u5747\u5300\u5185\u5b58\u5229\u7528\uff0c\u6df7\u5408\u6ce8\u610f\u529b\u7ed3\u5408\u5f20\u91cf\u548c\u6570\u636e\u5e76\u884c\u6d88\u9664\u6ede\u540e\uff0c\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u611f\u77e5\u8def\u7531\u52a8\u6001\u5e73\u8861\u8bf7\u6c42\uff0c\u5e76\u91c7\u7528\u4e3b\u52a8KVCache\u5907\u4efd\u548c\u6309\u9700\u6743\u91cd\u6062\u590d\u3002", "result": "\u57288xH100 DGX\u7cfb\u7edf\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6\u6545\u969c\u5904\u7406\u65b9\u6cd5\uff0cFailSafe\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u541e\u5410\u91cf\u548c\u4e24\u4e2a\u6570\u91cf\u7ea7\u66f4\u4f4e\u7684\u6062\u590d\u5ef6\u8fdf\uff0c\u5373\u4f7f\u6700\u591a\u4e09\u4e2aGPU\u6545\u969c\u4e5f\u80fd\u7ef4\u6301\u9ad8\u541e\u5410\u91cf\u548c\u5e73\u8861\u5229\u7528\u7387\u3002", "conclusion": "FailSafe\u5c55\u793a\u4e86\u5728\u52a8\u6001\u4e0d\u53ef\u9760\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a33\u5065\u9ad8\u6548LLM\u670d\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2511.14023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14023", "abs": "https://arxiv.org/abs/2511.14023", "authors": ["Chiharu Hagiwara", "Naoki Nonaka", "Yuhta Hashimoto", "Ryu Uchimido", "Jun Seita"], "title": "Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation", "comment": "Introducing an open dataset", "summary": "Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.", "AI": {"tldr": "Syn-STARTS\u662f\u4e00\u4e2a\u4f7f\u7528LLM\u751f\u6210\u5927\u89c4\u6a21\u4f24\u5458\u4e8b\u6545\u5206\u7c7b\u6848\u4f8b\u7684\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728\u5f00\u53d1\u9ad8\u6027\u80fd\u533b\u7597AI\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5927\u89c4\u6a21\u4f24\u5458\u4e8b\u6545\u4e2d\u5206\u7c7b\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u771f\u5b9e\u6570\u636e\u96be\u4ee5\u6536\u96c6\uff0c\u9700\u8981\u5f00\u53d1\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u6765\u652f\u6301AI\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1Syn-STARTS\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u5206\u7c7b\u6848\u4f8b\uff0c\u5e76\u4e0e\u4eba\u5de5\u6574\u7406\u7684TRIAGE\u5f00\u653e\u6570\u636e\u96c6\u8fdb\u884c\u5b9a\u6027\u6bd4\u8f83\uff0c\u8bc4\u4f30LLM\u5728\u6807\u51c6START\u5206\u7c7b\u65b9\u6cd5\u4e0b\u7684\u51c6\u786e\u6027\u3002", "result": "Syn-STARTS\u751f\u6210\u7684\u5206\u7c7b\u6848\u4f8b\u5728\u8d28\u91cf\u4e0a\u4e0e\u4eba\u5de5\u6574\u7406\u7684\u6570\u636e\u96c6\u96be\u4ee5\u533a\u5206\uff0cLLM\u5728\u7eff\u3001\u9ec4\u3001\u7ea2\u3001\u9ed1\u56db\u4e2a\u5206\u7c7b\u7c7b\u522b\u4e2d\u7684\u51c6\u786e\u6027\u8868\u73b0\u9ad8\u5ea6\u7a33\u5b9a\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u5728\u5f00\u53d1\u4e25\u91cd\u548c\u5173\u952e\u533b\u7597\u573a\u666f\u7684\u9ad8\u6027\u80fdAI\u6a21\u578b\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4e3a\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.14124", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14124", "abs": "https://arxiv.org/abs/2511.14124", "authors": ["Sabiha Afroz", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Jingoo Han", "Ali R. Butt"], "title": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training", "comment": "This paper accepted for presentation to the 16th ACM Symposium on Cloud Computing (SOCC'25)", "summary": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.\n  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.", "AI": {"tldr": "10Cache\u662f\u4e00\u4e2a\u8d44\u6e90\u611f\u77e5\u7684\u5f20\u91cf\u7f13\u5b58\u548c\u8fc1\u79fb\u7cfb\u7edf\uff0c\u901a\u8fc7\u667a\u80fd\u534f\u8c03GPU\u3001CPU\u548cNVMe\u4e4b\u95f4\u7684\u5185\u5b58\u4f7f\u7528\u6765\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53ef\u8fbe\u52302\u500d\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u73b0\u6709GPU\u5185\u5b58\u5378\u8f7d\u65b9\u6cd5\u5b58\u5728\u9ad8\u5f20\u91cf\u8fc1\u79fb\u5ef6\u8fdf\u548c\u8bbe\u5907\u5185\u5b58\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u589e\u52a0\u548c\u4e91\u6210\u672c\u4e0a\u5347\u3002", "method": "10Cache\u901a\u8fc7\u5206\u6790\u5f20\u91cf\u6267\u884c\u987a\u5e8f\u6784\u5efa\u9884\u53d6\u7b56\u7565\uff0c\u6839\u636e\u5f20\u91cf\u5927\u5c0f\u5206\u5e03\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d\u5206\u914d\u5185\u5b58\u7f13\u51b2\u533a\uff0c\u5e76\u91cd\u7528\u5185\u5b58\u7f13\u51b2\u533a\u4ee5\u6700\u5c0f\u5316\u5206\u914d\u5f00\u9500\u3002", "result": "\u5728\u591a\u6837\u5316LLM\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5378\u8f7d\u65b9\u6cd5\uff0c10Cache\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\uff0cGPU\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u9ad886.6\u500d\uff0cCPU\u548cGPU\u5185\u5b58\u5229\u7528\u7387\u5206\u522b\u63d0\u9ad82.15\u500d\u548c1.33\u500d\u3002", "conclusion": "10Cache\u662f\u4f18\u5316\u4e91\u7aefLLM\u8bad\u7ec3\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.14052", "categories": ["cs.AI", "cs.CE", "stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.14052", "abs": "https://arxiv.org/abs/2511.14052", "authors": ["Amirreza Mehrabi", "Jason W. Morphew", "Breejha Quezada", "N. Sanjay Rebello"], "title": "Making Evidence Actionable in Adaptive Learning", "comment": null, "summary": "Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6559\u5e08\u4e3b\u5bfc\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u53cd\u9988\u5faa\u73af\uff0c\u5c06\u6982\u5ff5\u7ea7\u8bc4\u4f30\u8bc1\u636e\u8f6c\u5316\u4e3a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u5fae\u5e72\u9884\u3002\u7b97\u6cd5\u5305\u542b\u5145\u5206\u6027\u3001\u6ce8\u610f\u529b\u548c\u591a\u6837\u6027\u4e09\u4e2a\u4fdd\u969c\u673a\u5236\uff0c\u901a\u8fc7\u6574\u6570\u89c4\u5212\u65b9\u6cd5\u5b9e\u73b0\u5e72\u9884\u5206\u914d\uff0c\u5728\u6a21\u62df\u548c\u5b9e\u9645\u7269\u7406\u8bfe\u7a0b\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u8bca\u65ad\u7cbe\u786e\u4f46\u5e72\u9884\u8584\u5f31\uff0c\u5bfc\u81f4\u5e2e\u52a9\u65f6\u673a\u4e0d\u5f53\u6216\u5185\u5bb9\u4e0d\u5339\u914d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5c06\u8bca\u65ad\u8bc1\u636e\u6709\u6548\u8f6c\u5316\u4e3a\u9488\u5bf9\u6027\u6559\u5b66\u5e72\u9884\u7684\u7cfb\u7edf\u3002", "method": "\u5c06\u5e72\u9884\u5206\u914d\u5efa\u6a21\u4e3a\u5e26\u7ea6\u675f\u7684\u4e8c\u8fdb\u5236\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u5305\u542b\u8986\u76d6\u5ea6\u3001\u65f6\u95f4\u3001\u96be\u5ea6\u7a97\u53e3\u3001\u5148\u51b3\u6761\u4ef6\u548c\u53cd\u5197\u4f59\u7b49\u7ea6\u675f\u3002\u91c7\u7528\u8d2a\u5fc3\u9009\u62e9\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u677e\u5f1b\u65b9\u6cd5\u548c\u6df7\u5408\u65b9\u6cd5\u4e09\u79cd\u6c42\u89e3\u7b56\u7565\u3002", "result": "\u57281204\u540d\u5b66\u751f\u7684\u7269\u7406\u8bfe\u7a0b\u90e8\u7f72\u4e2d\uff0c\u4e24\u79cd\u6c42\u89e3\u5668\u90fd\u80fd\u5728\u6709\u9650\u89c2\u770b\u65f6\u95f4\u5185\u4e3a\u51e0\u4e4e\u6240\u6709\u5b66\u4e60\u8005\u5b9e\u73b0\u5b8c\u6574\u7684\u6280\u80fd\u8986\u76d6\u3002\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u6bd4\u8d2a\u5fc3\u65b9\u6cd5\u51cf\u5c11\u7ea612%\u7684\u5197\u4f59\u8986\u76d6\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u96be\u5ea6\u6c34\u5e73\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u534f\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u8ffd\u8e2a\u548c\u53ef\u5ba1\u8ba1\u7684\u63a7\u5236\u5668\uff0c\u95ed\u5408\u4e86\u8bca\u65ad-\u6559\u5b66\u5faa\u73af\uff0c\u5728\u8bfe\u5802\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u516c\u5e73\u4e14\u8d1f\u8f7d\u611f\u77e5\u7684\u4e2a\u6027\u5316\u6559\u5b66\u3002"}}
{"id": "2511.14456", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14456", "abs": "https://arxiv.org/abs/2511.14456", "authors": ["Fabian Stricker", "David Bermbach", "Christian Zirpins"], "title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning", "comment": "Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)", "summary": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.\n  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u4e0e\u8005\u6545\u969c\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u6545\u969c\u65f6\u673a\u3001\u6570\u636e\u5206\u5e03\u4ee5\u53ca\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u9ad8\u5ea6\u6570\u636e\u503e\u659c\u60c5\u51b5\u4e0b\u8bc4\u4f30\u7ed3\u679c\u8fc7\u4e8e\u4e50\u89c2\uff0c\u6545\u969c\u65f6\u673a\u4e5f\u4f1a\u5f71\u54cd\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8de8\u7ec4\u7ec7\u534f\u4f5c\u573a\u666f\u4e2d\u9700\u8981\u4fdd\u8bc1\u7cfb\u7edf\u53ef\u9760\u6027\uff0c\u4f46\u53c2\u4e0e\u8005\u53ef\u80fd\u56e0\u5404\u79cd\u539f\u56e0\u6545\u969c\u3002\u867d\u7136\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u4e2d\u5df2\u6709\u76f8\u5173\u7814\u7a76\uff0c\u4f46\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53c2\u4e0e\u8005\u6545\u969c\u5f71\u54cd\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7814\u7a76\u5206\u6790\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u4e0e\u8005\u6545\u969c\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u6545\u969c\u65f6\u673a\u3001\u6570\u636e\u5206\u5e03\u4ee5\u53ca\u8bc4\u4f30\u7ed3\u679c\u7b49\u5173\u952e\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u5ea6\u6570\u636e\u503e\u659c\u60c5\u51b5\u4e0b\uff0c\u8bc4\u4f30\u7ed3\u679c\u8fc7\u4e8e\u4e50\u89c2\uff0c\u63a9\u76d6\u4e86\u771f\u5b9e\u5f71\u54cd\uff1b\u540c\u65f6\u6545\u969c\u65f6\u673a\u5bf9\u8bad\u7ec3\u6a21\u578b\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u7814\u7a76\u4eba\u5458\u548c\u8f6f\u4ef6\u67b6\u6784\u5e08\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5728\u8de8\u7ec4\u7ec7\u8054\u90a6\u5b66\u4e60\u4e2d\u8003\u8651\u53c2\u4e0e\u8005\u6545\u969c\u5f71\u54cd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.14608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14608", "abs": "https://arxiv.org/abs/2511.14608", "authors": ["Dave Dice", "Alex Kogan"], "title": "Hapax Locks : Value-Based Mutual Exclusion", "comment": null, "summary": "We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.", "AI": {"tldr": "Hapax Locks\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9501\u7b97\u6cd5\uff0c\u5177\u6709\u7b80\u5355\u6027\u3001\u6052\u5b9a\u65f6\u95f4\u5230\u8fbe\u548c\u89e3\u9501\u8def\u5f84\u3001FIFO\u51c6\u5165\u987a\u5e8f\u3001\u7a7a\u95f4\u6548\u7387\u9ad8\u3001\u5728\u4e89\u7528\u60c5\u51b5\u4e0b\u4ea7\u751f\u8f83\u5c11\u4e00\u81f4\u6027\u6d41\u91cf\u7b49\u7279\u70b9\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u9501\u7b97\u6cd5\u76f8\u5f53\uff0c\u4f46\u5bf9\u8fd0\u884c\u65f6\u73af\u5883\u7ea6\u675f\u66f4\u5c11\u3001\u66f4\u5bb9\u6613\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u7684\u9501\u7b97\u6cd5\u3002", "method": "\u63d0\u51faHapax Locks\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u7ebf\u7a0b\u95f4\u4e0d\u8f6c\u79fb\u6307\u9488\u6240\u6709\u6743\uff0c\u63d0\u4f9b\u6052\u5b9a\u65f6\u95f4\u7684\u5230\u8fbe\u548c\u89e3\u9501\u64cd\u4f5c\u3002", "result": "Hapax Locks\u5728\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u9501\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u73af\u5883\u7684\u4f9d\u8d56\u548c\u7ea6\u675f\u3002", "conclusion": "Hapax Locks\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u9501\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u5408\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u6216\u73b0\u6709API\u4e0b\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2511.14101", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14101", "abs": "https://arxiv.org/abs/2511.14101", "authors": ["Xinpeng Chen", "Xiaofeng Han", "Kaihao Zhang", "Guochao Ren", "Yujie Wang", "Wenhao Cao", "Yang Zhou", "Jianfeng Lu", "Zhenbo Song"], "title": "APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design", "comment": null, "summary": "Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.", "AI": {"tldr": "APD-agents\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u79fb\u52a8\u5e94\u7528\u9875\u9762\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5c06\u7528\u6237\u63cf\u8ff0\u8f6c\u5316\u4e3a\u9875\u9762\u5e03\u5c40\u8bbe\u8ba1\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u9875\u9762\u5e03\u5c40\u8bbe\u8ba1\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u6280\u80fd\uff0c\u73b0\u6709\u8bbe\u8ba1\u8f6f\u4ef6\u9700\u8981\u5927\u91cf\u57f9\u8bad\uff0c\u8de8\u9875\u9762\u534f\u4f5c\u8bbe\u8ba1\u8fd8\u9700\u8981\u989d\u5916\u65f6\u95f4\u7edf\u4e00\u6807\u51c6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u7f16\u6392\u667a\u80fd\u4f53\u3001\u8bed\u4e49\u89e3\u6790\u667a\u80fd\u4f53\u3001\u4e3b\u5e03\u5c40\u667a\u80fd\u4f53\u3001\u6a21\u677f\u68c0\u7d22\u667a\u80fd\u4f53\u548c\u9012\u5f52\u7ec4\u4ef6\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u5c06\u7528\u6237\u63cf\u8ff0\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5e76\u751f\u6210\u9875\u9762\u5e03\u5c40\u3002", "result": "\u5728RICO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPD-agents\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5145\u5206\u5229\u7528\u4e86\u5927\u6a21\u578b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u52a8\u534f\u4f5c\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u9875\u9762\u8bbe\u8ba1\u3002"}}
{"id": "2511.14617", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14617", "abs": "https://arxiv.org/abs/2511.14617", "authors": ["Ruoyu Qin", "Weiran He", "Weixiao Huang", "Yangkun Zhang", "Yikai Zhao", "Bo Pang", "Xinran Xu", "Yingdi Shan", "Yongwei Wu", "Mingxing Zhang"], "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables", "summary": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.", "AI": {"tldr": "Seer\u7cfb\u7edf\u901a\u8fc7\u5229\u7528\u76f8\u540c\u63d0\u793a\u8bf7\u6c42\u95f4\u7684\u8f93\u51fa\u957f\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2drollout\u9636\u6bb5\u7684\u957f\u5c3e\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u540c\u6b65\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728rollout\u9636\u6bb5\u9762\u4e34\u4e25\u91cd\u6027\u80fd\u74f6\u9888\uff0c\u5305\u62ec\u957f\u5c3e\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u7531\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u5bfc\u81f4\u3002", "method": "Seer\u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u5206\u5272rollout\u5b9e\u73b0\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\u3001\u81ea\u9002\u5e94\u5206\u7ec4\u63a8\u6d4b\u89e3\u7801\uff0c\u5229\u7528\u76f8\u540c\u63d0\u793a\u8bf7\u6c42\u95f4\u7684\u76f8\u4f3c\u6027\u6765\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u751f\u4ea7\u7ea7RL\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u540c\u6b65RL\u7cfb\u7edf\uff0cSeer\u5c06\u7aef\u5230\u7aefrollout\u541e\u5410\u91cf\u63d0\u9ad8\u4e8674%\u523097%\uff0c\u957f\u5c3e\u5ef6\u8fdf\u964d\u4f4e\u4e8675%\u523093%\u3002", "conclusion": "Seer\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u5728\u7ebf\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fed\u4ee3\uff0c\u6709\u6548\u89e3\u51b3\u4e86rollout\u9636\u6bb5\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.14131", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14131", "abs": "https://arxiv.org/abs/2511.14131", "authors": ["Yu Zhong", "Zihao Zhang", "Rui Zhang", "Lingdong Huang", "Haihan Gao", "Shuo Wang", "Da Li", "Ruijian Han", "Jiaming Guo", "Shaohui Peng", "Di Huang", "Yunji Chen"], "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation", "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86R3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8fc7\u7a0b\u601d\u7ef4\u5c06LLMs\u7684\u6cdb\u5316\u80fd\u529b\u4e0eVLN\u4e13\u5bb6\u77e5\u8bc6\u7ed3\u5408\uff0c\u5305\u542bRunner\u3001Ruminator\u548cRegulator\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5728REVERIE\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VLN\u4efb\u52a1\u4e2dLLMs\u5728\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u7ed3\u5408LLMs\u7684\u5e38\u8bc6\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u8fc7\u7a0b\u601d\u7ef4\u6846\u67b6\uff0cRunner\u662f\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\u8d1f\u8d23\u5e38\u89c4\u5bfc\u822a\uff0cRuminator\u4f7f\u7528\u591a\u6a21\u6001LLM\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0cRegulator\u6839\u636e\u4e09\u4e2a\u6807\u51c6\u76d1\u63a7\u5bfc\u822a\u8fdb\u5ea6\u5e76\u63a7\u5236\u601d\u7ef4\u6a21\u5f0f\u3002", "result": "\u5728REVERIE\u57fa\u51c6\u4e0aSPL\u548cRGSPL\u5206\u522b\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u65b9\u6cd53.28%\u548c3.30%\uff0c\u663e\u8457\u63d0\u5347\u4e86VLN\u4efb\u52a1\u7684\u5b8c\u6210\u6027\u80fd\u3002", "conclusion": "R3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLN\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408LLMs\u6cdb\u5316\u80fd\u529b\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2511.14214", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14214", "abs": "https://arxiv.org/abs/2511.14214", "authors": ["Pattaraphon Kenny Wongchamcharoen", "Paul Glasserman"], "title": "Do Large Language Models (LLMs) Understand Chronology?", "comment": "47 pages", "summary": "Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.", "AI": {"tldr": "\u672c\u6587\u6d4b\u8bd5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u7ecf\u6d4e\u5b66\u5e94\u7528\u4e2d\u5bf9\u65f6\u95f4\u987a\u5e8f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u65f6\u5e8f\u6392\u5e8f\u3001\u6761\u4ef6\u6392\u5e8f\u548c\u65f6\u4ee3\u9519\u8bef\u68c0\u6d4b\u7b49\u4efb\u52a1\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4fdd\u6301\u5c40\u90e8\u987a\u5e8f\u7684\u540c\u65f6\u96be\u4ee5\u7ef4\u6301\u5168\u5c40\u4e00\u81f4\u7684\u65f6\u95f4\u7ebf\uff0c\u800c\u589e\u52a0\u63a8\u7406\u9884\u7b97\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u548c\u7ecf\u6d4e\u5b66\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u65f6\u95f4\u987a\u5e8f\uff0c\u4ee5\u907f\u514d\u524d\u77bb\u6027\u504f\u5dee\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u590d\u6742\u5ea6\u9012\u589e\u7684\u65f6\u95f4\u987a\u5e8f\u4efb\u52a1\uff0c\u5305\u62ec\u65f6\u5e8f\u6392\u5e8f\u3001\u6761\u4ef6\u6392\u5e8f\uff08\u5148\u8fc7\u6ee4\u540e\u6392\u5e8f\uff09\u548c\u65f6\u4ee3\u9519\u8bef\u68c0\u6d4b\uff0c\u8bc4\u4f30\u4e86GPT-4.1\u3001Claude-3.7 Sonnet\uff08\u6709/\u65e0\u6269\u5c55\u601d\u7ef4\uff09\u548cGPT-5\u5728\u4e0d\u540c\u63a8\u7406\u5f3a\u5ea6\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u7cbe\u786e\u5339\u914d\u7387\u6025\u5267\u4e0b\u964d\uff0c\u4f46\u7b49\u7ea7\u76f8\u5173\u6027\u4fdd\u6301\u8f83\u9ad8\uff1b\u6761\u4ef6\u6392\u5e8f\u4e2d\u5927\u591a\u6570\u5931\u8d25\u6e90\u4e8e\u8fc7\u6ee4\u6b65\u9aa4\u800c\u975e\u6392\u5e8f\u6b65\u9aa4\uff1b\u65f6\u4ee3\u9519\u8bef\u68c0\u6d4b\u662f\u6700\u7b80\u5355\u7684\u4efb\u52a1\uff0c\u4f46\u6027\u80fd\u4ecd\u968f\u65f6\u95f4\u7ebf\u6216\u5b9e\u4f53\u91cd\u53e0\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "\u5206\u914d\u660e\u786e\u7684\u63a8\u7406\u9884\u7b97\u6709\u52a9\u4e8e\u65f6\u95f4\u987a\u5e8f\u4efb\u52a1\uff0cGPT-5\u5728\u4e2d\u7b49/\u9ad8\u63a8\u7406\u5f3a\u5ea6\u4e0b\u80fd\u5728\u6240\u6709\u957f\u5ea6\u4e0a\u5b9e\u73b0\u5b8c\u7f8e\u6392\u5e8f\u548c\u6761\u4ef6\u6392\u5e8f\uff0c\u800c\u4f4e\u63a8\u7406\u5f3a\u5ea6\u4f1a\u968f\u5217\u8868\u53d8\u957f\u800c\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3aLLMs\u5728\u91d1\u878d\u9886\u57df\u7684\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.14219", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.14219", "abs": "https://arxiv.org/abs/2511.14219", "authors": ["Kumud Tripathi", "Aditya Srinivas Menon", "Aman Gaurav", "Raj Prakash Gohil", "Pankaj Wasnik"], "title": "Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation", "comment": "Accepted at AAAI 2026 - Main Technical Track", "summary": "The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u67b6\u6784\u6765\u51cf\u5c11Whisper\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u5e7b\u89c9\u9519\u8bef\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u5c42\u6ce8\u610f\u529b\u589e\u5f3a\u7f16\u7801\u5668\u9c81\u68d2\u6027\u548c\u591a\u76ee\u6807\u77e5\u8bc6\u84b8\u998f\u6291\u5236\u5e7b\u89c9\u3002", "motivation": "Whisper\u6a21\u578b\u5728\u566a\u58f0\u58f0\u5b66\u6761\u4ef6\u4e0b\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\u9519\u8bef\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u97f3\u9891\u9884\u5904\u7406\u6216\u8f6c\u5f55\u540e\u5904\u7406\uff0c\u5bf9\u6a21\u578b\u672c\u8eab\u7684\u4fee\u6539\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u81ea\u9002\u5e94\u5c42\u6ce8\u610f\u529b\u5c06\u7f16\u7801\u5668\u5c42\u5206\u7ec4\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u5757\uff0c\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u878d\u5408\u5757\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u591a\u76ee\u6807\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5728\u566a\u58f0\u97f3\u9891\u4e0a\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u4ee5\u5bf9\u9f50\u5176\u8bed\u4e49\u548c\u6ce8\u610f\u529b\u5206\u5e03\u3002", "result": "\u5728\u566a\u58f0\u8bed\u97f3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u8bcd\u9519\u8bef\u7387\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u8bed\u97f3\u4e0a\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "conclusion": "\u81ea\u9002\u5e94\u5c42\u6ce8\u610f\u529b\u548c\u77e5\u8bc6\u84b8\u998f\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u6761\u4ef6\u4e0b\u63d0\u9ad8Whisper\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7b56\u7565\u3002"}}
{"id": "2511.14227", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14227", "abs": "https://arxiv.org/abs/2511.14227", "authors": ["Yuxiang Wang", "Siwen Wang", "Haowei Han", "Ao Wang", "Boya Liu", "Yong Zhao", "Chengbo Wu", "Bin Zhu", "Bin Qin", "Xiaokai Zhou", "Xiao Yan", "Jiawei Jiang", "Bo Du"], "title": "DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home", "comment": null, "summary": "Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.", "AI": {"tldr": "DevPiolt\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7269\u8054\u7f51\u8bbe\u5907\u64cd\u4f5c\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u591a\u4efb\u52a1\u5fae\u8c03\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u5e76\u5728\u5c0f\u7c73\u5bb6\u5ead\u5e94\u7528\u4e2d\u6210\u529f\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u6a21\u578b\u5728\u5904\u7406\u7269\u8054\u7f51\u8bbe\u5907\u64cd\u4f5c\u65f6\u9762\u4e34\u590d\u6742\u64cd\u4f5c\u903b\u8f91\u3001\u591a\u6837\u5316\u7528\u6237\u504f\u597d\u548c\u5bf9\u6b21\u4f18\u5efa\u8bae\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u3002", "method": "1. \u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u4e3aLLM\u88c5\u5907\u7269\u8054\u7f51\u64cd\u4f5c\u9886\u57df\u77e5\u8bc6\uff1b2. \u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u4f7f\u5fae\u8c03\u540e\u7684LLM\u4e0e\u7279\u5b9a\u7528\u6237\u504f\u597d\u5bf9\u9f50\uff1b3. \u8bbe\u8ba1\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u66dd\u5149\u63a7\u5236\u673a\u5236\u907f\u514d\u4f4e\u8d28\u91cf\u63a8\u8350\u5e26\u6765\u7684\u8d1f\u9762\u7528\u6237\u4f53\u9a8c\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cDevPiolt\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6240\u6709\u6307\u6807\u5e73\u5747\u63d0\u534769.5%\u3002\u5728\u7ebf\u90e8\u7f72\u7ed3\u679c\u663e\u793a\uff0c\u72ec\u7acb\u8bbf\u5ba2\u8bbe\u5907\u8986\u76d6\u7387\u589e\u52a021.6%\uff0c\u9875\u9762\u6d4f\u89c8\u63a5\u53d7\u7387\u589e\u52a029.1%\u3002", "conclusion": "DevPiolt\u6210\u529f\u89e3\u51b3\u4e86\u7269\u8054\u7f51\u8bbe\u5907\u64cd\u4f5c\u63a8\u8350\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.14248", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14248", "abs": "https://arxiv.org/abs/2511.14248", "authors": ["Hongju Lee", "Youngjun Park", "Jisun An", "Dongman Lee"], "title": "Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility", "comment": "Accepted at ASONAM 2025", "summary": "The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u533a\u57df\u5c42\u9762\u7684Airbnb\u5173\u952e\u6307\u6807\uff08\u6536\u5165\u3001\u9884\u8ba2\u5929\u6570\u3001\u9884\u8ba2\u6570\u91cf\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u623f\u6e90\u7279\u5f81\u548c\u5916\u90e8\u4e0a\u4e0b\u6587\u56e0\u7d20\u6784\u5efa\u533a\u57df\u8868\u793a\uff0c\u4f7f\u7528LLM\u751f\u6210\u533a\u57df\u5d4c\u5165\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8fdb\u884c1-3\u4e2a\u6708\u7684\u9884\u6d4b\u3002", "motivation": "\u77ed\u671f\u79df\u8d41\u5e73\u53f0\uff08\u5982Airbnb\uff09\u7684\u6269\u5f20\u4e25\u91cd\u6270\u4e71\u4e86\u5f53\u5730\u4f4f\u623f\u5e02\u573a\uff0c\u5bfc\u81f4\u79df\u91d1\u4e0a\u6da8\u548c\u4f4f\u623f\u8d1f\u62c5\u80fd\u529b\u95ee\u9898\u3002\u51c6\u786e\u9884\u6d4b\u533a\u57dfAirbnb\u5e02\u573a\u8d8b\u52bf\u53ef\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u57ce\u5e02\u89c4\u5212\u8005\u63d0\u4f9b\u5173\u952e\u89c1\u89e3\u4ee5\u51cf\u8f7b\u8fd9\u4e9b\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\uff0c\u5c06\u7ed3\u6784\u5316\u8868\u683c\u6570\u636e\u8f6c\u6362\u4e3a\u57fa\u4e8e\u63d0\u793a\u7684LLM\u8f93\u5165\uff0c\u751f\u6210\u5168\u9762\u7684\u533a\u57df\u5d4c\u5165\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5d4c\u5165\u8f93\u5165\u5230\u5148\u8fdb\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08RNN\u3001LSTM\u3001Transformer\uff09\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5728\u9996\u5c14Airbnb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u57fa\u7ebf\uff08\u5305\u62ec\u4f20\u7edf\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff09\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u5e73\u5747RMSE\u548cMAE\u964d\u4f4e\u4e86\u7ea648%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u68c0\u6d4b\u4f9b\u5e94\u8fc7\u5269\u533a\u57df\u548c\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u57ce\u5e02\u653f\u7b56\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2511.14256", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.14256", "abs": "https://arxiv.org/abs/2511.14256", "authors": ["Yu Liu", "Xixun Lin", "Yanmin Shang", "Yangxi Li", "Shi Wang", "Yanan Cao"], "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models", "comment": "AAAI 2026, Long Paper, Oral", "summary": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.", "AI": {"tldr": "PathMind\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5f15\u5bfcLLM\u4f7f\u7528\u91cd\u8981\u63a8\u7406\u8def\u5f84\u6765\u589e\u5f3a\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002\u5b83\u91c7\u7528\"\u68c0\u7d22-\u4f18\u5148\u5316-\u63a8\u7406\"\u8303\u5f0f\uff0c\u5305\u542b\u8def\u5f84\u4f18\u5148\u5316\u673a\u5236\u548c\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM-based\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u65e0\u5dee\u522b\u63d0\u53d6\u63a8\u7406\u8def\u5f84\u53ef\u80fd\u5f15\u5165\u65e0\u5173\u566a\u58f0\u8bef\u5bfcLLM\uff1b2) \u52a8\u6001\u63a2\u7d22\u63a8\u7406\u8def\u5f84\u9700\u8981\u9ad8\u68c0\u7d22\u9700\u6c42\u548c\u9891\u7e41LLM\u8c03\u7528\u3002", "method": "PathMind\u6846\u67b6\u9075\u5faa\"\u68c0\u7d22-\u4f18\u5148\u5316-\u63a8\u7406\"\u8303\u5f0f\uff1a1) \u68c0\u7d22\u6a21\u5757\u4eceKG\u4e2d\u63d0\u53d6\u67e5\u8be2\u5b50\u56fe\uff1b2) \u8def\u5f84\u4f18\u5148\u5316\u673a\u5236\u4f7f\u7528\u8bed\u4e49\u611f\u77e5\u8def\u5f84\u4f18\u5148\u7ea7\u51fd\u6570\u8bc6\u522b\u91cd\u8981\u63a8\u7406\u8def\u5f84\uff1b3) \u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u548c\u8def\u5f84\u504f\u597d\u5bf9\u9f50\u7684\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u751f\u6210\u51c6\u786e\u54cd\u5e94\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPathMind\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u63a8\u7406\u8def\u5f84\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u8f93\u5165token\u5b9e\u73b0\u66f4\u597d\u6027\u80fd\u3002", "conclusion": "PathMind\u901a\u8fc7\u9009\u62e9\u6027\u5f15\u5bfcLLM\u4f7f\u7528\u91cd\u8981\u63a8\u7406\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5fe0\u5b9e\u3001\u53ef\u89e3\u91ca\u548c\u9ad8\u6548\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2511.14334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14334", "abs": "https://arxiv.org/abs/2511.14334", "authors": ["Alessio Pellegrino", "Jacopo Mauro"], "title": "When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling", "comment": null, "summary": "One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u548c\u6270\u52a8\u7ecf\u5178CSPLib\u95ee\u9898\u6765\u6d4b\u8bd5LLMs\u7684\u6a21\u578b\u751f\u6210\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136LLMs\u80fd\u751f\u6210\u8bed\u6cd5\u6709\u6548\u4e14\u8bed\u4e49\u5408\u7406\u7684\u6a21\u578b\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u548c\u8bed\u8a00\u53d8\u5316\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u8868\u660e\u5176\u7406\u89e3\u6d45\u5c42\u4e14\u5bf9\u63aa\u8f9e\u654f\u611f\u3002", "motivation": "\u68c0\u9a8cLLMs\u81ea\u52a8\u751f\u6210\u4f18\u5316\u548c\u7ea6\u675f\u7f16\u7a0b\u6a21\u578b\u7684\u80fd\u529b\u662f\u5426\u6e90\u4e8e\u6570\u636e\u6c61\u67d3\u800c\u975e\u771f\u6b63\u63a8\u7406\uff0c\u56e0\u4e3a\u8bb8\u591a\u6807\u51c6CP\u95ee\u9898\u53ef\u80fd\u5df2\u5305\u542b\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u91cd\u65b0\u8868\u8ff0\u548c\u6270\u52a8\u4e00\u7ec4\u77e5\u540dCSPLib\u95ee\u9898\uff0c\u4fdd\u6301\u7ed3\u6784\u4e0d\u53d8\u4f46\u4fee\u6539\u4e0a\u4e0b\u6587\u5e76\u5f15\u5165\u8bef\u5bfc\u5143\u7d20\uff0c\u7136\u540e\u6bd4\u8f83\u4e09\u4e2a\u4ee3\u8868\u6027LLMs\u5728\u539f\u59cb\u548c\u4fee\u6539\u63cf\u8ff0\u4e0b\u751f\u6210\u7684\u6a21\u578b\u3002", "result": "LLMs\u80fd\u751f\u6210\u8bed\u6cd5\u6709\u6548\u4e14\u8bed\u4e49\u5408\u7406\u7684\u6a21\u578b\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u548c\u8bed\u8a00\u53d8\u5316\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u663e\u793a\u51fa\u6d45\u5c42\u7406\u89e3\u548c\u5bf9\u63aa\u8f9e\u7684\u654f\u611f\u6027\u3002", "conclusion": "LLMs\u5728\u81ea\u52a8\u751f\u6210\u4f18\u5316\u6a21\u578b\u65b9\u9762\u7684\u6210\u529f\u53ef\u80fd\u66f4\u591a\u6e90\u4e8e\u6570\u636e\u6c61\u67d3\u800c\u975e\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5176\u6027\u80fd\u5bf9\u95ee\u9898\u8868\u8ff0\u7684\u53d8\u5316\u975e\u5e38\u654f\u611f\u3002"}}
{"id": "2511.14476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14476", "abs": "https://arxiv.org/abs/2511.14476", "authors": ["Dalia Ali", "Dora Zhao", "Allison Koenecke", "Orestis Papakyriakopoulos"], "title": "Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior", "comment": null, "summary": "Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728LLM\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8003\u8651\u591a\u5143\u793e\u4f1a\u4ef7\u503c\u89c2\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\u548c\u6280\u672f\u8bbe\u8ba1\u9009\u62e9\u90fd\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u5f53\u524dLLM\u7684\u5bf9\u9f50\u51b3\u7b56\u5f80\u5f80\u5ffd\u89c6\u4eba\u7c7b\u793e\u4f1a\u7684\u591a\u6837\u6027\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u5c06\u591a\u5143\u4ef7\u503c\u89c2\u7eb3\u5165\u5bf9\u9f50\u6d41\u7a0b\u3002", "method": "\u6536\u96c6\u7f8e\u56fd\u548c\u5fb7\u56fd\u53c2\u4e0e\u8005\u7684\u5bf9\u9f50\u6570\u636e\uff0c\u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u8bc4\u4f30LLM\u54cd\u5e94\uff0c\u5e76\u4f7f\u7528\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u7684\u504f\u597d\u5fae\u8c03\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u63a8\u7406\u6a21\u578b\uff0c\u540c\u65f6\u6539\u53d8\u8bc4\u5206\u5c3a\u5ea6\u3001\u5206\u6b67\u5904\u7406\u65b9\u6cd5\u548c\u4f18\u5316\u6280\u672f\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u6027\u4eba\u53e3\u7edf\u8ba1\u6548\u5e94\uff1a\u7537\u6027\u53c2\u4e0e\u8005\u5bf9\u6bd2\u6027\u7684\u8bc4\u5206\u6bd4\u5973\u6027\u4f4e18%\uff1b\u4fdd\u5b88\u6d3e\u548c\u9ed1\u4eba\u53c2\u4e0e\u8005\u5bf9\u60c5\u611f\u610f\u8bc6\u7684\u8bc4\u5206\u5206\u522b\u6bd4\u81ea\u7531\u6d3e\u548c\u767d\u4eba\u53c2\u4e0e\u8005\u9ad827.9%\u548c44%\u3002\u6280\u672f\u8bbe\u8ba1\u9009\u62e9\u4e5f\u663e\u793a\u5f3a\u70c8\u5f71\u54cd\uff1a\u4fdd\u7559\u8bc4\u5206\u8005\u5206\u6b67\u6bd4\u591a\u6570\u6295\u7968\u5b9e\u73b0\u7ea653%\u66f4\u5927\u7684\u6bd2\u6027\u51cf\u5c11\uff1b5\u70b9\u91cf\u8868\u6bd4\u4e8c\u5143\u683c\u5f0f\u4ea7\u751f\u7ea622%\u66f4\u591a\u51cf\u5c11\uff1bDPO\u5728\u591a\u503c\u4f18\u5316\u4e2d\u6301\u7eed\u4f18\u4e8eGRPO\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u56de\u7b54\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u521d\u6b65\u6b65\u9aa4\uff1a\u5bf9\u9f50\u5e94\u5982\u4f55\u5e73\u8861\u4e13\u5bb6\u9a71\u52a8\u548c\u7528\u6237\u9a71\u52a8\u7684\u4fe1\u53f7\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u548c\u516c\u5e73\u4ee3\u8868\u6027\u3002"}}
{"id": "2511.14595", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.14595", "abs": "https://arxiv.org/abs/2511.14595", "authors": ["Yuan An", "Ruhma Hashmi", "Michelle Rogers", "Jane Greenberg", "Brian K. Smith"], "title": "Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport", "comment": "Accepted in the 5th Workshop on Knowledge Graphs and Big Data in Conjunction with IEEE Big Data 2025", "summary": "Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7387\u5931\u771f\u7406\u8bba\u548c\u6700\u4f18\u4f20\u8f93\u51e0\u4f55\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7FGW\u8026\u5408\u91cf\u5316\u8bed\u4e49\u5931\u771f\uff0c\u4f7f\u7528\u7cbe\u70bc\u64cd\u4f5c\u6700\u5c0f\u5316\u7387\u5931\u771f\u62c9\u683c\u6717\u65e5\u91cf\uff0c\u751f\u6210\u7d27\u51d1\u4e14\u4fdd\u7559\u4fe1\u606f\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "motivation": "\u89e3\u51b3\u5c06\u975e\u7ed3\u6784\u5316\u6559\u80b2\u6750\u6599\u8f6c\u6362\u4e3a\u6355\u6349\u5173\u952e\u6559\u5b66\u5185\u5bb9\u7684\u77e5\u8bc6\u56fe\u8c31\u7684\u56f0\u96be\uff0c\u4e3aAI\u8f85\u52a9\u6559\u80b2\u4e2d\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u8bb2\u5ea7\u5185\u5bb9\u5efa\u6a21\u4e3a\u5ea6\u91cf-\u6d4b\u5ea6\u7a7a\u95f4\uff0c\u4f7f\u7528Fused Gromov-Wasserstein\u8026\u5408\u5bf9\u9f50\u5019\u9009\u77e5\u8bc6\u56fe\u8c31\u4ee5\u91cf\u5316\u8bed\u4e49\u5931\u771f\uff0c\u901a\u8fc7\u6dfb\u52a0\u3001\u5408\u5e76\u3001\u62c6\u5206\u3001\u79fb\u9664\u548c\u91cd\u8fde\u7b49\u7cbe\u70bc\u64cd\u4f5c\u6700\u5c0f\u5316\u7387\u5931\u771f\u62c9\u683c\u6717\u65e5\u91cf\u3002", "result": "\u5728\u6570\u636e\u79d1\u5b66\u8bb2\u5ea7\u4e0a\u7684\u539f\u578b\u5e94\u7528\u663e\u793a\uff0c\u4ece\u4f18\u5316\u540e\u7684\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u7684\u591a\u9879\u9009\u62e9\u9898\u572815\u4e2a\u8d28\u91cf\u6807\u51c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u539f\u59cb\u7b14\u8bb0\u751f\u6210\u7684\u95ee\u9898\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e2a\u6027\u5316AI\u8f85\u52a9\u6559\u80b2\u4e2d\u7684\u4fe1\u606f\u8bba\u77e5\u8bc6\u56fe\u8c31\u4f18\u5316\u5efa\u7acb\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
