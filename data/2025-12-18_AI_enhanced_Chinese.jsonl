{"id": "2512.15306", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15306", "abs": "https://arxiv.org/abs/2512.15306", "authors": ["Erik Schultheis", "Dan Alistarh"], "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs", "comment": null, "summary": "We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.", "AI": {"tldr": "LLMQ\u662f\u4e00\u4e2a\u9488\u5bf9\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff083B-32B\u53c2\u6570\uff09\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8bad\u7ec3\u7684\u7aef\u5230\u7aefCUDA/C++\u5b9e\u73b0\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u548c\u901a\u4fe1\u74f6\u9888\uff0c\u80fd\u5728\u5355\u5f2016GB\u663e\u5361\u4e0a\u8bad\u7ec37B\u6a21\u578b\uff0c\u6216\u57284\u5f20RTX 4090\u4e0a\u8bad\u7ec332B\u6a21\u578b\u3002", "motivation": "\u9488\u5bf9\u6d88\u8d39\u7ea7GPU\u5185\u5b58\u6709\u9650\u3001\u901a\u4fe1\u901f\u5ea6\u6162\u7684\u7279\u70b9\uff0c\u4e3a\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u666e\u901a\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u80fd\u591f\u5728\u76f8\u5bf9\u5ec9\u4ef7\u7684\u786c\u4ef6\u4e0a\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u6fc0\u6d3b\u68c0\u67e5\u70b9\u3001\u5378\u8f7d\u6280\u672f\u548c\u57fa\u4e8e\u590d\u5236\u5f15\u64ce\u7684\u96c6\u5408\u901a\u4fe1\u7b49\u4f18\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u6807\u51c6\u76848\u4f4d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4e0d\u4f9d\u8d56\u989d\u5916\u7684\u7b97\u6cd5\u8fd1\u4f3c\uff0c\u4fdd\u6301\u7ea650%\u7684FLOP\u5229\u7528\u7387\u3002", "result": "\u80fd\u591f\u5728\u5355\u5f2016GB\u4e2d\u7aef\u6e38\u620f\u663e\u5361\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c037B\u6a21\u578b\uff0c\u5728\u914d\u59074\u5f20RTX 4090\u7684\u5de5\u4f5c\u7ad9\u4e0a\u8bad\u7ec332B\u6a21\u578b\uff0c\u6548\u7387\u53ef\u4e0e\u5728\u66f4\u6602\u8d35\u7684\u4e91\u7ea7GPU\u4e0a\u8fd0\u884c\u7684\u751f\u4ea7\u7ea7\u7cfb\u7edf\u76f8\u5ab2\u7f8e\u3002", "conclusion": "LLMQ\u5c55\u793a\u4e86\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8bad\u7ec3\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u5b9e\u60e0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u95e8\u69db\u3002"}}
{"id": "2512.15595", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15595", "abs": "https://arxiv.org/abs/2512.15595", "authors": ["Daniel J\u00fcnger", "Kevin Kristensen", "Yunsong Wang", "Xiangyao Yu", "Bertil Schmidt"], "title": "Optimizing Bloom Filters for Modern GPU Architectures", "comment": "13 pages, 12 figures", "summary": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86GPU\u4e0aBloom\u8fc7\u6ee4\u5668\u7684\u4f18\u5316\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u3001\u7ebf\u7a0b\u534f\u4f5c\u548c\u8ba1\u7b97\u5ef6\u8fdf\u4e09\u4e2a\u7ef4\u5ea6\u7684\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\uff0c\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "Bloom\u8fc7\u6ee4\u5668\u662f\u8fd1\u4f3c\u6210\u5458\u67e5\u8be2\u7684\u57fa\u7840\u6570\u636e\u7ed3\u6784\uff0cGPU\u56e0\u5176\u5927\u89c4\u6a21\u7ebf\u7a0b\u7ea7\u5e76\u884c\u6027\u548c\u9ad8\u5e26\u5bbd\u5185\u5b58\uff0c\u662f\u52a0\u901fBloom\u8fc7\u6ee4\u5668\u7684\u7406\u60f3\u5e73\u53f0\u3002\u5c3d\u7ba1CPU\u4f18\u5316\u5b9e\u73b0\u5df2\u6709\u6df1\u5165\u7814\u7a76\uff0c\u4f46GPU\u8bbe\u8ba1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u63a2\u7d22GPU\u4e0a\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff1a\u5411\u91cf\u5316\u3001\u7ebf\u7a0b\u534f\u4f5c\u548c\u8ba1\u7b97\u5ef6\u8fdf\u3002\u901a\u8fc7\u5206\u6790\u786c\u4ef6\u5bf9\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u7684\u54cd\u5e94\uff0c\u5e76\u5c06\u8fd9\u4e9b\u89c2\u5bdf\u4e0e\u6027\u80fd\u8d8b\u52bf\u5173\u8054\uff0c\u5f00\u53d1\u4e86\u4f18\u5316\u7684GPU\u5b9e\u73b0\u3002", "result": "\u4f18\u5316\u8bbe\u8ba1\u7a81\u7834\u4e86\u4f20\u7edf\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u914d\u7f6e\u4f18\u8d8a\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u901a\u5e38\u4ec5\u9650\u4e8e\u9ad8\u8bef\u5dee\u53d8\u4f53\u7684\u541e\u5410\u91cf\u3002\u5728\u76f8\u540c\u9519\u8bef\u7387\u4e0b\uff0c\u6279\u91cf\u8fc7\u6ee4\u5668\u67e5\u627e\u6027\u80fd\u63d0\u534711.35\u500d\uff0c\u6784\u5efa\u6027\u80fd\u63d0\u534715.4\u500d\uff0c\u5728B200 GPU\u4e0a\u8fbe\u5230\u5b9e\u7528\u901f\u5ea6\u6781\u9650\u768492%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86GPU\u4e0aBloom\u8fc7\u6ee4\u5668\u4f18\u5316\u8bbe\u8ba1\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u7684CUDA/C++\u5b9e\u73b0\uff0c\u5e76\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2512.15398", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.15398", "abs": "https://arxiv.org/abs/2512.15398", "authors": ["Zanxiang He", "Meng Li", "Liyun Shi", "Weiye Daia", "Liming Nie"], "title": "Mapis: A Knowledge-Graph Grounded Multi-Agent Framework for Evidence-Based PCOS Diagnosis", "comment": null, "summary": "Polycystic Ovary Syndrome (PCOS) constitutes a significant public health issue affecting 10% of reproductive-aged women, highlighting the critical importance of developing effective diagnostic tools. Previous machine learning and deep learning detection tools are constrained by their reliance on large-scale labeled data and an lack of interpretability. Although multi-agent systems have demonstrated robust capabilities, the potential of such systems for PCOS detection remains largely unexplored. Existing medical multi-agent frameworks are predominantly designed for general medical tasks, suffering from insufficient domain integration and a lack of specific domain knowledge. To address these challenges, we propose Mapis, the first knowledge-grounded multi-agent framework explicitly designed for guideline-based PCOS diagnosis. Specifically, it built upon the 2023 International Guideline into a structured collaborative workflow that simulates the clinical diagnostic process. It decouples complex diagnostic tasks across specialized agents: a gynecological endocrine agent and a radiology agent collaborative to verify inclusion criteria, while an exclusion agent strictly rules out other causes. Furthermore, we construct a comprehensive PCOS knowledge graph to ensure verifiable, evidence-based decision-making. Extensive experiments on public benchmarks and specialized clinical datasets, benchmarking against nine diverse baselines, demonstrate that Mapis significantly outperforms competitive methods. On the clinical dataset, it surpasses traditional machine learning models by 13.56%, single-agent by 6.55%, and previous medical multi-agent systems by 7.05% in Accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mapis\uff0c\u9996\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u57fa\u4e8e\u6307\u5357\u7684\u591a\u56ca\u5375\u5de2\u7efc\u5408\u5f81\u8bca\u65ad\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "PCOS\u5f71\u54cd10%\u80b2\u9f84\u5973\u6027\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u533b\u7597\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9PCOS\u8bca\u65ad\u7684\u6846\u67b6", "method": "\u5c062023\u56fd\u9645\u6307\u5357\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u534f\u4f5c\u5de5\u4f5c\u6d41\uff0c\u6a21\u62df\u4e34\u5e8a\u8bca\u65ad\u8fc7\u7a0b\uff0c\u5305\u542b\u5987\u79d1\u5185\u5206\u6ccc\u667a\u80fd\u4f53\u3001\u653e\u5c04\u5b66\u667a\u80fd\u4f53\u534f\u4f5c\u9a8c\u8bc1\u7eb3\u5165\u6807\u51c6\uff0c\u6392\u9664\u667a\u80fd\u4f53\u4e25\u683c\u6392\u9664\u5176\u4ed6\u75c5\u56e0\uff0c\u5e76\u6784\u5efaPCOS\u77e5\u8bc6\u56fe\u8c31\u786e\u4fdd\u57fa\u4e8e\u8bc1\u636e\u7684\u51b3\u7b56", "result": "\u5728\u516c\u5171\u57fa\u51c6\u548c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0cMapis\u663e\u8457\u4f18\u4e8e9\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u6bd4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad813.56%\uff0c\u6bd4\u5355\u667a\u80fd\u4f53\u63d0\u9ad86.55%\uff0c\u6bd4\u5148\u524d\u533b\u7597\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u9ad87.05%", "conclusion": "Mapis\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u6307\u5357\u7684PCOS\u8bca\u65ad\u77e5\u8bc6\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u548c\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u8bca\u65ad"}}
{"id": "2512.14709", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14709", "abs": "https://arxiv.org/abs/2512.14709", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning", "comment": "12 pages with references. Submitted to 'Logical and Symbolic Reasoning in Language Models @ AAAI 2026' conference and is under review", "summary": "Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring \"VSA-likeness\" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6b8b\u5dee\u6d41\u89e3\u91ca\u4e3a\u8fd1\u4f3c\u7684\u5411\u91cf\u7b26\u53f7\u67b6\u6784(VSA)\uff0c\u7528\u4ee3\u6570\u89c6\u89d2\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u53ca\u5176\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51faVSA\u542f\u53d1\u7684\u67b6\u6784\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "Transformer\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u7c7b\u4f3c\u63a8\u7406\u7684\u884c\u4e3a\uff0c\u4f46\u5728\u9700\u8981\u7a33\u5b9a\u7b26\u53f7\u64cd\u4f5c\u7684\u4efb\u52a1\u4e0a\u4ecd\u7136\u8106\u5f31\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u7684\u89c6\u89d2\u7edf\u4e00\u7406\u89e3\u8fd9\u4e9b\u73b0\u8c61\uff0c\u89e3\u91ca\u6a21\u578b\u5185\u90e8\u673a\u5236\u4e0e\u63a8\u7406\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "method": "\u5c06\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u91ca\u4e3aVSA\uff1a\u67e5\u8be2\u548c\u952e\u5b9a\u4e49\u89d2\u8272\u7a7a\u95f4\uff0c\u503c\u7f16\u7801\u586b\u5145\u7269\uff0c\u6ce8\u610f\u529b\u6743\u91cd\u6267\u884c\u8f6f\u89e3\u7ed1\uff0c\u6b8b\u5dee\u8fde\u63a5\u5b9e\u73b0\u591a\u4e2a\u7ed1\u5b9a\u7ed3\u6784\u7684\u53e0\u52a0\u3002\u57fa\u4e8e\u6b64\u89c6\u89d2\u5206\u6790\u601d\u7ef4\u94fe\u3001\u7a0b\u5e8f\u63a8\u7406\u548c\u8bb0\u5fc6\u589e\u5f3a\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u63d0\u51faVSA\u542f\u53d1\u7684\u67b6\u6784\u504f\u7f6e\uff08\u5982\u663e\u5f0f\u7ed1\u5b9a/\u89e3\u7ed1\u5934\u3001\u8d85\u7ef4\u8bb0\u5fc6\u5c42\uff09\u548c\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5efa\u7acb\u4e86Transformer\u5185\u90e8\u673a\u5236\u4e0e\u5411\u91cf\u7b26\u53f7\u8ba1\u7b97\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u89e3\u91ca\u4e86\u53d8\u91cf\u6df7\u6dc6\u548c\u903b\u8f91\u76f8\u5173\u63d0\u793a\u4e0d\u4e00\u81f4\u7b49\u7279\u5f81\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u4fc3\u8fdb\u89d2\u8272-\u586b\u5145\u7269\u5206\u79bb\u548c\u9c81\u68d2\u53e0\u52a0\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "conclusion": "\u5c06\u6ce8\u610f\u529b\u89c6\u4e3a\u8f6f\u5411\u91cf\u7b26\u53f7\u8ba1\u7b97\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u548c\u903b\u8f91\u53ef\u9760\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u8861\u91cf\"VSA\u76f8\u4f3c\u6027\"\u548c\u903b\u8f91\u7ec4\u5408\u6027\u7684\u5ea6\u91cf\u6807\u51c6\u4ee5\u53ca\u7406\u8bba\u548c\u67b6\u6784\u4e0a\u7684\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2512.15705", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15705", "abs": "https://arxiv.org/abs/2512.15705", "authors": ["Xuting Liu", "Daniel Alexander", "Siva Kesava Reddy Kakarla", "Behnaz Arzani", "Vincent Liu"], "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX", "comment": null, "summary": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.", "AI": {"tldr": "DREX\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u91cd\u6279\u5904\u7406\u6280\u672f\u4f18\u5316\u65e9\u671f\u9000\u51faLLM\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u541e\u5410\u91cf2-12%\uff0c\u5b8c\u5168\u6d88\u9664\u975e\u81ea\u613f\u9000\u51fa", "motivation": "\u4f20\u7edf\u6279\u5904\u7406\u6846\u67b6\u4e0d\u9002\u5408\u65e9\u671f\u9000\u51faLLM\u67b6\u6784\uff0c\u56e0\u4e3a\u6279\u5904\u7406\u4e2d\u6240\u6709\u8bf7\u6c42\u53ef\u80fd\u4e0d\u4f1a\u540c\u65f6\u51c6\u5907\u597d\u9000\u51fa\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u5f3a\u5236\u7edf\u4e00\u51b3\u7b56\u800c\u9519\u8fc7\u65e9\u671f\u9000\u51fa\u673a\u4f1a\uff0c\u8981\u4e48\u901a\u8fc7\u5f3a\u5236\u63d0\u524d\u9000\u51fa\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf", "method": "\u63d0\u51fa\u52a8\u6001\u91cd\u6279\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff1a\u5728\u65e9\u671f\u9000\u51fa\u70b9\u52a8\u6001\u91cd\u7ec4\u6279\u6b21\u3002\u6ee1\u8db3\u9000\u51fa\u6761\u4ef6\u7684\u8bf7\u6c42\u7acb\u5373\u5904\u7406\uff0c\u7ee7\u7eed\u7684\u8bf7\u6c42\u4fdd\u7559\u5728\u7f13\u51b2\u533a\uff0c\u91cd\u65b0\u5206\u7ec4\u4e3a\u65b0\u6279\u6b21\u5e76\u8f6c\u53d1\u5230\u66f4\u6df1\u5c42\u3002\u5b9e\u73b0DREX\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u4f18\u5316\uff1a1) \u65e0\u590d\u5236\u91cd\u6279\u5904\u7406\u7f13\u51b2\u533a\u907f\u514d\u7269\u7406\u6570\u636e\u79fb\u52a8\uff1b2) EE\u548cSLA\u611f\u77e5\u8c03\u5ea6\u5668\u5206\u6790\u9884\u6d4b\u91cd\u6279\u64cd\u4f5c\u662f\u5426\u6709\u5229\u3002\u540c\u65f6\u9ad8\u6548\u5904\u7406\u8df3\u8fc7\u5c42\u7684\u7f3a\u5931KV\u7f13\u5b58", "result": "\u8bc4\u4f30\u663e\u793aDREX\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u541e\u5410\u91cf2-12%\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002\u5173\u952e\u7684\u662f\uff0cDREX\u5b8c\u5168\u6d88\u9664\u4e86\u975e\u81ea\u613f\u9000\u51fa\uff0c\u4e3a\u4fdd\u6301EE\u6a21\u578b\u9884\u671f\u7684\u8f93\u51fa\u8d28\u91cf\u63d0\u4f9b\u4e86\u5173\u952e\u4fdd\u8bc1", "conclusion": "DREX\u901a\u8fc7\u52a8\u6001\u91cd\u6279\u5904\u7406\u6709\u6548\u89e3\u51b3\u4e86\u65e9\u671f\u9000\u51faLLM\u7684\u6279\u5904\u7406\u6311\u6218\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.14766", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14766", "abs": "https://arxiv.org/abs/2512.14766", "authors": ["Dongzhuoran Zhou", "Yuqicheng Zhu", "Xiaxia Wang", "Hongkuan Zhou", "Jiaoyan Chen", "Steffen Staab", "Yuan He", "Evgeny Kharlamov"], "title": "GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge", "comment": null, "summary": "Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u56fe\u63a8\u7406\u667a\u80fd\u4f53\uff08GR-Agent\uff09\u6765\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08KGQA\uff09\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5047\u8bbe\u77e5\u8bc6\u56fe\u8c31\u662f\u5b8c\u6574\u7684\uff0c\u8fd9\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e2d\u77e5\u8bc6\u56fe\u8c31\u5f80\u5f80\u4e0d\u5b8c\u6574\u7684\u4e8b\u5b9e\u3002\u5f53\u76f4\u63a5\u652f\u6301\u7684\u4e09\u5143\u7ec4\u7f3a\u5931\u65f6\uff0c\u9700\u8981\u4ece\u73b0\u6709\u4e8b\u5b9e\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002", "method": "1. \u63d0\u51fa\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u6027\u57fa\u51c6\u7684\u65b9\u6cd5\u8bba\uff1a\u79fb\u9664\u76f4\u63a5\u652f\u6301\u7684\u4e09\u5143\u7ec4\uff0c\u540c\u65f6\u786e\u4fdd\u4fdd\u7559\u63a8\u65ad\u7b54\u6848\u6240\u9700\u7684\u66ff\u4ee3\u63a8\u7406\u8def\u5f84\u30022. \u5f00\u53d1\u81ea\u9002\u5e94\u56fe\u63a8\u7406\u667a\u80fd\u4f53\uff08GR-Agent\uff09\uff1a\u4ece\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4ea4\u4e92\u73af\u5883\uff0c\u5c06KGQA\u5f62\u5f0f\u5316\u4e3a\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\uff1b\u4f7f\u7528\u5305\u542b\u56fe\u63a8\u7406\u5de5\u5177\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7ef4\u62a4\u6f5c\u5728\u652f\u6301\u63a8\u7406\u8bc1\u636e\u7684\u8bb0\u5fc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1. \u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u5b8c\u6574\u6027\u60c5\u51b5\u4e0b\u6027\u80fd\u6301\u7eed\u4e0b\u964d\uff0c\u7a81\u663e\u5176\u63a8\u7406\u80fd\u529b\u6709\u9650\u30022. GR-Agent\u5728\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u975e\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0e\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u6027\u662fKGQA\u8bc4\u4f30\u7684\u91cd\u8981\u73b0\u5b9e\u56e0\u7d20\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e0b\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u63d0\u51fa\u7684GR-Agent\u901a\u8fc7\u667a\u80fd\u4f53\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3aKGQA\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2512.14792", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.14792", "abs": "https://arxiv.org/abs/2512.14792", "authors": ["Roman Nekrasov", "Stefano Fossati", "Indika Kumara", "Damian Andrew Tamburri", "Willem-Jan van den Heuvel"], "title": "IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection", "comment": "Submitted to ACM", "summary": "Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a \"Correctness-Congruence Gap\" where LLMs can become proficient \"coders\" but remain limited \"architects\" in fulfilling nuanced user intent.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u6784\u5316\u914d\u7f6e\u77e5\u8bc6\u6ce8\u5165\uff0c\u5c06LLM\u751f\u6210Terraform\u4ee3\u7801\u7684\u6574\u4f53\u6210\u529f\u7387\u4ece27.1%\u63d0\u5347\u81f362.6%\uff0c\u4f46\u53d1\u73b0\u610f\u56fe\u5bf9\u9f50\u5b58\u5728\u74f6\u9888\uff0c\u63ed\u793a\u4e86\"\u6b63\u786e\u6027-\u4e00\u81f4\u6027\u9e3f\u6c9f\"\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6b63\u786e\u4e14\u610f\u56fe\u5bf9\u9f50\u7684\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u65b9\u9762\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u7279\u522b\u662f\u5728Terraform\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1. \u589e\u5f3aIaC-Eval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u52a0\u5165\u4e91\u4eff\u771f\u548c\u81ea\u52a8\u9519\u8bef\u5206\u6790\uff1b2. \u5f00\u53d1LLM\u8f85\u52a9IaC\u4ee3\u7801\u751f\u6210\u7684\u9519\u8bef\u5206\u7c7b\u6cd5\uff1b3. \u5b9e\u65bd\u4ece\u6734\u7d20\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5230\u56feRAG\u7684\u77e5\u8bc6\u6ce8\u5165\u6280\u672f\uff0c\u5305\u62ec\u56fe\u7ec4\u4ef6\u7684\u8bed\u4e49\u4e30\u5bcc\u5316\u548c\u8d44\u6e90\u95f4\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u57fa\u7ebfLLM\u6027\u80fd\u8f83\u5dee\uff08\u6574\u4f53\u6210\u529f\u738727.1%\uff09\uff0c\u6ce8\u5165\u7ed3\u6784\u5316\u914d\u7f6e\u77e5\u8bc6\u540e\uff0c\u6280\u672f\u9a8c\u8bc1\u6210\u529f\u7387\u63d0\u5347\u81f375.3%\uff0c\u6574\u4f53\u6210\u529f\u7387\u63d0\u5347\u81f362.6%\u3002\u4f46\u610f\u56fe\u5bf9\u9f50\u51fa\u73b0\u5e73\u53f0\u671f\u3002", "conclusion": "\u867d\u7136\u7ed3\u6784\u5316\u77e5\u8bc6\u6ce8\u5165\u663e\u8457\u63d0\u9ad8\u4e86\u6280\u672f\u6b63\u786e\u6027\uff0c\u4f46\u610f\u56fe\u5bf9\u9f50\u5b58\u5728\u74f6\u9888\uff0c\u63ed\u793a\u4e86\"\u6b63\u786e\u6027-\u4e00\u81f4\u6027\u9e3f\u6c9f\"\uff1aLLM\u53ef\u4ee5\u6210\u4e3a\u719f\u7ec3\u7684\"\u7f16\u7801\u8005\"\uff0c\u4f46\u5728\u6ee1\u8db3\u7ec6\u5fae\u7528\u6237\u610f\u56fe\u65b9\u9762\u4ecd\u53d7\u9650\u4e8e\u4f5c\u4e3a\"\u67b6\u6784\u5e08\"\u7684\u80fd\u529b\u3002"}}
{"id": "2512.15044", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.15044", "abs": "https://arxiv.org/abs/2512.15044", "authors": ["Wenwen Xie", "Geng Sun", "Ruichen Zhang", "Xuejie Liu", "Yinqiu Liu", "Jiacheng Wang", "Dusit Niyato", "Ping Zhang"], "title": "Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study", "comment": null, "summary": "Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53AI\u7684ISAC\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5728\u4f18\u5316ISAC\u6027\u80fd\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u73af\u5883\u65e5\u76ca\u52a8\u6001\u590d\u6742\uff0cISAC\u7cfb\u7edf\u9700\u8981\u66f4\u667a\u80fd\u7684\u5904\u7406\u548c\u66f4\u81ea\u4e3b\u7684\u64cd\u4f5c\u6765\u4fdd\u6301\u6548\u7387\u548c\u9002\u5e94\u6027\u3002\u667a\u80fd\u4f53AI\u901a\u8fc7\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6301\u7eed\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u5faa\u73af\uff0c\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u5168\u9762\u56de\u987e\u4e86\u667a\u80fd\u4f53AI\u548cISAC\u7cfb\u7edf\u7684\u5173\u952e\u7279\u6027\uff1b\u5176\u6b21\u5c55\u793a\u4e86ISAC\u7cfb\u7edf\u7684\u51e0\u79cd\u5e38\u89c1\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u7a81\u51fa\u4e86\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u667a\u80fd\u4f53AI\u7684\u663e\u8457\u4f18\u52bf\uff1b\u7b2c\u4e09\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u667a\u80fd\u4f53ISAC\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "result": "\u63d0\u51fa\u7684\u667a\u80fd\u4f53ISAC\u6846\u67b6\u5728\u4f18\u5316ISAC\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u4e3aISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u73b0\u667a\u80fd\u3001\u81ea\u4e3b\u548c\u9ad8\u6548\u64cd\u4f5c\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u660e\u786e\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53AI\u7684ISAC\u7cfb\u7edf\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.15089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15089", "abs": "https://arxiv.org/abs/2512.15089", "authors": ["Jinwu Hu", "Dongjin Yang", "Langyu Bian", "Zhiquan Wen", "Yufeng Wang", "Yaofo Chen", "Bin Xiao", "Yuanqing Li", "Mingkui Tan"], "title": "Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models", "comment": "under review", "summary": "Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.", "AI": {"tldr": "CogER\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u5206\u5c42\u63a8\u7406\u542f\u53d1\u7684\u5f39\u6027\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u6bcf\u4e2a\u67e5\u8be2\u7684\u63a8\u7406\u7b56\u7565\u6765\u5e73\u8861LLM\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56LLM\u81ea\u8eab\u7684\u5feb\u901f\u6216\u6162\u901f\u6a21\u5f0f\uff08\u5982o1\u601d\u7ef4\uff09\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u96be\u5ea6\u67e5\u8be2\u95f4\u5e73\u8861\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "1) \u8bc4\u4f30\u67e5\u8be2\u590d\u6742\u5ea6\u5e76\u5206\u914d\u5230\u9884\u5b9a\u4e49\u5c42\u7ea7\uff1b2) \u5c06\u7b56\u7565\u9009\u62e9\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3CogER-Agent\uff1b3) \u5f15\u5165\u8ba4\u77e5\u5de5\u5177\u8f85\u52a9\u63a8\u7406\uff0c\u4f7fLLM\u80fd\u5728\u601d\u7ef4\u94fe\u4e2d\u81ea\u4e3b\u8c03\u7528\u5916\u90e8\u5de5\u5177\u3002", "result": "CogER\u5728In-Domain\u4efb\u52a1\u4e0a\u5b9e\u73b0\u81f3\u5c1113%\u7684\u76f8\u5bf9\u5e73\u5747\u7cbe\u786e\u5339\u914d\u63d0\u5347\uff0c\u5728Out-of-Domain\u4efb\u52a1\u4e0a\u5b9e\u73b08%\u7684\u76f8\u5bf9\u589e\u76ca\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "CogER\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u9009\u62e9\u548c\u5de5\u5177\u96c6\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u4e0d\u540c\u96be\u5ea6\u67e5\u8be2\u63d0\u4f9b\u4e86\u5f39\u6027\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.15198", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.15198", "abs": "https://arxiv.org/abs/2512.15198", "authors": ["Mohsen Nafar", "Michael R\u00f6mer", "Lin Xie"], "title": "A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem", "comment": null, "summary": "Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u7684\u53d8\u91cf\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53d8\u91cf\u5206\u7ec4\u51cf\u5c11\u52a8\u6001\u6392\u5e8f\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5728MWISP\u95ee\u9898\u4e0a\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u677e\u5f1b\u51b3\u7b56\u56fe\u5728\u79bb\u6563\u4f18\u5316\u4e2d\u63d0\u4f9b\u5bf9\u5076\u8fb9\u754c\uff0c\u4f46\u5176\u8d28\u91cf\u4e25\u91cd\u4f9d\u8d56\u53d8\u91cf\u6392\u5e8f\u548c\u5408\u5e76\u51b3\u7b56\u3002\u52a8\u6001\u53d8\u91cf\u6392\u5e8f\u867d\u7136\u80fd\u6536\u7d27\u8fb9\u754c\uff0c\u4f46\u5728\u6574\u4e2a\u53d8\u91cf\u96c6\u4e0a\u8bc4\u4f30\u4f1a\u4ea7\u751f\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u5e73\u8861\u8fd9\u79cd\u6743\u8861", "method": "\u5f15\u5165\u57fa\u4e8e\u805a\u7c7b\u7684\u53d8\u91cf\u6392\u5e8f\u6846\u67b6\uff1a\u5148\u5c06\u53d8\u91cf\u5206\u533a\u6210\u7c07\uff0c\u7136\u540e\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u5206\u89e3\u6307\u5bfc\u6392\u5e8f\u8fc7\u7a0b\u3002\u7814\u7a76\u4e24\u79cd\u7b56\u7565\uff1a1) Cluster-to-Cluster\uff1a\u4f7f\u7528\u95ee\u9898\u7279\u5b9a\u805a\u5408\u6807\u51c6\uff08\u5982MWISP\u4e2d\u7684\u7d2f\u79ef\u9876\u70b9\u6743\u91cd\uff09\u987a\u5e8f\u5904\u7406\u7c07\uff1b2) Pick-and-Sort\uff1a\u8fed\u4ee3\u5730\u4ece\u6bcf\u4e2a\u7c07\u4e2d\u9009\u62e9\u5e76\u6392\u5e8f\u4ee3\u8868\u6027\u53d8\u91cf\uff0c\u5e73\u8861\u5c40\u90e8\u591a\u6837\u6027\u4e0e\u542f\u53d1\u5f0f\u6307\u5bfc\u3002\u57fa\u4e8eMWISP\u4e2dDD\u5927\u5c0f\u589e\u957f\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8bbe\u7f6e\u7c07\u6570\u91cf\u7684\u7b56\u7565", "result": "\u5728MWISP\u57fa\u51c6\u5b9e\u4f8b\u4e0a\uff0c\u5c06\u7b56\u7565\u5d4c\u5165\u57fa\u4e8eDD\u7684\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u52a8\u6001\u53d8\u91cf\u6392\u5e8f\u57fa\u7ebf\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6301\u7eed\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c", "conclusion": "\u57fa\u4e8e\u805a\u7c7b\u7684\u53d8\u91cf\u6392\u5e8f\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u52a8\u6001\u6392\u5e8f\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5728MWISP\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u79bb\u6563\u4f18\u5316\u4e2d\u7684\u51b3\u7b56\u56fe\u7f16\u8bd1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u53d8\u91cf\u6392\u5e8f\u65b9\u6cd5"}}
{"id": "2512.15298", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.15298", "abs": "https://arxiv.org/abs/2512.15298", "authors": ["Seok-Hyun Ga", "Chun-Yen Chang"], "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I", "comment": "23 pages, 9 tables, 1 figure", "summary": "The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that \"Perception Errors\" were dominant, highlighting a \"Perception-Cognition Gap\" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a \"Calculation-Conceptualization Discrepancy,\" successfully performing calculations while failing to apply the underlying scientific concepts, and \"Process Hallucination,\" where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing \"AI-resistant questions\" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u67902025\u5e74\u97e9\u56fd\u9ad8\u8003\u5730\u7403\u79d1\u5b66I\u8bd5\u9898\uff0c\u8bc4\u4f30\u4e86GPT-4o\u3001Gemini 2.5 Flash\u548cGemini 2.5 Pro\u7b49\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u548c\u8ba4\u77e5\u5c40\u9650\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u611f\u77e5-\u8ba4\u77e5\u5dee\u8ddd\u3001\u8ba1\u7b97-\u6982\u5ff5\u5316\u5dee\u5f02\u548c\u8fc7\u7a0b\u5e7b\u89c9\u7b49\u65b9\u9762\u7684\u7f3a\u9677\uff0c\u4e3a\u8bbe\u8ba1\"\u6297AI\u95ee\u9898\"\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u968f\u7740\u5b66\u751f\u4f7f\u7528AI\u5b8c\u6210\u4f5c\u4e1a\u7684\u73b0\u8c61\u65e5\u76ca\u666e\u904d\uff0c\u5b66\u672f\u8bda\u4fe1\u548c\u8bc4\u4f30\u6709\u6548\u6027\u53d7\u5230\u5a01\u80c1\u3002\u7814\u7a76\u65e8\u5728\u6df1\u5165\u5206\u6790\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u53ca\u5176\u8ba4\u77e5\u5c40\u9650\uff0c\u4e3a\u5e94\u5bf9AI\u5728\u8bfe\u7a0b\u4f5c\u4e1a\u4e2d\u7684\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u75282025\u5e74\u97e9\u56fd\u9ad8\u8003\u5730\u7403\u79d1\u5b66I\u8bd5\u9898\uff0c\u8bbe\u8ba1\u4e09\u79cd\u5b9e\u9a8c\u6761\u4ef6\uff08\u6574\u9875\u8f93\u5165\u3001\u5355\u9879\u8f93\u5165\u548c\u4f18\u5316\u591a\u6a21\u6001\u8f93\u5165\uff09\uff0c\u5b9a\u91cf\u8bc4\u4f30GPT-4o\u3001Gemini 2.5 Flash\u548cGemini 2.5 Pro\u5728\u4e0d\u540c\u6570\u636e\u7ed3\u6784\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u975e\u7ed3\u6784\u5316\u8f93\u5165\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u5373\u4f7f\u5728\u4f18\u5316\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u4ecd\u8868\u73b0\u51fa\u57fa\u672c\u63a8\u7406\u7f3a\u9677\u3002\u5b9a\u6027\u5206\u6790\u53d1\u73b0\"\u611f\u77e5\u9519\u8bef\"\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5b58\u5728\"\u611f\u77e5-\u8ba4\u77e5\u5dee\u8ddd\"\uff08\u6a21\u578b\u80fd\u8bc6\u522b\u89c6\u89c9\u6570\u636e\u4f46\u65e0\u6cd5\u89e3\u91ca\u56fe\u8868\u4e2d\u7684\u7b26\u53f7\u610f\u4e49\uff09\u3001\"\u8ba1\u7b97-\u6982\u5ff5\u5316\u5dee\u5f02\"\uff08\u80fd\u6267\u884c\u8ba1\u7b97\u4f46\u65e0\u6cd5\u5e94\u7528\u57fa\u7840\u79d1\u5b66\u6982\u5ff5\uff09\u548c\"\u8fc7\u7a0b\u5e7b\u89c9\"\uff08\u8df3\u8fc7\u89c6\u89c9\u9a8c\u8bc1\u800c\u4f9d\u8d56\u65e0\u6839\u636e\u7684\u80cc\u666f\u77e5\u8bc6\uff09\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9AI\u7684\u7279\u5b9a\u8ba4\u77e5\u5f31\u70b9\uff08\u5982\u611f\u77e5\u4e0e\u8ba4\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff09\uff0c\u6559\u80b2\u8005\u53ef\u4ee5\u8bbe\u8ba1\"\u6297AI\u95ee\u9898\"\u6765\u533a\u5206\u771f\u5b9e\u7684\u5b66\u751f\u80fd\u529b\u4e0eAI\u751f\u6210\u7684\u56de\u7b54\uff0c\u4ece\u800c\u786e\u4fdd\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2512.15388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15388", "abs": "https://arxiv.org/abs/2512.15388", "authors": ["Reinhard Moratz", "Niklas Daute", "James Ondieki", "Markus Kattenbeck", "Mario Krajina", "Ioannis Giannopoulos"], "title": "Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations", "comment": null, "summary": "This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5b9a\u6027\u7a7a\u95f4\u5173\u7cfb\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u884c\u4eba\u63d0\u4f9b\u8def\u7ebf\u6307\u5f15\u7684\u80fd\u529b", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u884c\u4eba\u8def\u7ebf\u6307\u5f15\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5b9a\u6027\u7a7a\u95f4\u5173\u7cfb\uff08\u5982\u4e0a/\u4e0b\u3001\u5de6/\u53f3\u3001\u524d/\u540e\u7b49\uff09\u8fdb\u884c\u7cbe\u786e\u5bfc\u822a\u65b9\u9762\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u6307\u4ee4\u751f\u6210\u80fd\u529b", "method": "\u901a\u8fc7\u5f15\u5165\u5b9a\u6027\u7a7a\u95f4\u5173\u7cfb\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u80fd\u5305\u62ec\uff1a1\uff09\u6784\u5efa\u5305\u542b\u5b9a\u6027\u7a7a\u95f4\u5173\u7cfb\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff1b2\uff09\u8bbe\u8ba1\u4e13\u95e8\u7684\u6a21\u578b\u67b6\u6784\u6216\u5fae\u8c03\u7b56\u7565\uff1b3\uff09\u96c6\u6210\u7a7a\u95f4\u63a8\u7406\u6a21\u5757\uff1b4\uff09\u5f00\u53d1\u8bc4\u4f30\u884c\u4eba\u5bfc\u822a\u6307\u4ee4\u8d28\u91cf\u7684\u6307\u6807", "result": "\u6539\u8fdb\u540e\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u884c\u4eba\u5bfc\u822a\u6307\u4ee4\uff0c\u4f7f\u7528\u5b9a\u6027\u7a7a\u95f4\u5173\u7cfb\u63d0\u9ad8\u4e86\u8def\u7ebf\u63cf\u8ff0\u7684\u81ea\u7136\u6027\u548c\u5b9e\u7528\u6027", "conclusion": "\u901a\u8fc7\u96c6\u6210\u5b9a\u6027\u7a7a\u95f4\u5173\u7cfb\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u884c\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u667a\u80fd\u5bfc\u822a\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2512.15435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15435", "abs": "https://arxiv.org/abs/2512.15435", "authors": ["Stefan Edelkamp"], "title": "Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat", "comment": null, "summary": "In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08AI\u6e38\u620f\u6269\u5c55\u4eba\u7c7b\u4e13\u5bb6\u6e38\u620f\u6570\u636e\u5e93\u7684\u901a\u7528\u6846\u67b6\uff0c\u63d0\u9ad8\u591a\u73a9\u5bb6\u7eb8\u724c\u6e38\u620f\u65e9\u671f\u51b3\u7b56\u7684\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u5728\u591a\u73a9\u5bb6\u7eb8\u724c\u6e38\u620f\uff08\u5982Skat\u6216Bridge\uff09\u4e2d\uff0c\u65e9\u671f\u9636\u6bb5\uff08\u5982\u53eb\u724c\u3001\u6e38\u620f\u9009\u62e9\u548c\u521d\u59cb\u724c\u9009\u62e9\uff09\u5bf9\u6e38\u620f\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u8ba1\u7b97\u9650\u5236\u4e0b\u8fd9\u4e9b\u51b3\u7b56\u4e3b\u8981\u4f9d\u8d56\u4eba\u7c7b\u4e13\u5bb6\u6e38\u620f\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u5b58\u5728\u6570\u636e\u6709\u9650\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u901a\u7528\u7684\u81ea\u4e3e\u5916\u90e8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7AI\u81ea\u6211\u5bf9\u5f08\u751f\u6210\u6570\u767e\u4e07\u6e38\u620f\u6269\u5c55\u4eba\u7c7b\u6e38\u620f\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u5b8c\u7f8e\u7279\u5f81\u54c8\u5e0c\u51fd\u6570\u5904\u7406\u538b\u7f29\u8868\uff0c\u5b9e\u73b0\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u7684\u7eb8\u724c\u6e38\u620f\u5f15\u64ce", "result": "\u5728Skat\u6e38\u620f\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u81ea\u52a8\u5316\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u652f\u6301\u6e38\u620f\u4e2d\u7684\u5404\u79cd\u51b3\u7b56", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u6e38\u620f\u548cAI\u81ea\u6211\u5bf9\u5f08\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u73a9\u5bb6\u7eb8\u724c\u6e38\u620f\u65e9\u671f\u51b3\u7b56\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u7684\u6e38\u620f\u5f15\u64ce"}}
{"id": "2512.15462", "categories": ["cs.AI", "cs.HC", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.15462", "abs": "https://arxiv.org/abs/2512.15462", "authors": ["Jeongseok Kim", "Kangjin Kim"], "title": "Intent-Driven UAM Rescheduling", "comment": "18 pages, 2 figures, AAIML submitted", "summary": "Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ASP\u548cMILP\u7684\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u4e2d\u52a8\u6001\u64cd\u4f5c\u9700\u6c42\u548c\u6a21\u7cca\u8c03\u5ea6\u8bf7\u6c42\u7684\u8c03\u5ea6\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7cfb\u7edf\u8d44\u6e90\u53d7\u9650\uff0c\u9ad8\u6548\u7684\u5782\u76f4\u8d77\u964d\u673a\u573a\u8c03\u5ea6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u540c\u65f6\u9700\u8981\u5904\u7406\u52a8\u6001\u64cd\u4f5c\u9700\u6c42\u548c\u4eba\u7c7b\u6a21\u7cca\u7684\u91cd\u65b0\u8c03\u5ea6\u8bf7\u6c42\uff0c\u8fd9\u9700\u8981\u7cfb\u7edf\u80fd\u591f\u89e3\u91ca\u6a21\u7cca\u7684\u7528\u6237\u610f\u56fe\u5e76\u63d0\u4f9b\u900f\u660e\u7684\u51b3\u7b56\u652f\u6301\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5904\u7406\u8d44\u6e90\u53d7\u9650\u9879\u76ee\u8c03\u5ea6\u95ee\u9898\uff0c\u7ed3\u5408\u4e09\u503c\u903b\u8f91\u89e3\u91ca\u6a21\u7cca\u7528\u6237\u610f\u56fe\uff0c\u4f7f\u7528\u51b3\u7b56\u6811\u548c\u7b54\u6848\u96c6\u7f16\u7a0b\uff0c\u63d0\u51faASP\u4e0eMILP\u96c6\u6210\u7684\u7cfb\u7edf\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u4f18\u5316\u8c03\u5ea6\u5e76\u900f\u660e\u652f\u6301\u4eba\u7c7b\u8f93\u5165\u7684\u96c6\u6210\u6846\u67b6\uff0c\u4e3a\u53ef\u89e3\u91ca\u3001\u81ea\u9002\u5e94\u7684UAM\u8c03\u5ea6\u63d0\u4f9b\u4e86\u9c81\u68d2\u7ed3\u6784\u3002", "conclusion": "\u8be5\u96c6\u6210\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u4e2d\u7684\u52a8\u6001\u8c03\u5ea6\u9700\u6c42\uff0c\u901a\u8fc7\u7ed3\u5408ASP\u548cMILP\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u3001\u81ea\u9002\u5e94\u4e14\u652f\u6301\u4eba\u7c7b\u8f93\u5165\u7684\u8c03\u5ea6\u4f18\u5316\u3002"}}
{"id": "2512.15489", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15489", "abs": "https://arxiv.org/abs/2512.15489", "authors": ["Wei Du", "Shubham Toshniwal", "Branislav Kisacanin", "Sadegh Mahdavi", "Ivan Moshkov", "George Armstrong", "Stephen Ge", "Edgar Minasyan", "Feng Chen", "Igor Gitman"], "title": "Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision", "comment": null, "summary": "High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).\n  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.\n  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.\n  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.", "AI": {"tldr": "Nemotron-Math\u662f\u4e00\u4e2a\u5305\u542b750\u4e07\u6761\u89e3\u9898\u8f68\u8ff9\u7684\u5927\u89c4\u6a21\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86AoPS\u7ade\u8d5b\u9898\u548cStackExchange-Math\u793e\u533a\u95ee\u9898\uff0c\u652f\u6301\u9ad8\u4e2d\u4f4e\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u5305\u542bPython\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7248\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u5728\u63a8\u7406\u98ce\u683c\u591a\u6837\u6027\u3001\u957f\u5f62\u5f0f\u89e3\u9898\u8f68\u8ff9\u548c\u5de5\u5177\u96c6\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5927\u89c4\u6a21\u7684\u76d1\u7763\u6570\u636e\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528GPT-OSS-120B\u7684\u591a\u6a21\u5f0f\u751f\u6210\u80fd\u529b\uff0c\u6574\u540885K AoPS\u7ade\u8d5b\u95ee\u9898\u548c262K StackExchange-Math\u793e\u533a\u95ee\u9898\uff0c\u751f\u6210\u5305\u542b\u9ad8\u4e2d\u4f4e\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u7684750\u4e07\u6761\u89e3\u9898\u8f68\u8ff9\uff0c\u540c\u65f6\u63d0\u4f9b\u5e26Python\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u7248\u672c\uff0c\u5e76\u5f00\u53d1\u4e86\u987a\u5e8f\u5206\u6876\u7b56\u7565\u6765\u52a0\u901f\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u3002", "result": "Nemotron-Math\u5728\u5339\u914d\u7684AoPS\u95ee\u9898\u4e0a\u6301\u7eed\u4f18\u4e8e\u539f\u59cbOpenMathReasoning\uff0c\u6574\u5408StackExchange-Math\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff08\u7279\u522b\u662f\u5728HLE-Math\u4e0a\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\u7684\u51c6\u786e\u6027\u3002\u5728AIME 2024\u548c2025\u4e0a\u4f7f\u7528Python TIR\u5b9e\u73b0\u4e86100% maj@16\u51c6\u786e\u7387\u3002", "conclusion": "Nemotron-Math\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u6570\u5b66\u63a8\u7406\u6570\u636e\uff0c\u7ed3\u5408\u6709\u6548\u7684\u8bad\u7ec3\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u9ad8\u8d28\u91cf\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.15567", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2512.15567", "abs": "https://arxiv.org/abs/2512.15567", "authors": ["Zhangde Song", "Jieyu Lu", "Yuanqi Du", "Botao Yu", "Thomas M. Pruyn", "Yue Huang", "Kehan Guo", "Xiuzhe Luo", "Yuanhao Qu", "Yi Qu", "Yinkai Wang", "Haorui Wang", "Jeff Guo", "Jingru Gan", "Parshin Shojaee", "Di Luo", "Andres M Bran", "Gen Li", "Qiyuan Zhao", "Shao-Xiong Lennon Luo", "Yuxuan Zhang", "Xiang Zou", "Wanru Zhao", "Yifan F. Zhang", "Wucheng Zhang", "Shunan Zheng", "Saiyang Zhang", "Sartaaj Takrim Khan", "Mahyar Rajabi-Kochi", "Samantha Paradi-Maropakis", "Tony Baltoiu", "Fengyu Xie", "Tianyang Chen", "Kexin Huang", "Weiliang Luo", "Meijing Fang", "Xin Yang", "Lixue Cheng", "Jiajun He", "Soha Hassoun", "Xiangliang Zhang", "Wei Wang", "Chandan K. Reddy", "Chao Zhang", "Zhiling Zheng", "Mengdi Wang", "Le Cong", "Carla P. Gomes", "Chang-Yu Hsieh", "Aditya Nandy", "Philippe Schwaller", "Heather J. Kulik", "Haojun Jia", "Huan Sun", "Seyed Mohamad Moosavi", "Chenru Duan"], "title": "Evaluating Large Language Models in Scientific Discovery", "comment": null, "summary": "Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific \"superintelligence\". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u573a\u666f\u7684\u79d1\u5b66\u53d1\u73b0\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5f31\u70b9\uff0c\u8ddd\u79bb\u901a\u7528\u79d1\u5b66\"\u8d85\u7ea7\u667a\u80fd\"\u8fd8\u5f88\u9065\u8fdc\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u53bb\u8bed\u5883\u5316\u7684\u77e5\u8bc6\uff0c\u800c\u5ffd\u7565\u4e86\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u7684\u8fed\u4ee3\u63a8\u7406\u3001\u5047\u8bbe\u751f\u6210\u548c\u89c2\u5bdf\u89e3\u91ca\u7b49\u5173\u952e\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8d34\u8fd1\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u8fc7\u7a0b\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7531\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u771f\u5b9e\u7814\u7a76\u9879\u76ee\u5e76\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7814\u7a76\u573a\u666f\uff0c\u4ece\u8fd9\u4e9b\u573a\u666f\u4e2d\u91c7\u6837\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u95ee\u9898\u3002\u6846\u67b6\u5728\u4e24\u4e2a\u5c42\u9762\u8bc4\u4f30\u6a21\u578b\uff1a(i) \u573a\u666f\u76f8\u5173\u95ee\u9898\u7684\u51c6\u786e\u6027\uff1b(ii) \u9879\u76ee\u5c42\u9762\u8868\u73b0\uff0c\u5305\u62ec\u63d0\u51fa\u53ef\u6d4b\u8bd5\u5047\u8bbe\u3001\u8bbe\u8ba1\u5b9e\u9a8c/\u6a21\u62df\u3001\u89e3\u91ca\u7ed3\u679c\u7b49\u80fd\u529b\u3002", "result": "\u5e94\u7528\u8be5\u6846\u67b6\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\uff1a\u76f8\u5bf9\u4e8e\u901a\u7528\u79d1\u5b66\u57fa\u51c6\u5b58\u5728\u4e00\u81f4\u7684\u6027\u80fd\u5dee\u8ddd\uff1b\u6a21\u578b\u89c4\u6a21\u548c\u63a8\u7406\u80fd\u529b\u7684\u6269\u5c55\u6536\u76ca\u9012\u51cf\uff1b\u4e0d\u540c\u63d0\u4f9b\u5546\u7684\u6700\u4f18\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u5f31\u70b9\uff1b\u7814\u7a76\u573a\u666f\u4e2d\u6027\u80fd\u53d8\u5316\u5927\uff0c\u5bfc\u81f4\u4e0d\u540c\u79d1\u5b66\u53d1\u73b0\u9879\u76ee\u7684\u6700\u4f73\u6a21\u578b\u9009\u62e9\u4f1a\u53d8\u5316\u3002", "conclusion": "\u5f53\u524d\u6240\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8ddd\u79bb\u901a\u7528\u79d1\u5b66\"\u8d85\u7ea7\u667a\u80fd\"\u8fd8\u5f88\u9065\u8fdc\uff0c\u4f46\u5df2\u5728\u591a\u79cd\u79d1\u5b66\u53d1\u73b0\u9879\u76ee\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6784\u6210\u573a\u666f\u5f97\u5206\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u7a81\u663e\u4e86\u5f15\u5bfc\u63a2\u7d22\u548c\u5076\u7136\u53d1\u73b0\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\u3002\u8be5\u6846\u67b6\u4e3aLLMs\u7684\u79d1\u5b66\u53d1\u73b0\u76f8\u5173\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u5e76\u4e3a\u5176\u5411\u79d1\u5b66\u53d1\u73b0\u65b9\u5411\u53d1\u5c55\u6307\u660e\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2512.15584", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.15584", "abs": "https://arxiv.org/abs/2512.15584", "authors": ["Daniel A. Herrmann", "Abinav Chari", "Isabelle Qian", "Sree Sharvesh", "B. A. Levinstein"], "title": "A Decision-Theoretic Approach for Managing Misalignment", "comment": "Second Conference of the International Association for Safe and Ethical Artificial Intelligence (IASEAI '26)", "summary": "When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u5224\u65ad\u4f55\u65f6\u5e94\u5c06\u51b3\u7b56\u59d4\u6258\u7ed9AI\u7cfb\u7edf\uff0c\u5f3a\u8c03\u9700\u8981\u5e73\u8861\u4ef7\u503c\u5bf9\u9f50\u3001\u8ba4\u77e5\u51c6\u786e\u6027\u548c\u884c\u52a8\u8303\u56f4\u4e09\u4e2a\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u4ef7\u503c\u5bf9\u9f50\u6587\u732e\u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u5851\u9020AI\u4ef7\u503c\u89c2\uff0c\u4f46\u8f83\u5c11\u7814\u7a76\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u5982\u4f55\u5224\u65ad\u4e0d\u5b8c\u7f8e\u7684\u5bf9\u9f50\u662f\u5426\u8db3\u4ee5\u8bc1\u660e\u59d4\u6258\u51b3\u7b56\u7684\u5408\u7406\u6027\u3002\u9700\u8981\u5efa\u7acb\u539f\u5219\u6027\u65b9\u6cd5\u6765\u51b3\u5b9a\u4f55\u65f6AI\u5728\u7279\u5b9a\u60c5\u5883\u4e0b\u8db3\u591f\u5bf9\u9f50\u3002", "method": "\u5f15\u5165\u6b63\u5f0f\u7684\u51b3\u7b56\u7406\u8bba\u6846\u67b6\uff0c\u7cbe\u786e\u5206\u6790\u59d4\u6258\u51b3\u7b56\u4e2d\u7684\u6743\u8861\u5173\u7cfb\uff0c\u8003\u8651\u59d4\u6258\u8005\u5bf9AI\u4ef7\u503c\u5bf9\u9f50\u3001\u8ba4\u77e5\u51c6\u786e\u6027\u548c\u884c\u52a8\u8303\u56f4\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5f00\u53d1\u4e86\u65b0\u7684\u8bc4\u5206\u6846\u67b6\u6765\u91cf\u5316\u8fd9\u79cd\u4e8b\u524d\u51b3\u7b56\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e24\u79cd\u59d4\u6258\u60c5\u666f\u7684\u660e\u663e\u533a\u522b\uff1a1\uff09\u901a\u7528\u59d4\u6258\uff08\u4fe1\u4efbAI\u5904\u7406\u4efb\u4f55\u95ee\u9898\uff09\u9700\u8981\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u4ef7\u503c\u5bf9\u9f50\u548c\u5b8c\u5168\u7684\u8ba4\u77e5\u4fe1\u4efb\uff0c\u5b9e\u8df5\u4e2d\u5f88\u5c11\u6ee1\u8db3\uff1b2\uff09\u60c5\u5883\u7279\u5b9a\u59d4\u6258\u5373\u4f7f\u5728\u663e\u8457\u4e0d\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u4e5f\u53ef\u80fd\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3aAI\u7684\u66f4\u9ad8\u51c6\u786e\u6027\u6216\u66f4\u5e7f\u884c\u52a8\u8303\u56f4\u53ef\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6574\u4f53\u51b3\u7b56\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u786e\u5b9aAI\u5728\u7ed9\u5b9a\u60c5\u5883\u4e0b\u662f\u5426\u8db3\u591f\u5bf9\u9f50\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5c06\u91cd\u70b9\u4ece\u5b9e\u73b0\u5b8c\u7f8e\u5bf9\u9f50\u8f6c\u5411\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7ba1\u7406\u59d4\u6258\u7684\u98ce\u9669\u548c\u56de\u62a5\u3002\u60c5\u5883\u7279\u5b9a\u59d4\u6258\u53ef\u4ee5\u5728\u663e\u8457\u4e0d\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u7406\u6027\u3002"}}
{"id": "2512.15662", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15662", "abs": "https://arxiv.org/abs/2512.15662", "authors": ["Jiaqi Xu", "Cuiling Lan", "Xuejin Chen", "Yan LU"], "title": "Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning", "comment": "Under Review", "summary": "Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.", "AI": {"tldr": "STC\u6846\u67b6\u5c06\u63a8\u7406\u4e0e\u81ea\u6211\u6279\u5224\u5728\u6bcf\u4e00\u6b65\u4ea4\u7ec7\uff0c\u901a\u8fc7\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u7406\u8d28\u91cf\u548c\u81ea\u6211\u8bc4\u4f30\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5c06\u63a8\u7406\u4e0e\u9a8c\u8bc1\u5206\u79bb\uff1a\u8981\u4e48\u751f\u6210\u63a8\u7406\u800c\u4e0d\u8fdb\u884c\u81ea\u6211\u68c0\u67e5\uff0c\u8981\u4e48\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u5668\u4e8b\u540e\u68c0\u6d4b\u9519\u8bef\u3002\u524d\u8005\u7f3a\u4e4f\u5373\u65f6\u53cd\u9988\uff0c\u540e\u8005\u589e\u52a0\u7cfb\u7edf\u590d\u6742\u6027\u5e76\u963b\u788d\u540c\u6b65\u5b66\u4e60\u3002\u53d7\u4eba\u7c7b\u6279\u5224\u6027\u601d\u7ef4\u542f\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faStepwise Think-Critique (STC)\u6846\u67b6\uff0c\u5728\u5355\u4e2a\u6a21\u578b\u5185\u6bcf\u4e00\u6b65\u4ea4\u7ec7\u63a8\u7406\u548c\u81ea\u6211\u6279\u5224\u3002\u4f7f\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u8bad\u7ec3\uff0c\u7ed3\u5408\u63a8\u7406\u5956\u52b1\u548c\u6279\u5224\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u5171\u540c\u4f18\u5316\u63a8\u7406\u8d28\u91cf\u548c\u81ea\u6211\u8bc4\u4f30\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTC\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6279\u5224\u6027\u601d\u7ef4\u80fd\u529b\uff0c\u5e76\u4ea7\u751f\u66f4\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "STC\u4ee3\u8868\u4e86\u5411\u5177\u6709\u5185\u7f6e\u6279\u5224\u6027\u601d\u7ef4\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u4e86\u63a8\u7406\u4e0e\u81ea\u6211\u6279\u5224\u7684\u6709\u673a\u7ed3\u5408\u3002"}}
{"id": "2512.15663", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15663", "abs": "https://arxiv.org/abs/2512.15663", "authors": ["Chase Walker", "Rickard Ewetz"], "title": "Explaining the Reasoning of Large Language Models Using Attribution Graphs", "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.", "AI": {"tldr": "CAGE\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u6709\u5411\u5c5e\u6027\u56fe\u6765\u6539\u8fdbLLM\u7684\u4e0a\u4e0b\u6587\u5f52\u56e0\uff0c\u91cf\u5316\u6bcf\u4e2a\u751f\u6210\u5185\u5bb9\u5982\u4f55\u53d7\u5230\u63d0\u793a\u548c\u5148\u524d\u751f\u6210\u5185\u5bb9\u7684\u5f71\u54cd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8640%\u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u5b58\u5728\u5b89\u5168\u548c\u4fe1\u4efb\u95ee\u9898\u3002\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5f52\u56e0\u65b9\u6cd5\u53ea\u76f4\u63a5\u5173\u8054\u751f\u6210\u6807\u8bb0\u548c\u63d0\u793a\uff0c\u5ffd\u7565\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4ee3\u9645\u5f71\u54cd\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51faCAGE\u6846\u67b6\uff0c\u6784\u5efa\u6709\u5411\u5c5e\u6027\u56fe\u6765\u91cf\u5316\u6bcf\u4e2a\u751f\u6210\u5185\u5bb9\u5982\u4f55\u53d7\u5230\u63d0\u793a\u548c\u6240\u6709\u5148\u524d\u751f\u6210\u5185\u5bb9\u7684\u5f71\u54cd\u3002\u8be5\u56fe\u4fdd\u6301\u56e0\u679c\u5173\u7cfb\u548c\u884c\u968f\u673a\u6027\uff0c\u901a\u8fc7\u6cbf\u56fe\u4e2d\u8def\u5f84\u8fb9\u7f18\u5316\u4e2d\u95f4\u8d21\u732e\u6765\u8ba1\u7b97\u4e0a\u4e0b\u6587\u5f52\u56e0\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001\u6307\u6807\u548c\u65b9\u6cd5\u4e0a\uff0cCAGE\u663e\u8457\u63d0\u9ad8\u4e86\u4e0a\u4e0b\u6587\u5f52\u56e0\u7684\u5fe0\u5b9e\u5ea6\uff0c\u5e73\u5747\u63d0\u5347\u9ad8\u8fbe40%\u3002", "conclusion": "CAGE\u6846\u67b6\u901a\u8fc7\u8003\u8651\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4ee3\u9645\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u548c\u5fe0\u5b9e\u7684LLM\u884c\u4e3a\u89e3\u91ca\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
