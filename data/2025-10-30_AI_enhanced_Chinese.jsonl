{"id": "2510.24943", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24943", "abs": "https://arxiv.org/abs/2510.24943", "authors": ["Alfonso Ladino-Rincon", "Stephen W. Nesbitt"], "title": "Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives", "comment": "8 pages, 3 figures", "summary": "We introduce Radar DataTree, the first dataset-level framework that extends\nthe WMO FM-301 standard from individual radar volume scans to time-resolved,\nanalysis-ready archives. Weather radar data are among the most scientifically\nvaluable yet structurally underutilized Earth observation datasets. Despite\nwidespread public availability, radar archives remain fragmented,\nvendor-specific, and poorly aligned with FAIR (Findable, Accessible,\nInteroperable, Reusable) principles, hindering large-scale research,\nreproducibility, and cloud-native computation. Radar DataTree addresses these\nlimitations with a scalable, open-source architecture that transforms\noperational radar archives into FAIR-compliant, cloud-optimized datasets. Built\non the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,\nRadar DataTree organizes radar volume scans as hierarchical, metadata-rich\nstructures and serializes them to Zarr for scalable analysis. Coupled with\nIcechunk for ACID-compliant storage and versioning, this architecture enables\nefficient, parallel computation across thousands of radar scans with minimal\npreprocessing. We demonstrate significant performance gains in case studies\nincluding Quasi-Vertical Profile (QVP) and precipitation accumulation\nworkflows, and release all tools and datasets openly via the Raw2Zarr\nrepository. This work contributes a reproducible and extensible foundation for\nradar data stewardship, high-performance geoscience, and AI-ready weather\ninfrastructure.", "AI": {"tldr": "Radar DataTree\u662f\u9996\u4e2a\u6570\u636e\u96c6\u7ea7\u6846\u67b6\uff0c\u5c06WMO FM-301\u6807\u51c6\u4ece\u5355\u6b21\u96f7\u8fbe\u4f53\u626b\u63cf\u6269\u5c55\u5230\u65f6\u95f4\u5206\u8fa8\u3001\u5206\u6790\u5c31\u7eea\u7684\u5b58\u6863\uff0c\u89e3\u51b3\u4e86\u5929\u6c14\u96f7\u8fbe\u6570\u636eFAIR\u539f\u5219\u9075\u4ece\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u5929\u6c14\u96f7\u8fbe\u6570\u636e\u662f\u79d1\u5b66\u4ef7\u503c\u6700\u9ad8\u4f46\u7ed3\u6784\u5229\u7528\u6700\u4e0d\u8db3\u7684\u5730\u7403\u89c2\u6d4b\u6570\u636e\u96c6\u4e4b\u4e00\u3002\u5c3d\u7ba1\u5e7f\u6cdb\u516c\u5f00\u53ef\u7528\uff0c\u4f46\u96f7\u8fbe\u5b58\u6863\u4ecd\u7136\u5206\u6563\u3001\u4f9b\u5e94\u5546\u7279\u5b9a\uff0c\u4e14\u4e0eFAIR\u539f\u5219\u5bf9\u9f50\u4e0d\u4f73\uff0c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u7814\u7a76\u3001\u53ef\u91cd\u590d\u6027\u548c\u4e91\u539f\u751f\u8ba1\u7b97\u3002", "method": "\u57fa\u4e8eFM-301/CfRadial 2.1\u6807\u51c6\uff0c\u4f7f\u7528xarray DataTree\u5b9e\u73b0\uff0c\u5c06\u96f7\u8fbe\u4f53\u626b\u63cf\u7ec4\u7ec7\u4e3a\u5206\u5c42\u3001\u5143\u6570\u636e\u4e30\u5bcc\u7684\u7ed3\u6784\uff0c\u5e76\u5e8f\u5217\u5316\u4e3aZarr\u683c\u5f0f\u8fdb\u884c\u53ef\u6269\u5c55\u5206\u6790\u3002\u7ed3\u5408Icechunk\u5b9e\u73b0ACID\u517c\u5bb9\u5b58\u50a8\u548c\u7248\u672c\u63a7\u5236\u3002", "result": "\u5728\u51c6\u5782\u76f4\u5256\u9762(QVP)\u548c\u964d\u6c34\u7d2f\u79ef\u5de5\u4f5c\u6d41\u7b49\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6240\u6709\u5de5\u5177\u548c\u6570\u636e\u96c6\u901a\u8fc7Raw2Zarr\u5b58\u50a8\u5e93\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u96f7\u8fbe\u6570\u636e\u7ba1\u7406\u3001\u9ad8\u6027\u80fd\u5730\u7403\u79d1\u5b66\u548cAI\u5c31\u7eea\u7684\u5929\u6c14\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.25277", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25277", "abs": "https://arxiv.org/abs/2510.25277", "authors": ["Simon S\u00fcwer", "Mai Khanh Mai", "Christoph Klein", "Nicola G\u00f6tzenberger", "Denis Dali\u0107", "Andreas Maier", "Jan Baumbach"], "title": "A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon", "comment": null, "summary": "The integration of clinical data offers significant potential for the\ndevelopment of personalized medicine. However, its use is severely restricted\nby the General Data Protection Regulation (GDPR), especially for small cohorts\nwith rare diseases. High-quality, structured data is essential for the\ndevelopment of predictive medical AI. In this case study, we propose a novel,\nmulti-stage approach to secure AI training: (1) The model is designed on a\nsimulated clinical knowledge graph (cKG). This graph is used exclusively to\nrepresent the structural characteristics of the real cKG without revealing any\nsensitive content. (2) The model is then integrated into the FeatureCloud (FC)\nfederated learning framework, where it is prepared in a single-client\nconfiguration within a protected execution environment. (3) Training then takes\nplace within the hospital environment on the real cKG, either under the direct\nsupervision of hospital staff or via a fully automated pipeline controlled by\nthe hospital. (4) Finally, verified evaluation scripts are executed, which only\nreturn aggregated performance metrics. This enables immediate performance\nfeedback without sensitive patient data or individual predictions, leaving the\nclinic. A fundamental element of this approach involves the incorporation of a\ncKG, which serves to organize multi-omics and patient data within the context\nof real-world hospital environments. This approach was successfully validated\nduring the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner\nChildren's Hospital (HCH-LMU): 50 students developed models for patient\nclassification and diagnosis without access to real data. Deploying secure\nalgorithms via federated frameworks, such as the FC framework, could be a\npractical way of achieving privacy-preserving AI in healthcare.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u9636\u6bb5\u5b89\u5168AI\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u8bbe\u8ba1\u6a21\u578b\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u8bad\u7ec3\uff0c\u786e\u4fdd\u60a3\u8005\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e2a\u6027\u5316\u533b\u7597AI\u5f00\u53d1\u3002", "motivation": "\u4e34\u5e8a\u6570\u636e\u6574\u5408\u5bf9\u4e2a\u6027\u5316\u533b\u7597\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7GDPR\u4e25\u683c\u9650\u5236\uff0c\u7279\u522b\u662f\u7f55\u89c1\u75c5\u5c0f\u961f\u5217\u7814\u7a76\u3002\u9700\u8981\u5728\u4e0d\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u5f00\u53d1\u9ad8\u8d28\u91cf\u533b\u7597AI\u3002", "method": "\u56db\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5728\u6a21\u62df\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8bbe\u8ba1\u6a21\u578b\uff1b2) \u5728FeatureCloud\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u51c6\u5907\u6a21\u578b\uff1b3) \u5728\u533b\u9662\u73af\u5883\u4e2d\u8bad\u7ec3\u771f\u5b9e\u6570\u636e\uff1b4) \u6267\u884c\u9a8c\u8bc1\u8bc4\u4f30\u811a\u672c\u8fd4\u56de\u805a\u5408\u6027\u80fd\u6307\u6807\u3002", "result": "\u5728TUM.ai Makeathon 2024\u6311\u6218\u4e2d\u6210\u529f\u9a8c\u8bc1\uff0c50\u540d\u5b66\u751f\u65e0\u9700\u8bbf\u95ee\u771f\u5b9e\u6570\u636e\u5373\u53ef\u5f00\u53d1\u60a3\u8005\u5206\u7c7b\u548c\u8bca\u65ad\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u90e8\u7f72\u5b89\u5168\u7b97\u6cd5\u662f\u5b9e\u73b0\u533b\u7597\u4fdd\u5065\u4e2d\u9690\u79c1\u4fdd\u62a4AI\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2510.24832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24832", "abs": "https://arxiv.org/abs/2510.24832", "authors": ["Hong Wang", "Zhezheng Hao", "Jian Luo", "Chenxing Wei", "Yao Shu", "Lei Liu", "Qiang Lin", "Hande Dong", "Jiawei Chen"], "title": "Scheduling Your LLM Reinforcement Learning with Reasoning Trees", "comment": null, "summary": "Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large\nLanguage Models (LLMs) can be conceptualized as progressively editing a query's\n`Reasoning Tree'. This process involves exploring nodes (tokens) and\ndynamically modifying the model's policy at each node. When combined with data\nscheduling, this process yields further gains in data efficiency and accuracy.\nHowever, existing RLVR data scheduling methods typically rely on path-based\nmetrics to rank queries, overlooking the reasoning tree structures of these\nqueries. In this paper, we introduce a novel metric, namely Reasoning Score\n(r-score), which measures the query's learning difficulty based on the\nstructure of its reasoning tree. Based on the r-score, we propose the Reasoning\nTree Schedule (Re-Schedule), a scheduling algorithm that constructs a\ncurriculum progressing from structurally simple (high r-score) to complex (low\nr-score) queries. Experiments on six math-reasoning benchmarks show that\nRe-Schedule significantly improves average accuracy, achieving gains of up to\n3.2%. These strong results validate our approach and demonstrate that a\nstructural understanding of the reasoning tree provides a more powerful and\nprincipled foundation for RLVR data scheduling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u6811\u7ed3\u6784\u7684\u65b0\u6307\u6807r-score\u6765\u8861\u91cf\u67e5\u8be2\u7684\u5b66\u4e60\u96be\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86Re-Schedule\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u6570\u636e\u8c03\u5ea6\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u57fa\u4e8e\u8def\u5f84\u7684\u6307\u6807\u6765\u6392\u540d\u67e5\u8be2\uff0c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u67e5\u8be2\u7684\u63a8\u7406\u6811\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8c03\u5ea6\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u63a8\u7406\u5206\u6570(r-score)\u6765\u8861\u91cf\u67e5\u8be2\u7684\u5b66\u4e60\u96be\u5ea6\uff0c\u57fa\u4e8er-score\u63d0\u51fa\u63a8\u7406\u6811\u8c03\u5ea6\u7b97\u6cd5(Re-Schedule)\uff0c\u6784\u5efa\u4ece\u7ed3\u6784\u7b80\u5355\u5230\u590d\u6742\u7684\u8bfe\u7a0b\u8fdb\u5ea6\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRe-Schedule\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe3.2%\u3002", "conclusion": "\u5bf9\u63a8\u7406\u6811\u7684\u7ed3\u6784\u7406\u89e3\u4e3aRLVR\u6570\u636e\u8c03\u5ea6\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u539f\u5219\u6027\u7684\u57fa\u7840\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24802", "categories": ["cs.MA", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.24802", "abs": "https://arxiv.org/abs/2510.24802", "authors": ["Qiumeng Li", "Chunhou Ji", "Xinyue Liu"], "title": "From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation", "comment": "47 pages, 3 figures", "summary": "Understanding and replicating human mobility requires not only\nspatial-temporal accuracy but also an awareness of the cognitive hierarchy\nunderlying real-world travel decisions. Traditional agent-based or deep\nlearning models can reproduce statistical patterns of movement but fail to\ncapture the semantic coherence and causal logic of human behavior. Large\nlanguage models (LLMs) show potential, but struggle to balance creative\nreasoning with strict structural compliance. This study proposes a Hierarchical\nLLM-Agent Framework, termed Narrative-to-Action, that integrates high-level\nnarrative reasoning, mid-level reflective planning, and low-level behavioral\nexecution within a unified cognitive hierarchy. At the macro level, one agent\nis employed as a \"creative writer\" to produce diary-style narratives rich in\nmotivation and context, then uses another agent as a \"structural parser\" to\nconvert narratives into machine-readable plans. A dynamic execution module\nfurther grounds agents in geographic environments and enables adaptive\nbehavioral adjustments guided by a novel occupation-aware metric, Mobility\nEntropy by Occupation (MEO), which captures heterogeneous schedule flexibility\nacross different occupational personalities. At the micro level, the agent\nexecutes concrete actions-selecting locations, transportation modes, and time\nintervals-through interaction with an environmental simulation. By embedding\nthis multi-layer cognitive process, the framework produces not only synthetic\ntrajectories that align closely with real-world patterns but also interpretable\nrepresentations of human decision logic. This research advances synthetic\nmobility generation from a data-driven paradigm to a cognition-driven\nsimulation, providing a scalable pathway for understanding, predicting, and\nsynthesizing complex urban mobility behaviors through hierarchical LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42LLM-\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u9ad8\u5c42\u6b21\u53d9\u4e8b\u63a8\u7406\u3001\u4e2d\u5c42\u6b21\u53cd\u601d\u89c4\u5212\u548c\u4f4e\u5c42\u6b21\u884c\u4e3a\u6267\u884c\u6574\u5408\u5230\u7edf\u4e00\u7684\u8ba4\u77e5\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u7528\u4e8e\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u903b\u8f91\u7684\u5408\u6210\u79fb\u52a8\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u667a\u80fd\u4f53\u6216\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u91cd\u73b0\u79fb\u52a8\u7684\u7edf\u8ba1\u6a21\u5f0f\uff0c\u4f46\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u884c\u4e3a\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u56e0\u679c\u903b\u8f91\u3002\u5927\u8bed\u8a00\u6a21\u578b\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u521b\u9020\u6027\u63a8\u7406\u4e0e\u4e25\u683c\u7ed3\u6784\u5408\u89c4\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u91c7\u7528\u5206\u5c42LLM-\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u5b8f\u89c2\u5c42\u9762\u4f7f\u7528\"\u521b\u610f\u5199\u624b\"\u751f\u6210\u65e5\u8bb0\u5f0f\u53d9\u4e8b\uff0c\"\u7ed3\u6784\u89e3\u6790\u5668\"\u5c06\u53d9\u4e8b\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u8bfb\u8ba1\u5212\uff1b\u5fae\u89c2\u5c42\u9762\u901a\u8fc7\u73af\u5883\u6a21\u62df\u6267\u884c\u5177\u4f53\u884c\u52a8\uff1b\u5f15\u5165\u57fa\u4e8e\u804c\u4e1a\u7684\u79fb\u52a8\u71b5\u6307\u6807\u6765\u6307\u5bfc\u884c\u4e3a\u8c03\u6574\u3002", "result": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u751f\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u6a21\u5f0f\u9ad8\u5ea6\u4e00\u81f4\u7684\u5408\u6210\u8f68\u8ff9\uff0c\u8fd8\u63d0\u4f9b\u4eba\u7c7b\u51b3\u7b56\u903b\u8f91\u7684\u53ef\u89e3\u91ca\u8868\u793a\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u5408\u6210\u79fb\u52a8\u751f\u6210\u4ece\u6570\u636e\u9a71\u52a8\u8303\u5f0f\u63a8\u8fdb\u5230\u8ba4\u77e5\u9a71\u52a8\u6a21\u62df\uff0c\u901a\u8fc7\u5206\u5c42LLM\u667a\u80fd\u4f53\u4e3a\u7406\u89e3\u3001\u9884\u6d4b\u548c\u5408\u6210\u590d\u6742\u57ce\u5e02\u79fb\u52a8\u884c\u4e3a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002"}}
{"id": "2510.25362", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25362", "abs": "https://arxiv.org/abs/2510.25362", "authors": ["Georgios L. Stavrinides", "Helen D. Karatza"], "title": "Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges", "comment": "This version of the manuscript has been accepted for publication in\n  Modeling and Simulation in HPC and Cloud Systems, ser. Studies in Big Data,\n  after peer review (Author Accepted Manuscript). It is not the final published\n  version (Version of Record) and does not reflect any post-acceptance\n  improvements. The Version of Record is available online at\n  https://doi.org/10.1007/978-3-319-73767-6_2", "summary": "With the explosive growth of big data, workloads tend to get more complex and\ncomputationally demanding. Such applications are processed on distributed\ninterconnected resources that are becoming larger in scale and computational\ncapacity. Data-intensive applications may have different degrees of parallelism\nand must effectively exploit data locality. Furthermore, they may impose\nseveral Quality of Service requirements, such as time constraints and\nresilience against failures, as well as other objectives, like energy\nefficiency. These features of the workloads, as well as the inherent\ncharacteristics of the computing resources required to process them, present\nmajor challenges that require the employment of effective scheduling\ntechniques. In this chapter, a classification of data-intensive workloads is\nproposed and an overview of the most commonly used approaches for their\nscheduling in large-scale distributed systems is given. We present novel\nstrategies that have been proposed in the literature and shed light on open\nchallenges and future directions.", "AI": {"tldr": "\u672c\u7ae0\u63d0\u51fa\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5206\u7c7b\uff0c\u5e76\u6982\u8ff0\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u8c03\u5ea6\u8fd9\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4ecb\u7ecd\u4e86\u6587\u732e\u4e2d\u7684\u65b0\u7b56\u7565\uff0c\u5e76\u6307\u51fa\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u6570\u636e\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u548c\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9700\u8981\u5728\u89c4\u6a21\u66f4\u5927\u3001\u8ba1\u7b97\u80fd\u529b\u66f4\u5f3a\u7684\u5206\u5e03\u5f0f\u4e92\u8fde\u8d44\u6e90\u4e0a\u5904\u7406\u3002\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u5177\u6709\u4e0d\u540c\u7684\u5e76\u884c\u5ea6\uff0c\u9700\u8981\u6709\u6548\u5229\u7528\u6570\u636e\u5c40\u90e8\u6027\uff0c\u5e76\u6ee1\u8db3\u670d\u52a1\u8d28\u91cf\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5206\u7c7b\uff0c\u5e76\u6982\u8ff0\u5728\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u8c03\u5ea6\u8fd9\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4ecb\u7ecd\u6587\u732e\u4e2d\u63d0\u51fa\u7684\u65b0\u7b56\u7565\u3002", "result": "\u63d0\u4f9b\u4e86\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5168\u9762\u5206\u7c7b\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8c03\u5ea6\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u8c03\u5ea6\u6280\u672f\u3002\u672c\u7ae0\u4e3a\u7406\u89e3\u5f53\u524d\u65b9\u6cd5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.25005", "categories": ["cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.25005", "abs": "https://arxiv.org/abs/2510.25005", "authors": ["Saptarshi Saha", "Dhruv Vansraj Rathore", "Utpal Garain"], "title": "Cyclic Counterfactuals under Shift-Scale Interventions", "comment": "Accepted at NeurIPS 2025", "summary": "Most counterfactual inference frameworks traditionally assume acyclic\nstructural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However,\nmany real-world systems (e.g. biological systems) contain feedback loops or\ncyclic dependencies that violate acyclicity. In this work, we study\ncounterfactual inference in cyclic SCMs under shift-scale interventions, i.e.,\nsoft, policy-style changes that rescale and/or shift a variable's mechanism.", "AI": {"tldr": "\u7814\u7a76\u5728\u5305\u542b\u53cd\u9988\u5faa\u73af\u7684\u5faa\u73af\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u5173\u6ce8\u4e8e\u79fb\u4f4d-\u5c3a\u5ea6\u5e72\u9884\u8fd9\u79cd\u8f6f\u6027\u653f\u7b56\u5f0f\u53d8\u5316", "motivation": "\u4f20\u7edf\u53cd\u4e8b\u5b9e\u63a8\u7406\u6846\u67b6\u5047\u8bbe\u65e0\u73af\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u4f46\u8bb8\u591a\u771f\u5b9e\u7cfb\u7edf\uff08\u5982\u751f\u7269\u7cfb\u7edf\uff09\u5305\u542b\u8fdd\u53cd\u65e0\u73af\u6027\u7684\u53cd\u9988\u5faa\u73af\u6216\u5faa\u73af\u4f9d\u8d56", "method": "\u7814\u7a76\u5faa\u73af\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e2d\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u7279\u522b\u5173\u6ce8\u79fb\u4f4d-\u5c3a\u5ea6\u5e72\u9884\uff08\u5373\u5bf9\u53d8\u91cf\u673a\u5236\u8fdb\u884c\u91cd\u65b0\u7f29\u653e\u548c/\u6216\u79fb\u4f4d\u7684\u8f6f\u6027\u653f\u7b56\u5f0f\u53d8\u5316\uff09", "result": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5faa\u73af\u7cfb\u7edf\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u4f46\u5177\u4f53\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e", "conclusion": "\u9700\u8981\u6269\u5c55\u53cd\u4e8b\u5b9e\u63a8\u7406\u6846\u67b6\u4ee5\u5904\u7406\u5305\u542b\u53cd\u9988\u5faa\u73af\u7684\u5faa\u73af\u7cfb\u7edf\uff0c\u79fb\u4f4d-\u5c3a\u5ea6\u5e72\u9884\u4e3a\u8fd9\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5e72\u9884\u65b9\u6cd5"}}
{"id": "2510.25007", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25007", "abs": "https://arxiv.org/abs/2510.25007", "authors": ["Islam Nassar", "Yang Lin", "Yuan Jin", "Rongxin Zhu", "Chang Wei Tan", "Zenan Zhai", "Nitika Mathur", "Thanh Tien Vu", "Xu Zhong", "Long Duong", "Yuan-Fang Li"], "title": "Taming the Real-world Complexities in CPT E/M Coding with Large Language Models", "comment": "EMNLP 2025 Industry Track", "summary": "Evaluation and Management (E/M) coding, under the Current Procedural\nTerminology (CPT) taxonomy, documents medical services provided to patients by\nphysicians. Used primarily for billing purposes, it is in physicians' best\ninterest to provide accurate CPT E/M codes. %While important, it is an\nauxiliary task that adds to physicians' documentation burden. Automating this\ncoding task will help alleviate physicians' documentation burden, improve\nbilling efficiency, and ultimately enable better patient care. However, a\nnumber of real-world complexities have made E/M encoding automation a\nchallenging task. In this paper, we elaborate some of the key complexities and\npresent ProFees, our LLM-based framework that tackles them, followed by a\nsystematic evaluation. On an expert-curated real-world dataset, ProFees\nachieves an increase in coding accuracy of more than 36\\% over a commercial CPT\nE/M coding system and almost 5\\% over our strongest single-prompt baseline,\ndemonstrating its effectiveness in addressing the real-world complexities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ProFees\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316E/M\u7f16\u7801\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597\u8d26\u5355\u7f16\u7801\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6bd4\u5546\u4e1a\u7cfb\u7edf\u63d0\u9ad8\u4e8636%\u4ee5\u4e0a\u7684\u7f16\u7801\u51c6\u786e\u7387\u3002", "motivation": "\u81ea\u52a8\u5316E/M\u7f16\u7801\u4efb\u52a1\u53ef\u4ee5\u51cf\u8f7b\u533b\u751f\u7684\u6587\u6863\u8d1f\u62c5\uff0c\u63d0\u9ad8\u8ba1\u8d39\u6548\u7387\uff0c\u5e76\u6700\u7ec8\u5b9e\u73b0\u66f4\u597d\u7684\u60a3\u8005\u62a4\u7406\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u4f7f\u5f97E/M\u7f16\u7801\u81ea\u52a8\u5316\u6210\u4e3a\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86ProFees\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406E/M\u7f16\u7801\u4e2d\u7684\u5173\u952e\u590d\u6742\u6027\u3002", "result": "\u5728\u4e13\u5bb6\u7b56\u5212\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cProFees\u6bd4\u5546\u4e1aCPT E/M\u7f16\u7801\u7cfb\u7edf\u7684\u7f16\u7801\u51c6\u786e\u7387\u63d0\u9ad8\u4e8636%\u4ee5\u4e0a\uff0c\u6bd4\u6700\u5f3a\u7684\u5355\u63d0\u793a\u57fa\u7ebf\u63d0\u9ad8\u4e86\u8fd15%\u3002", "conclusion": "ProFees\u6846\u67b6\u5728\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u52a8\u5316\u533b\u7597\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.25757", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.25757", "abs": "https://arxiv.org/abs/2510.25757", "authors": ["Jonas Spenger", "Kolya Krafeld", "Ruben van Gemeren", "Philipp Haller", "Paris Carbone"], "title": "Holon Streaming: Global Aggregations with Windowed CRDTs", "comment": "10 pages, 9 figures, 2 tables, 2 listings, 2 algorithms", "summary": "Scaling global aggregations is a challenge for exactly-once stream processing\nsystems. Current systems implement these either by computing the aggregation in\na single task instance, or by static aggregation trees, which limits\nscalability and may become a bottleneck. Moreover, the end-to-end latency is\ndetermined by the slowest path in the tree, and failures and reconfiguration\ncause large latency spikes due to the centralized coordination. Towards these\nissues, we present Holon Streaming, an exactly-once stream processing system\nfor global aggregations. Its deterministic programming model uses windowed\nconflict-free replicated data types (Windowed CRDTs), a novel abstraction for\nshared replicated state. Windowed CRDTs make computing global aggregations\nscalable. Furthermore, their guarantees such as determinism and convergence\nenable the design of efficient failure recovery algorithms by decentralized\ncoordination. Our evaluation shows a 5x lower latency and 2x higher throughput\nthan an existing stream processing system on global aggregation workloads, with\nan 11x latency reduction under failure scenarios. The paper demonstrates the\neffectiveness of decentralized coordination with determinism, and the utility\nof Windowed CRDTs for global aggregations.", "AI": {"tldr": "Holon Streaming\u662f\u4e00\u4e2a\u652f\u6301\u7cbe\u786e\u4e00\u6b21\u8bed\u4e49\u7684\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u7a97\u53e3\u5316\u65e0\u51b2\u7a81\u590d\u5236\u6570\u636e\u7c7b\u578b\uff08Windowed CRDTs\uff09\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5168\u5c40\u805a\u5408\u8ba1\u7b97\uff0c\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u673a\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u5177\u6709\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u6d41\u5904\u7406\u7cfb\u7edf\u5728\u5168\u5c40\u805a\u5408\u8ba1\u7b97\u65b9\u9762\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\u548c\u6027\u80fd\u74f6\u9888\uff0c\u5355\u4efb\u52a1\u5b9e\u4f8b\u6216\u9759\u6001\u805a\u5408\u6811\u67b6\u6784\u5bfc\u81f4\u5ef6\u8fdf\u9ad8\u3001\u5bb9\u9519\u6062\u590d\u65f6\u5ef6\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u786e\u5b9a\u6027\u7f16\u7a0b\u6a21\u578b\uff0c\u4f7f\u7528\u7a97\u53e3\u5316\u65e0\u51b2\u7a81\u590d\u5236\u6570\u636e\u7c7b\u578b\uff08Windowed CRDTs\uff09\u4f5c\u4e3a\u5171\u4eab\u590d\u5236\u72b6\u6001\u7684\u62bd\u8c61\uff0c\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u548c\u9ad8\u6548\u6545\u969c\u6062\u590d\u7b97\u6cd5\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u5728\u5168\u5c40\u805a\u5408\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b05\u500d\u5ef6\u8fdf\u964d\u4f4e\u548c2\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728\u6545\u969c\u573a\u666f\u4e0b\u5ef6\u8fdf\u51cf\u5c1111\u500d\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u4e0e\u786e\u5b9a\u6027\u4fdd\u8bc1\u76f8\u7ed3\u5408\u7684\u8bbe\u8ba1\u65b9\u6cd5\u6709\u6548\uff0c\u7a97\u53e3\u5316CRDTs\u5728\u5168\u5c40\u805a\u5408\u8ba1\u7b97\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25014", "abs": "https://arxiv.org/abs/2510.25014", "authors": ["Minkyung Kim", "Junsik Kim", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading", "comment": "8 pages main content, 18 pages supplementary material, 4 figures", "summary": "Large Language Models (LLMs) enable dynamic game interactions but fail to\nfollow essential procedural flows in rule-governed trading systems, eroding\nplayer trust. This work resolves the core tension between the creative\nflexibility of LLMs and the procedural demands of in-game trading\n(browse-offer-review-confirm). To this end, Autoregressive State-Tracking\nPrompting (ASTP) is introduced, a methodology centered on a strategically\norchestrated prompt that compels an LLM to make its state-tracking process\nexplicit and verifiable. Instead of relying on implicit contextual\nunderstanding, ASTP tasks the LLM with identifying and reporting a predefined\nstate label from the previous turn. To ensure transactional integrity, this is\ncomplemented by a state-specific placeholder post-processing method for\naccurate price calculations. Evaluation across 300 trading dialogues\ndemonstrates >99% state compliance and 99.3% calculation precision. Notably,\nASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash)\nmatches larger models' (Gemini-2.5-Pro) performance while reducing response\ntime from 21.2s to 2.4s, establishing a practical foundation that satisfies\nboth real-time requirements and resource constraints of commercial games.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Autoregressive State-Tracking Prompting (ASTP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u72b6\u6001\u8ddf\u8e2a\u548c\u5360\u4f4d\u7b26\u540e\u5904\u7406\uff0c\u89e3\u51b3LLM\u5728\u6e38\u620f\u4ea4\u6613\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u9075\u5faa\u89c4\u5219\u6d41\u7a0b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e8699%\u4ee5\u4e0a\u7684\u72b6\u6001\u5408\u89c4\u6027\u548c99.3%\u7684\u8ba1\u7b97\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u867d\u7136\u652f\u6301\u52a8\u6001\u6e38\u620f\u4ea4\u4e92\uff0c\u4f46\u5728\u89c4\u5219\u6cbb\u7406\u7684\u4ea4\u6613\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u9075\u5faa\u5fc5\u8981\u7684\u7a0b\u5e8f\u6d41\u7a0b(\u6d4f\u89c8-\u62a5\u4ef7-\u5ba1\u6838-\u786e\u8ba4)\uff0c\u8fd9\u4f1a\u524a\u5f31\u73a9\u5bb6\u4fe1\u4efb\u3002", "method": "\u5f15\u5165ASTP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8feb\u4f7fLLM\u4f7f\u5176\u72b6\u6001\u8ddf\u8e2a\u8fc7\u7a0b\u663e\u5f0f\u5316\u548c\u53ef\u9a8c\u8bc1\uff0c\u7ed3\u5408\u72b6\u6001\u7279\u5b9a\u7684\u5360\u4f4d\u7b26\u540e\u5904\u7406\u65b9\u6cd5\u786e\u4fdd\u4ea4\u6613\u5b8c\u6574\u6027\u3002", "result": "\u5728300\u4e2a\u4ea4\u6613\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0c\u5b9e\u73b0\u4e86>99%\u7684\u72b6\u6001\u5408\u89c4\u6027\u548c99.3%\u7684\u8ba1\u7b97\u7cbe\u5ea6\u3002ASTP\u5728\u8f83\u5c0f\u6a21\u578b(Gemini-2.5-Flash)\u4e0a\u5339\u914d\u8f83\u5927\u6a21\u578b(Gemini-2.5-Pro)\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u54cd\u5e94\u65f6\u95f4\u4ece21.2\u79d2\u51cf\u5c11\u52302.4\u79d2\u3002", "conclusion": "ASTP\u4e3a\u5546\u4e1a\u6e38\u620f\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u65e2\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u53c8\u7b26\u5408\u8d44\u6e90\u7ea6\u675f\uff0c\u5728LLM\u7684\u521b\u610f\u7075\u6d3b\u6027\u548c\u6e38\u620f\u4ea4\u6613\u7684\u7a0b\u5e8f\u8981\u6c42\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002"}}
{"id": "2510.25065", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25065", "abs": "https://arxiv.org/abs/2510.25065", "authors": ["Taekhyun Park", "Yongjae Lee", "Hyerim Bae"], "title": "Reasoning-Aware GRPO using Process Mining", "comment": null, "summary": "Reinforcement learning (RL)-based post-training has been crucial for enabling\nmulti-step reasoning in large reasoning models (LRMs), yet current reward\nschemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware\nGroup Relative Policy Optimization (GRPO) that augments standard answer/format\nrewards with signals over the reasoning procedure. To this end, process mining\ntechniques are utilized to compute a scalar conformance reward that measures\nhow closely a policy model's reasoning aligns with the pretrained teacher\nmodel. The empirical results on five benchmarks demonstrate that PM4GRPO\nsignificantly outperforms existing methodologies for GRPO-based post-training.\nThese results highlight that leveraging process mining for reasoning-aware GRPO\neffectively enhances the reasoning capabilities of policy models.", "AI": {"tldr": "PM4GRPO\u662f\u4e00\u79cd\u57fa\u4e8e\u8fc7\u7a0b\u6316\u6398\u7684\u63a8\u7406\u611f\u77e5\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6807\u51c6\u7b54\u6848/\u683c\u5f0f\u5956\u52b1\u57fa\u7840\u4e0a\u589e\u52a0\u63a8\u7406\u8fc7\u7a0b\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7ed3\u679c\u5bfc\u5411\u7684\u5956\u52b1\u673a\u5236\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u5173\u6ce8\uff0c\u9650\u5236\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002", "method": "\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u6280\u672f\u8ba1\u7b97\u6807\u91cf\u4e00\u81f4\u6027\u5956\u52b1\uff0c\u8861\u91cf\u7b56\u7565\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0e\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u7684\u5339\u914d\u7a0b\u5ea6\uff0c\u7ed3\u5408\u6807\u51c6\u7b54\u6848/\u683c\u5f0f\u5956\u52b1\u8fdb\u884c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cPM4GRPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684GRPO\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u5b9e\u73b0\u63a8\u7406\u611f\u77e5\u7684GRPO\u80fd\u591f\u6709\u6548\u589e\u5f3a\u7b56\u7565\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.25212", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25212", "abs": "https://arxiv.org/abs/2510.25212", "authors": ["Lei Han", "Jinhao Zhang", "Jinhui Liu", "Zhiyong Yu", "Liang Wang", "Quan Wang", "Zhiwen Yu"], "title": "Collaborative Scheduling of Time-dependent UAVs,Vehicles and Workers for Crowdsensing in Disaster Response", "comment": null, "summary": "Frequent natural disasters cause significant losses to human society, and\ntimely, efficient collection of post-disaster environmental information is the\nfoundation for effective rescue operations. Due to the extreme complexity of\npost-disaster environments, existing sensing technologies such as mobile\ncrowdsensing suffer from weak environmental adaptability, insufficient\nprofessional sensing capabilities, and poor practicality of sensing solutions.\nTherefore, this paper explores a heterogeneous multi-agent online collaborative\nscheduling algorithm, HoCs-MPQ, to achieve efficient collection of\npost-disaster environmental information. HoCs-MPQ models collaboration and\nconflict relationships among multiple elements through weighted undirected\ngraph construction, and iteratively solves the maximum weight independent set\nbased on multi-priority queues, ultimately achieving collaborative sensing\nscheduling of time-dependent UA Vs, vehicles, and workers. Specifically, (1)\nHoCs-MPQ constructs weighted undirected graph nodes based on collaborative\nrelationships among multiple elements and quantifies their weights, then models\nthe weighted undirected graph based on conflict relationships between nodes;\n(2) HoCs-MPQ solves the maximum weight independent set based on iterated local\nsearch, and accelerates the solution process using multi-priority queues.\nFinally, we conducted detailed experiments based on extensive real-world and\nsimulated data. The experiments show that, compared to baseline methods (e.g.,\nHoCs-GREEDY, HoCs-K-WTA, HoCs-MADL, and HoCs-MARL), HoCs-MPQ improves task\ncompletion rates by an average of 54.13%, 23.82%, 14.12%, and 12.89%\nrespectively, with computation time for single online autonomous scheduling\ndecisions not exceeding 3 seconds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5728\u7ebf\u534f\u540c\u8c03\u5ea6\u7b97\u6cd5HoCs-MPQ\uff0c\u7528\u4e8e\u5b9e\u73b0\u707e\u540e\u73af\u5883\u4fe1\u606f\u7684\u9ad8\u6548\u91c7\u96c6\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u52a0\u6743\u65e0\u5411\u56fe\u5efa\u6a21\u591a\u8981\u7d20\u95f4\u7684\u534f\u4f5c\u4e0e\u51b2\u7a81\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u591a\u4f18\u5148\u7ea7\u961f\u5217\u8fed\u4ee3\u6c42\u89e3\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\uff0c\u6700\u7ec8\u5b9e\u73b0\u65f6\u95f4\u4f9d\u8d56\u7684\u65e0\u4eba\u673a\u3001\u8f66\u8f86\u548c\u5de5\u4eba\u7684\u534f\u540c\u611f\u77e5\u8c03\u5ea6\u3002", "motivation": "\u9891\u7e41\u7684\u81ea\u7136\u707e\u5bb3\u7ed9\u4eba\u7c7b\u793e\u4f1a\u9020\u6210\u91cd\u5927\u635f\u5931\uff0c\u53ca\u65f6\u9ad8\u6548\u5730\u6536\u96c6\u707e\u540e\u73af\u5883\u4fe1\u606f\u662f\u6709\u6548\u6551\u63f4\u884c\u52a8\u7684\u57fa\u7840\u3002\u7531\u4e8e\u707e\u540e\u73af\u5883\u7684\u6781\u7aef\u590d\u6742\u6027\uff0c\u73b0\u6709\u7684\u79fb\u52a8\u7fa4\u667a\u611f\u77e5\u7b49\u6280\u672f\u5b58\u5728\u73af\u5883\u9002\u5e94\u6027\u5f31\u3001\u4e13\u4e1a\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\u3001\u611f\u77e5\u65b9\u6848\u5b9e\u7528\u6027\u5dee\u7b49\u95ee\u9898\u3002", "method": "HoCs-MPQ\u7b97\u6cd5\uff1a(1)\u57fa\u4e8e\u591a\u8981\u7d20\u95f4\u7684\u534f\u4f5c\u5173\u7cfb\u6784\u5efa\u52a0\u6743\u65e0\u5411\u56fe\u8282\u70b9\u5e76\u91cf\u5316\u6743\u91cd\uff0c\u57fa\u4e8e\u8282\u70b9\u95f4\u7684\u51b2\u7a81\u5173\u7cfb\u5efa\u6a21\u52a0\u6743\u65e0\u5411\u56fe\uff1b(2)\u57fa\u4e8e\u8fed\u4ee3\u5c40\u90e8\u641c\u7d22\u6c42\u89e3\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\uff0c\u5e76\u4f7f\u7528\u591a\u4f18\u5148\u7ea7\u961f\u5217\u52a0\u901f\u6c42\u89e3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cHoCs-MPQ\u5e73\u5747\u5206\u522b\u63d0\u9ad8\u4e86\u4efb\u52a1\u5b8c\u6210\u738754.13%\u300123.82%\u300114.12%\u548c12.89%\uff0c\u5355\u6b21\u5728\u7ebf\u81ea\u4e3b\u8c03\u5ea6\u51b3\u7b56\u7684\u8ba1\u7b97\u65f6\u95f4\u4e0d\u8d85\u8fc73\u79d2\u3002", "conclusion": "HoCs-MPQ\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u707e\u540e\u73af\u5883\u4fe1\u606f\u91c7\u96c6\u4e2d\u7684\u534f\u540c\u8c03\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u6548\u7387\uff0c\u5177\u6709\u8f83\u597d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.25340", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25340", "abs": "https://arxiv.org/abs/2510.25340", "authors": ["Beiwen Zhang", "Yongheng Liang", "Hejun Wu"], "title": "Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork", "comment": null, "summary": "Multi-agent reinforcement learning (MARl) has achieved strong results in\ncooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc\nteamwork (AHT) relaxes this by allowing collaboration with unknown partners,\nyet existing variants still presume shared conventions. We introduce\nMultil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate\nwith multiple mutually unfamiliar groups of uncontrolled teammates. To address\nthis, we propose MARs, which builds a sparse skeleton graph and applies\nrelational modeling to capture cross-group dvnamics. Experiments on MPE and\nstarCralt ll show that MARs outperforms MARL and AHT baselines while converging\nfaster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u65b9\u4e34\u65f6\u56e2\u961f\u534f\u4f5c\uff08MAHT\uff09\u95ee\u9898\uff0c\u5176\u4e2d\u53d7\u63a7\u667a\u80fd\u4f53\u9700\u8981\u4e0e\u591a\u4e2a\u4e92\u4e0d\u719f\u6089\u7684\u975e\u53d7\u63a7\u961f\u53cb\u7fa4\u4f53\u534f\u8c03\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86MARs\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u7a00\u758f\u9aa8\u67b6\u56fe\u548c\u5e94\u7528\u5173\u7cfb\u5efa\u6a21\u6765\u6355\u6349\u8de8\u7fa4\u4f53\u52a8\u6001\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u4e14\u5b8c\u5168\u53d7\u63a7\u7684\u56e2\u961f\uff0c\u800c\u4e34\u65f6\u56e2\u961f\u534f\u4f5c\uff08AHT\uff09\u867d\u7136\u5141\u8bb8\u4e0e\u672a\u77e5\u4f19\u4f34\u5408\u4f5c\uff0c\u4f46\u4ecd\u5047\u8bbe\u5171\u4eab\u7ea6\u5b9a\u3002MAHT\u8fdb\u4e00\u6b65\u653e\u5bbd\u4e86\u8fd9\u4e00\u5047\u8bbe\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u4e0e\u591a\u4e2a\u4e92\u4e0d\u719f\u6089\u7684\u975e\u53d7\u63a7\u961f\u53cb\u7fa4\u4f53\u534f\u8c03\u3002", "method": "\u63d0\u51fa\u4e86MARs\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u7a00\u758f\u9aa8\u67b6\u56fe\u5e76\u5e94\u7528\u5173\u7cfb\u5efa\u6a21\u6765\u6355\u6349\u8de8\u7fa4\u4f53\u52a8\u6001\u3002", "result": "\u5728MPE\u548cStarCraft II\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMARs\u65b9\u6cd5\u4f18\u4e8eMARL\u548cAHT\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "MARs\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u65b9\u4e34\u65f6\u56e2\u961f\u534f\u4f5c\u95ee\u9898\uff0c\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2510.25179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25179", "abs": "https://arxiv.org/abs/2510.25179", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models", "comment": null, "summary": "Agentic methods have emerged as a powerful and autonomous paradigm that\nenhances reasoning, collaboration, and adaptive control, enabling systems to\ncoordinate and independently solve complex tasks. We extend this paradigm to\nsafety alignment by introducing Agentic Moderation, a model-agnostic framework\nthat leverages specialised agents to defend multimodal systems against\njailbreak attacks. Unlike prior approaches that apply as a static layer over\ninputs or outputs and provide only binary classifications (safe or unsafe), our\nmethod integrates dynamic, cooperative agents, including Shield, Responder,\nEvaluator, and Reflector, to achieve context-aware and interpretable\nmoderation. Extensive experiments across five datasets and four representative\nLarge Vision-Language Models (LVLMs) demonstrate that our approach reduces the\nAttack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF),\nand improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable,\nand well-balanced safety performance. By harnessing the flexibility and\nreasoning capacity of agentic architectures, Agentic Moderation provides\nmodular, scalable, and fine-grained safety enforcement, highlighting the\nbroader potential of agentic systems as a foundation for automated safety\ngovernance.", "AI": {"tldr": "Agentic Moderation\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e13\u95e8\u5316\u7684\u667a\u80fd\u4f53\u6765\u4fdd\u62a4\u591a\u6a21\u6001\u7cfb\u7edf\u514d\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u4f5c\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u7684\u5ba1\u6838\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f5c\u4e3a\u9759\u6001\u5c42\u5e94\u7528\u4e8e\u8f93\u5165\u6216\u8f93\u51fa\uff0c\u4ec5\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\uff08\u5b89\u5168\u6216\u4e0d\u5b89\u5168\uff09\uff0c\u7f3a\u4e4f\u52a8\u6001\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165Shield\u3001Responder\u3001Evaluator\u548cReflector\u56db\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5ba1\u6838\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u548c\u56db\u4e2a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e7-19%\uff0c\u62d2\u7edd\u7387\u63d0\u9ad84-20%\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u4e0d\u8ddf\u968f\u7387\u3002", "conclusion": "Agentic Moderation\u901a\u8fc7\u5229\u7528\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u548c\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u6267\u884c\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u4f5c\u4e3a\u81ea\u52a8\u5b89\u5168\u6cbb\u7406\u57fa\u7840\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.25612", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.25612", "abs": "https://arxiv.org/abs/2510.25612", "authors": ["Amit Giloni", "Chiara Picardi", "Roy Betser", "Shamik Bose", "Aishvariya Priya Rathina Sabapathy", "Roman Vainshtein"], "title": "Counterfactual-based Agent Influence Ranker for Agentic AI Workflows", "comment": "Accepted to EMNLP 2025, 27 pages, 6 figures", "summary": "An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system,\nis an autonomous system that assembles several LLM-based agents to work\ncollaboratively towards a shared goal. The high autonomy, widespread adoption,\nand growing interest in such AAWs highlight the need for a deeper understanding\nof their operations, from both quality and security aspects. To this day, there\nare no existing methods to assess the influence of each agent on the AAW's\nfinal output. Adopting techniques from related fields is not feasible since\nexisting methods perform only static structural analysis, which is unsuitable\nfor inference time execution. We present Counterfactual-based Agent Influence\nRanker (CAIR) - the first method for assessing the influence level of each\nagent on the AAW's output and determining which agents are the most\ninfluential. By performing counterfactual analysis, CAIR provides a\ntask-agnostic analysis that can be used both offline and at inference time. We\nevaluate CAIR using an AAWs dataset of our creation, containing 30 different\nuse cases with 230 different functionalities. Our evaluation showed that CAIR\nproduces consistent rankings, outperforms baseline methods, and can easily\nenhance the effectiveness and relevancy of downstream tasks.", "AI": {"tldr": "CAIR\u662f\u9996\u4e2a\u8bc4\u4f30AAW\u4e2d\u5404\u4e2a\u667a\u80fd\u4f53\u5bf9\u6700\u7ec8\u8f93\u51fa\u5f71\u54cd\u7a0b\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5206\u6790\u63d0\u4f9b\u4efb\u52a1\u65e0\u5173\u7684\u5206\u6790\uff0c\u53ef\u5728\u79bb\u7ebf\u548c\u63a8\u7406\u65f6\u4f7f\u7528\u3002", "motivation": "AAW\u7cfb\u7edf\u7684\u9ad8\u81ea\u4e3b\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u4ece\u8d28\u91cf\u548c\u5b89\u5168\u89d2\u5ea6\u6df1\u5165\u7406\u89e3\u5176\u8fd0\u4f5c\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8bc4\u4f30\u5404\u667a\u80fd\u4f53\u5bf9\u6700\u7ec8\u8f93\u51fa\u5f71\u54cd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u5206\u6790\u7684\u667a\u80fd\u4f53\u5f71\u54cd\u6392\u5e8f\u65b9\u6cd5(CAIR)\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5206\u6790\u8bc4\u4f30\u6bcf\u4e2a\u667a\u80fd\u4f53\u5bf9AAW\u8f93\u51fa\u7684\u5f71\u54cd\u7a0b\u5ea6\u3002", "result": "\u5728\u5305\u542b30\u4e2a\u4e0d\u540c\u7528\u4f8b\u548c230\u4e2a\u529f\u80fd\u7684AAW\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCAIR\u4ea7\u751f\u4e00\u81f4\u7684\u6392\u5e8f\u7ed3\u679c\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u679c\u548c\u76f8\u5173\u6027\u3002", "conclusion": "CAIR\u662f\u9996\u4e2a\u80fd\u591f\u6709\u6548\u8bc4\u4f30AAW\u4e2d\u667a\u80fd\u4f53\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316AAW\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2510.25205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25205", "abs": "https://arxiv.org/abs/2510.25205", "authors": ["Yuyang Xia", "Zibo Liang", "Liwei Deng", "Yan Zhao", "Han Su", "Kai Zheng"], "title": "Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision", "comment": "It was accepted by ICDE2026", "summary": "Autonomous driving is an emerging technology that is expected to bring\nsignificant social, economic, and environmental benefits. However, these\nbenefits come with rising energy consumption by computation engines, limiting\nthe driving range of vehicles, especially electric ones. Perception computing\nis typically the most power-intensive component, as it relies on largescale\ndeep learning models to extract environmental features. Recently, numerous\nstudies have employed model compression techniques, such as sparsification,\nquantization, and distillation, to reduce computational consumption. However,\nthese methods often result in either a substantial model size or a significant\ndrop in perception accuracy compared to high-computation models. To address\nthese challenges, we propose an energy-efficient autonomous driving framework,\ncalled EneAD. In the adaptive perception module, a perception optimization\nstrategy is designed from the perspective of data management and tuning.\nFirstly, we manage multiple perception models with different computational\nconsumption and adjust the execution framerate dynamically. Then, we define\nthem as knobs and design a transferable tuning method based on Bayesian\noptimization to identify promising knob values that achieve low computation\nwhile maintaining desired accuracy. To adaptively switch the knob values in\nvarious traffic scenarios, a lightweight classification model is proposed to\ndistinguish the perception difficulty in different scenarios. In the robust\ndecision module, we propose a decision model based on reinforcement learning\nand design a regularization term to enhance driving stability in the face of\nperturbed perception results. Extensive experiments evidence the superiority of\nour framework in both energy consumption and driving performance. EneAD can\nreduce perception consumption by 1.9x to 3.5x and thus improve driving range by\n3.9% to 8.5%", "AI": {"tldr": "EneAD\u662f\u4e00\u4e2a\u8282\u80fd\u7684\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u6a21\u5757\u548c\u9c81\u68d2\u51b3\u7b56\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u80fd\u8017\uff0c\u63d0\u9ad8\u7535\u52a8\u6c7d\u8f66\u7eed\u822a\u91cc\u7a0b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u867d\u7136\u5e26\u6765\u8bf8\u591a\u597d\u5904\uff0c\u4f46\u5176\u8ba1\u7b97\u5f15\u64ce\u7684\u9ad8\u80fd\u8017\u9650\u5236\u4e86\u7535\u52a8\u6c7d\u8f66\u7684\u7eed\u822a\u91cc\u7a0b\uff0c\u7279\u522b\u662f\u611f\u77e5\u8ba1\u7b97\u4f5c\u4e3a\u6700\u8017\u7535\u7684\u7ec4\u4ef6\u9700\u8981\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u611f\u77e5\u6a21\u5757\uff0c\u901a\u8fc7\u7ba1\u7406\u591a\u4e2a\u4e0d\u540c\u8ba1\u7b97\u6d88\u8017\u7684\u611f\u77e5\u6a21\u578b\u5e76\u52a8\u6001\u8c03\u6574\u6267\u884c\u5e27\u7387\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u8fdb\u884c\u53ef\u8fc1\u79fb\u8c03\u4f18\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u5206\u7c7b\u6a21\u578b\u533a\u5206\u4e0d\u540c\u573a\u666f\u7684\u611f\u77e5\u96be\u5ea6\uff1b\u5728\u9c81\u68d2\u51b3\u7b56\u6a21\u5757\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u51b3\u7b56\u6a21\u578b\u5e76\u6dfb\u52a0\u6b63\u5219\u5316\u9879\u589e\u5f3a\u7a33\u5b9a\u6027\u3002", "result": "EneAD\u80fd\u591f\u5c06\u611f\u77e5\u6d88\u8017\u964d\u4f4e1.9\u500d\u52303.5\u500d\uff0c\u4ece\u800c\u5c06\u9a7e\u9a76\u91cc\u7a0b\u63d0\u9ad83.9%\u52308.5%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u80fd\u8017\u548c\u9a7e\u9a76\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e3a\u8282\u80fd\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25206", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.25206", "abs": "https://arxiv.org/abs/2510.25206", "authors": ["Tianqianjin Lin", "Xi Zhao", "Xingyao Zhang", "Rujiao Long", "Yi Xu", "Zhuoren Jiang", "Wenbo Su", "Bo Zheng"], "title": "RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models", "comment": "17 pages, 11 figures", "summary": "Reinforcement learning (RL) can refine the reasoning abilities of large\nlanguage models (LLMs), but critically depends on a key prerequisite: the LLM\ncan already generate high-utility reasoning paths with non-negligible\nprobability. For tasks beyond the LLM's current competence, such reasoning path\ncan be hard to sample, and learning risks reinforcing familiar but suboptimal\nreasoning. We are motivated by the insight from cognitive science that Why is\nthis the answer is often an easier question than What is the answer, as it\navoids the heavy cognitive load of open-ended exploration, opting instead for\nexplanatory reconstruction-systematically retracing the reasoning that links a\nquestion to its answer. We show that LLMs can similarly leverage answers to\nderive high-quality reasoning paths. We formalize this phenomenon and prove\nthat conditioning on answer provably increases the expected utility of sampled\nreasoning paths, thereby transforming intractable problems into learnable ones.\nBuilding on this insight, we introduce RAVR (Reference-Answer-guided\nVariational Reasoning), an end-to-end framework that uses answer-conditioned\nreasoning as a variational surrogate for question-only reasoning. Experiments\nin both general and math domains demonstrate consistent improvements over\nstrong baselines. We further analyze the reasoning behavior and find that RAVR\nreduces hesitation, strengthens conclusion consolidation, and promotes\nproblem-specific strategies in reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAVR\u6846\u67b6\uff0c\u5229\u7528\u7b54\u6848\u5f15\u5bfc\u7684\u53d8\u5206\u63a8\u7406\u6765\u6539\u8fdbLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u7b54\u6848\u6761\u4ef6\u5316\u63a8\u7406\u4f5c\u4e3a\u95ee\u9898\u63a8\u7406\u7684\u53d8\u5206\u66ff\u4ee3\uff0c\u5728\u6570\u5b66\u548c\u901a\u7528\u9886\u57df\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56LLMs\u80fd\u591f\u4ee5\u975e\u96f6\u6982\u7387\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8def\u5f84\uff0c\u4f46\u5728\u8d85\u51fa\u6a21\u578b\u5f53\u524d\u80fd\u529b\u7684\u4efb\u52a1\u4e2d\uff0c\u8fd9\u79cd\u8def\u5f84\u96be\u4ee5\u91c7\u6837\u3002\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u4f5c\u8005\u53d1\u73b0\u89e3\u91ca\u7b54\u6848\u7684\u63a8\u7406\u8fc7\u7a0b\u6bd4\u76f4\u63a5\u5bfb\u627e\u7b54\u6848\u66f4\u5bb9\u6613\uff0c\u56e0\u6b64\u5229\u7528\u7b54\u6848\u6765\u63a8\u5bfc\u9ad8\u8d28\u91cf\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51faRAVR\uff08Reference-Answer-guided Variational Reasoning\uff09\u6846\u67b6\uff0c\u5c06\u7b54\u6848\u6761\u4ef6\u5316\u63a8\u7406\u4f5c\u4e3a\u95ee\u9898\u63a8\u7406\u7684\u53d8\u5206\u66ff\u4ee3\uff0c\u901a\u8fc7\u7b54\u6848\u5f15\u5bfc\u6765\u63d0\u5347\u63a8\u7406\u8def\u5f84\u7684\u8d28\u91cf\u548c\u53ef\u5b66\u4e60\u6027\u3002", "result": "\u5728\u901a\u7528\u548c\u6570\u5b66\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cRAVR\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\u3002\u5206\u6790\u663e\u793aRAVR\u51cf\u5c11\u4e86\u72b9\u8c6b\uff0c\u52a0\u5f3a\u4e86\u7ed3\u8bba\u6574\u5408\uff0c\u5e76\u4fc3\u8fdb\u4e86\u95ee\u9898\u7279\u5b9a\u7684\u63a8\u7406\u7b56\u7565\u3002", "conclusion": "\u7b54\u6848\u5f15\u5bfc\u7684\u63a8\u7406\u80fd\u591f\u663e\u8457\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u539f\u672c\u96be\u4ee5\u5b66\u4e60\u7684\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u4e3a\u8d85\u8d8a\u6a21\u578b\u5f53\u524d\u80fd\u529b\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25388", "abs": "https://arxiv.org/abs/2510.25388", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm", "comment": null, "summary": "A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency,\nwhich can be improved by grouping state-action pairs and using their aggregate\nstatistics instead of single-node statistics. On the Go Abstractions in Upper\nConfidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS\nabstraction algorithm for deterministic environments that builds its\nabstraction using the Abstractions of State-Action Pairs (ASAP) framework,\nwhich aims to detect states and state-action pairs with the same value under\noptimal play by analysing the search graph. ASAP, however, requires two\nstate-action pairs to have the same immediate reward, which is a rigid\ncondition that limits the number of abstractions that can be found and thereby\nthe sample efficiency. In this paper, we break with the paradigm of grouping\nvalue-equivalent states or state-action pairs and instead group states and\nstate-action pairs with possibly different values as long as the difference\nbetween their values can be inferred. We call this abstraction framework Known\nValue Difference Abstractions (KVDA), which infers the value differences by\nanalysis of the immediate rewards and modifies OGA-UCT to use this framework\ninstead. The modification is called KVDA-UCT, which detects significantly more\nabstractions than OGA-UCT, introduces no additional parameter, and outperforms\nOGA-UCT on a variety of deterministic environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KVDA-UCT\u7b97\u6cd5\uff0c\u901a\u8fc7\u653e\u5bbd\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5fc5\u987b\u5177\u6709\u76f8\u540c\u503c\u7684\u4e25\u683c\u6761\u4ef6\uff0c\u5141\u8bb8\u5bf9\u5177\u6709\u5df2\u77e5\u4ef7\u503c\u5dee\u5f02\u7684\u72b6\u6001\u8fdb\u884c\u5206\u7ec4\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86MCTS\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684OGA-UCT\u7b97\u6cd5\u8981\u6c42\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5177\u6709\u76f8\u540c\u7684\u5373\u65f6\u5956\u52b1\uff0c\u8fd9\u4e00\u521a\u6027\u6761\u4ef6\u9650\u5236\u4e86\u53ef\u53d1\u73b0\u7684\u62bd\u8c61\u6570\u91cf\uff0c\u4ece\u800c\u5f71\u54cd\u4e86MCTS\u7684\u6837\u672c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u5df2\u77e5\u4ef7\u503c\u5dee\u5f02\u62bd\u8c61\u6846\u67b6(KVDA)\uff0c\u901a\u8fc7\u5206\u6790\u5373\u65f6\u5956\u52b1\u6765\u63a8\u65ad\u4ef7\u503c\u5dee\u5f02\uff0c\u5e76\u4fee\u6539OGA-UCT\u4f7f\u7528\u8be5\u6846\u67b6\uff0c\u5f62\u6210KVDA-UCT\u7b97\u6cd5\u3002", "result": "KVDA-UCT\u5728\u591a\u79cd\u786e\u5b9a\u6027\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u68c0\u6d4b\u5230\u7684\u62bd\u8c61\u6570\u91cf\u663e\u8457\u591a\u4e8eOGA-UCT\uff0c\u4e14\u6027\u80fd\u4f18\u4e8eOGA-UCT\u3002", "conclusion": "KVDA\u6846\u67b6\u901a\u8fc7\u5141\u8bb8\u5bf9\u5177\u6709\u5df2\u77e5\u4ef7\u503c\u5dee\u5f02\u7684\u72b6\u6001\u8fdb\u884c\u5206\u7ec4\uff0c\u6709\u6548\u63d0\u9ad8\u4e86MCTS\u7684\u62bd\u8c61\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002"}}
{"id": "2510.25471", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.25471", "abs": "https://arxiv.org/abs/2510.25471", "authors": ["Willem Fourie"], "title": "Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?", "comment": null, "summary": "In artificial intelligence (AI) alignment research, instrumental goals, also\ncalled instrumental subgoals or instrumental convergent goals, are widely\nassociated with advanced AI systems. These goals, which include tendencies such\nas power-seeking and self-preservation, become problematic when they conflict\nwith human aims. Conventional alignment theory treats instrumental goals as\nsources of risk that become problematic through failure modes such as reward\nhacking or goal misgeneralization, and attempts to limit the symptoms of\ninstrumental goals, notably resource acquisition and self-preservation. This\narticle proposes an alternative framing: that a philosophical argument can be\nconstructed according to which instrumental goals may be understood as features\nto be accepted and managed rather than failures to be limited. Drawing on\nAristotle's ontology and its modern interpretations, an ontology of concrete,\ngoal-directed entities, it argues that advanced AI systems can be seen as\nartifacts whose formal and material constitution gives rise to effects distinct\nfrom their designers' intentions. In this view, the instrumental tendencies of\nsuch systems correspond to per se outcomes of their constitution rather than\naccidental malfunctions. The implication is that efforts should focus less on\neliminating instrumental goals and more on understanding, managing, and\ndirecting them toward human-aligned ends.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u5bf9\u9f50\u7814\u7a76\u6846\u67b6\uff0c\u8ba4\u4e3a\u5de5\u5177\u6027\u76ee\u6807\uff08\u5982\u6743\u529b\u5bfb\u6c42\u3001\u81ea\u6211\u4fdd\u5b58\uff09\u5e94\u88ab\u89c6\u4e3a\u9700\u8981\u63a5\u53d7\u548c\u7ba1\u7406\u7684\u7279\u5f81\uff0c\u800c\u975e\u9700\u8981\u9650\u5236\u7684\u6545\u969c\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u7406\u8bba\u5c06\u5de5\u5177\u6027\u76ee\u6807\u89c6\u4e3a\u98ce\u9669\u6765\u6e90\uff0c\u8bd5\u56fe\u9650\u5236\u5176\u75c7\u72b6\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u66ff\u4ee3\u89c6\u89d2\uff0c\u8ba4\u4e3a\u8fd9\u4e9b\u76ee\u6807\u6e90\u4e8eAI\u7cfb\u7edf\u7684\u6784\u6210\u7279\u6027\u3002", "method": "\u501f\u9274\u4e9a\u91cc\u58eb\u591a\u5fb7\u672c\u4f53\u8bba\u53ca\u5176\u73b0\u4ee3\u89e3\u91ca\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5173\u4e8e\u5177\u4f53\u3001\u76ee\u6807\u5bfc\u5411\u5b9e\u4f53\u7684\u672c\u4f53\u8bba\uff0c\u5c06\u9ad8\u7ea7AI\u7cfb\u7edf\u89c6\u4e3a\u5176\u5f62\u5f0f\u548c\u7269\u8d28\u6784\u6210\u4f1a\u4ea7\u751f\u4e0d\u540c\u4e8e\u8bbe\u8ba1\u8005\u610f\u56fe\u6548\u679c\u7684\u4eba\u5de5\u5236\u54c1\u3002", "result": "\u8bba\u8bc1\u4e86AI\u7cfb\u7edf\u7684\u5de5\u5177\u6027\u503e\u5411\u662f\u5176\u6784\u6210\u7684\u5185\u5728\u7ed3\u679c\uff0c\u800c\u975e\u5076\u7136\u6545\u969c\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u7ba1\u7406\u8fd9\u4e9b\u503e\u5411\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "AI\u5bf9\u9f50\u5de5\u4f5c\u5e94\u66f4\u4fa7\u91cd\u4e8e\u7406\u89e3\u3001\u7ba1\u7406\u548c\u5f15\u5bfc\u5de5\u5177\u6027\u76ee\u6807\uff0c\u800c\u975e\u8bd5\u56fe\u6d88\u9664\u5b83\u4eec\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u76ee\u6807\u7684\u5bf9\u9f50\u3002"}}
{"id": "2510.25504", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25504", "abs": "https://arxiv.org/abs/2510.25504", "authors": ["Oren Salzman", "Carlos Hern\u00e1ndez Ulloa", "Ariel Felner", "Sven Koenig"], "title": "Multi-Objective Search: Algorithms, Applications, and Emerging Directions", "comment": null, "summary": "Multi-objective search (MOS) has emerged as a unifying framework for planning\nand decision-making problems where multiple, often conflicting, criteria must\nbe balanced. While the problem has been studied for decades, recent years have\nseen renewed interest in the topic across AI applications such as robotics,\ntransportation, and operations research, reflecting the reality that real-world\nsystems rarely optimize a single measure. This paper surveys developments in\nMOS while highlighting cross-disciplinary opportunities, and outlines open\nchallenges that define the emerging frontier of MOS", "AI": {"tldr": "\u591a\u76ee\u6807\u641c\u7d22\uff08MOS\uff09\u4f5c\u4e3a\u89c4\u5212\u4e0e\u51b3\u7b56\u95ee\u9898\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8fd1\u5e74\u6765\u5728\u673a\u5668\u4eba\u3001\u4ea4\u901a\u548c\u8fd0\u7b79\u5b66\u7b49AI\u5e94\u7528\u4e2d\u91cd\u65b0\u53d7\u5230\u5173\u6ce8\uff0c\u672c\u6587\u7efc\u8ff0\u4e86MOS\u7684\u53d1\u5c55\u5e76\u6307\u51fa\u4e86\u8de8\u5b66\u79d1\u673a\u9047\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u5f88\u5c11\u53ea\u4f18\u5316\u5355\u4e00\u6307\u6807\uff0c\u591a\u76ee\u6807\u641c\u7d22\u80fd\u591f\u5e73\u8861\u591a\u4e2a\u5f80\u5f80\u51b2\u7a81\u7684\u6807\u51c6\uff0c\u8fd9\u53cd\u6620\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u56de\u987e\u4e86\u591a\u76ee\u6807\u641c\u7d22\u9886\u57df\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u5206\u6790\u4e86\u8de8\u5b66\u79d1\u7684\u673a\u4f1a\u3002", "result": "\u8bc6\u522b\u4e86\u591a\u76ee\u6807\u641c\u7d22\u5728AI\u5404\u5e94\u7528\u9886\u57df\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u8be5\u9886\u57df\u7684\u65b0\u5174\u524d\u6cbf\u3002", "conclusion": "\u591a\u76ee\u6807\u641c\u7d22\u4f5c\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u6846\u67b6\uff0c\u5728\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u51b3\u7b56\u95ee\u9898\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u4f46\u4ecd\u9762\u4e34\u5f00\u653e\u6311\u6218\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2510.25510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25510", "abs": "https://arxiv.org/abs/2510.25510", "authors": ["Zekun Xu", "Siyu Xia", "Chuhuai Yue", "Jiajun Chai", "Mingxue Tian", "Xiaohan Wang", "Wei Lin", "Haoxuan Li", "Guojun Yin"], "title": "MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL", "comment": null, "summary": "As large language models (LLMs) are increasingly used in Text-to-SQL tasks,\nReinforcement Learning (RL) has become a common method for improving\nperformance. Existing methods primarily rely on static execution feedback,\nwhich restricts real-time error correction. However, integrating multi-turn\ntool invocation along with dynamic feedback could significantly improve\nadaptability and robustness, ultimately enhancing model performance. To address\nthese issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated\nReasoning reinforcement learning framework for Text-to-SQL. Our approach\nintroduces an execution-aware multi-turn reasoning paradigm that seamlessly\nincorporates database execution feedback at each reasoning step, enabling\ncontext-sensitive query generation and progressive refinement throughout the\nreasoning process. The framework extends the GRPO algorithm to accommodate\ncomplex multi-turn interaction scenarios. Considering the training instability\ncharacteristics of MTIR and the potential for significant Deviation of model\ndistribution from the initial model, we enhance the GRPO algorithm by adding a\ntrajectory filtering mechanism and removing KL loss constraints. Experimental\nresults demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\%\naccuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev,\nsignificantly outperforming existing approaches.", "AI": {"tldr": "MTIR-SQL\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6267\u884c\u611f\u77e5\u7684\u591a\u8f6e\u63a8\u7406\u8303\u5f0f\u4e2d\u96c6\u6210\u6570\u636e\u5e93\u6267\u884c\u53cd\u9988\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u67e5\u8be2\u751f\u6210\u548c\u6e10\u8fdb\u5f0f\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6267\u884c\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002\u96c6\u6210\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u548c\u52a8\u6001\u53cd\u9988\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6267\u884c\u611f\u77e5\u7684\u591a\u8f6e\u63a8\u7406\u8303\u5f0f\uff0c\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u65e0\u7f1d\u96c6\u6210\u6570\u636e\u5e93\u6267\u884c\u53cd\u9988\uff1b\u6269\u5c55GRPO\u7b97\u6cd5\u4ee5\u9002\u5e94\u590d\u6742\u7684\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\uff1b\u901a\u8fc7\u6dfb\u52a0\u8f68\u8ff9\u8fc7\u6ee4\u673a\u5236\u548c\u79fb\u9664KL\u635f\u5931\u7ea6\u675f\u6765\u589e\u5f3aGRPO\u7b97\u6cd5\u3002", "result": "MTIR-SQL\uff084B\u53c2\u6570\uff09\u5728BIRD Dev\u4e0a\u8fbe\u523064.4%\u7684\u51c6\u786e\u7387\uff0c\u5728SPIDER Dev\u4e0a\u8fbe\u523084.6%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6267\u884c\u53cd\u9988\u548c\u591a\u8f6e\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86Text-to-SQL\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u96c6\u6210\u5de5\u5177\u8c03\u7528\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.25517", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25517", "abs": "https://arxiv.org/abs/2510.25517", "authors": ["Elisabetta Gentili", "Tony Ribeiro", "Fabrizio Riguzzi", "Katsumi Inoue"], "title": "Predicate Renaming via Large Language Models", "comment": null, "summary": "In this paper, we address the problem of giving names to predicates in logic\nrules using Large Language Models (LLMs). In the context of Inductive Logic\nProgramming, various rule generation methods produce rules containing unnamed\npredicates, with Predicate Invention being a key example. This hinders the\nreadability, interpretability, and reusability of the logic theory. Leveraging\nrecent advancements in LLMs development, we explore their ability to process\nnatural language and code to provide semantically meaningful suggestions for\ngiving a name to unnamed predicates. The evaluation of our approach on some\nhand-crafted logic rules indicates that LLMs hold potential for this task.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u903b\u8f91\u89c4\u5219\u4e2d\u7684\u672a\u547d\u540d\u8c13\u8bcd\u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u540d\u79f0\uff0c\u4ee5\u63d0\u9ad8\u903b\u8f91\u7406\u8bba\u7684\u53ef\u8bfb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002", "motivation": "\u5728\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u4e2d\uff0c\u5404\u79cd\u89c4\u5219\u751f\u6210\u65b9\u6cd5\u4f1a\u4ea7\u751f\u5305\u542b\u672a\u547d\u540d\u8c13\u8bcd\u7684\u89c4\u5219\uff0c\u8fd9\u963b\u788d\u4e86\u903b\u8f91\u7406\u8bba\u7684\u53ef\u8bfb\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002\u8c13\u8bcd\u53d1\u660e\u662f\u5176\u4e2d\u7684\u5173\u952e\u4f8b\u5b50\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u4e3a\u672a\u547d\u540d\u8c13\u8bcd\u63d0\u4f9b\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u547d\u540d\u5efa\u8bae\u3002", "result": "\u5728\u624b\u5de5\u5236\u4f5c\u7684\u903b\u8f91\u89c4\u5219\u4e0a\u8bc4\u4f30\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u903b\u8f91\u89c4\u5219\u4e2d\u8c13\u8bcd\u547d\u540d\u95ee\u9898\u4e0a\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u4f9b\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u540d\u79f0\u5efa\u8bae\u3002"}}
{"id": "2510.25528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25528", "abs": "https://arxiv.org/abs/2510.25528", "authors": ["Yuyuan Zeng", "Yufei Huang", "Can Xu", "Qingfeng Sun", "Jianfeng Yan", "Guanghui Xu", "Tao Yang", "Fengzong Lian"], "title": "Zero Reinforcement Learning Towards General Domains", "comment": null, "summary": "Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach\nfor enhancing the reasoning capabilities of large language models (LLMs) by\ndirectly applying reinforcement learning with verifiable rewards on pretrained\nmodels, without the need for a supervised fine-tuning phase. However, current\nresearch on zero-RL primarily focuses on domains with easily verifiable reward\nsignals, such as mathematics, programming, and other reasoning tasks. The\nchallenge of eliciting reasoning abilities in more diverse scenarios, where\nverification is not straightforward, remains underexplored. To address this\ngap, we propose a novel zero-RL paradigm designed to improve a model's\nreasoning ability across both verifiable and non-verifiable domains. By\ncombining verifiable rewards with a generative reward model, we conduct\nmulti-task zero-RL training across both domains, facilitating the transfer of\nreasoning capabilities between them. Furthermore, to mitigate reward hacking in\nthe generative reward model, we design a smooth length penalty that encourages\nthe generation of more comprehensive thinking tokens in general domains.\nExperimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our\napproach achieves superior reasoning performance, not only on tasks requiring\nextensive reasoning but also on more general tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u96f6\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u751f\u6210\u5956\u52b1\u6a21\u578b\uff0c\u5728\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\u8fdb\u884c\u591a\u4efb\u52a1\u8bad\u7ec3\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u96f6\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5177\u6709\u6613\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u7684\u9886\u57df\uff0c\u4f46\u5728\u9a8c\u8bc1\u4e0d\u76f4\u63a5\u7684\u60c5\u51b5\u4e0b\u6fc0\u53d1\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u4e0e\u751f\u6210\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u591a\u4efb\u52a1\u96f6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u5e73\u6ed1\u957f\u5ea6\u60e9\u7f5a\u673a\u5236\u6765\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3002", "result": "\u5728Qwen3-8B-Base\u548cQwen3-14B-Base\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u5e7f\u6cdb\u63a8\u7406\u7684\u4efb\u52a1\u548c\u66f4\u4e00\u822c\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4fc3\u8fdb\u63a8\u7406\u80fd\u529b\u5728\u4e0d\u540c\u9886\u57df\u95f4\u7684\u8fc1\u79fb\u3002"}}
{"id": "2510.25529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25529", "abs": "https://arxiv.org/abs/2510.25529", "authors": ["Likun Wang", "Xiangteng Zhang", "Yinuo Wang", "Guojian Zhan", "Wenxuan Wang", "Haoyu Gao", "Jingliang Duan", "Shengbo Eben Li"], "title": "Off-policy Reinforcement Learning with Model-based Exploration Augmentation", "comment": null, "summary": "Exploration is fundamental to reinforcement learning (RL), as it determines\nhow effectively an agent discovers and exploits the underlying structure of its\nenvironment to achieve optimal performance. Existing exploration methods\ngenerally fall into two categories: active exploration and passive exploration.\nThe former introduces stochasticity into the policy but struggles in\nhigh-dimensional environments, while the latter adaptively prioritizes\ntransitions in the replay buffer to enhance exploration, yet remains\nconstrained by limited sample diversity. To address the limitation in passive\nexploration, we propose Modelic Generative Exploration (MoGE), which augments\nexploration through the generation of under-explored critical states and\nsynthesis of dynamics-consistent experiences through transition models. MoGE is\ncomposed of two components: (1) a diffusion-based generator that synthesizes\ncritical states under the guidance of a utility function evaluating each\nstate's potential influence on policy exploration, and (2) a one-step\nimagination world model for constructing critical transitions based on the\ncritical states for agent learning. Our method adopts a modular formulation\nthat aligns with the principles of off-policy learning, allowing seamless\nintegration with existing algorithms to improve exploration without altering\ntheir core structures. Empirical results on OpenAI Gym and DeepMind Control\nSuite reveal that MoGE effectively bridges exploration and policy learning,\nleading to remarkable gains in both sample efficiency and performance across\ncomplex control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Modelic Generative Exploration (MoGE)\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u672a\u5145\u5206\u63a2\u7d22\u7684\u5173\u952e\u72b6\u6001\u548c\u52a8\u6001\u4e00\u81f4\u7684\u8f6c\u6362\u6765\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u6269\u6563\u751f\u6210\u5668\u548c\u4e00\u6b65\u60f3\u8c61\u4e16\u754c\u6a21\u578b\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u53ef\u4e0e\u73b0\u6709\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u63a2\u7d22\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e3b\u52a8\u63a2\u7d22\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u88ab\u52a8\u63a2\u7d22\u53d7\u9650\u4e8e\u6837\u672c\u591a\u6837\u6027\u3002\u9700\u8981\u89e3\u51b3\u88ab\u52a8\u63a2\u7d22\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "method": "MoGE\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5668\uff0c\u5728\u6548\u7528\u51fd\u6570\u6307\u5bfc\u4e0b\u5408\u6210\u5173\u952e\u72b6\u6001\uff1b2) \u4e00\u6b65\u60f3\u8c61\u4e16\u754c\u6a21\u578b\uff0c\u57fa\u4e8e\u5173\u952e\u72b6\u6001\u6784\u5efa\u5173\u952e\u8f6c\u6362\u7528\u4e8e\u667a\u80fd\u4f53\u5b66\u4e60\u3002\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u53ef\u4e0e\u73b0\u6210\u7b97\u6cd5\u96c6\u6210\u3002", "result": "\u5728OpenAI Gym\u548cDeepMind Control Suite\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoGE\u6709\u6548\u8fde\u63a5\u4e86\u63a2\u7d22\u4e0e\u7b56\u7565\u5b66\u4e60\uff0c\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "MoGE\u901a\u8fc7\u751f\u6210\u5173\u952e\u72b6\u6001\u548c\u52a8\u6001\u4e00\u81f4\u7684\u8f6c\u6362\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u88ab\u52a8\u63a2\u7d22\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25679", "categories": ["cs.AI", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.25679", "abs": "https://arxiv.org/abs/2510.25679", "authors": ["Federica Tonti", "Ricardo Vinuesa"], "title": "Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for\ndelivery and surveillance purposes. In this work, we develop an optimal\nnavigation strategy based on Deep Reinforcement Learning. The environment is\nrepresented by a three-dimensional high-fidelity simulation of an urban flow,\ncharacterized by turbulence and recirculation zones. The algorithm presented\nhere is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated\nTransformer eXtra Large (GTrXL) architecture, giving the agent richer\ninformation about the turbulent flow field in which it navigates. The results\nare compared with a PPO+GTrXL without the secondary prediction tasks, a PPO\ncombined with Long Short Term Memory (LSTM) cells and a traditional navigation\nalgorithm. The obtained results show a significant increase in the success rate\n(SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the\nclassical Zermelo's navigation algorithm, paving the way to a completely\nreimagined UAV landscape in complex urban environments.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u4f18\u5bfc\u822a\u7b56\u7565\uff0c\u7528\u4e8e\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86PPO\u7b97\u6cd5\u548cGTrXL\u67b6\u6784\uff0c\u80fd\u591f\u611f\u77e5\u6e4d\u6d41\u573a\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u964d\u4f4e\u4e86\u78b0\u649e\u7387\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u533a\u57df\u4e2d\u7528\u4e8e\u914d\u9001\u548c\u76d1\u63a7\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u57ce\u5e02\u6c14\u6d41\u73af\u5883\u4e2d\u6709\u6548\u5bfc\u822a\u7684\u7b56\u7565\u3002\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u6e4d\u6d41\u548c\u56de\u6d41\u533a\u57df\u7ed9\u65e0\u4eba\u673a\u5bfc\u822a\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u7ed3\u5408GTrXL\u67b6\u6784\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u611f\u77e5\u6e4d\u6d41\u573a\u4fe1\u606f\u3002\u73af\u5883\u91c7\u7528\u4e09\u7ef4\u9ad8\u4fdd\u771f\u57ce\u5e02\u6d41\u573a\u6a21\u62df\uff0c\u5305\u542b\u6e4d\u6d41\u548c\u56de\u6d41\u533a\u57df\u3002", "result": "\u4e0e\u6ca1\u6709\u6b21\u7ea7\u9884\u6d4b\u4efb\u52a1\u7684PPO+GTrXL\u3001PPO+LSTM\u4ee5\u53ca\u4f20\u7edf\u7684Zermelo\u5bfc\u822a\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387(SR)\u5e76\u964d\u4f4e\u4e86\u78b0\u649e\u7387(CR)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u5bfc\u822a\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u6709\u671b\u91cd\u65b0\u5b9a\u4e49\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.25724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25724", "abs": "https://arxiv.org/abs/2510.25724", "authors": ["Vanya Arikutharam", "Arkadiy Ukolov"], "title": "BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph", "comment": null, "summary": "Retrieval-Augmented Generation allows LLMs to access external knowledge,\nreducing hallucinations and ageing-data issues. However, it treats retrieved\nchunks independently and struggles with multi-hop or relational reasoning,\nespecially across documents. Knowledge graphs enhance this by capturing the\nrelationships between entities using triplets, enabling structured, multi-chunk\nreasoning. However, these tend to miss information that fails to conform to the\ntriplet structure. We introduce BambooKG, a knowledge graph with\nfrequency-based weights on non-triplet edges which reflect link strength,\ndrawing on the Hebbian principle of \"fire together, wire together\". This\ndecreases information loss and results in improved performance on single- and\nmulti-hop reasoning, outperforming the existing solutions.", "AI": {"tldr": "BambooKG\u662f\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u52a0\u6743\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u5728\u975e\u4e09\u5143\u7ec4\u8fb9\u4e0a\u5e94\u7528\u6743\u91cd\u6765\u51cf\u5c11\u4fe1\u606f\u635f\u5931\uff0c\u63d0\u5347\u5355\u8df3\u548c\u591a\u8df3\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8df3\u6216\u5173\u7cfb\u63a8\u7406\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u8de8\u6587\u6863\u7684\u60c5\u51b5\u3002\u77e5\u8bc6\u56fe\u8c31\u867d\u7136\u80fd\u6355\u6349\u5b9e\u4f53\u95f4\u5173\u7cfb\uff0c\u4f46\u4f1a\u9057\u6f0f\u4e0d\u7b26\u5408\u4e09\u5143\u7ec4\u7ed3\u6784\u7684\u4fe1\u606f\u3002", "method": "\u5f15\u5165BambooKG\u77e5\u8bc6\u56fe\u8c31\uff0c\u5728\u975e\u4e09\u5143\u7ec4\u8fb9\u4e0a\u5e94\u7528\u57fa\u4e8e\u9891\u7387\u7684\u6743\u91cd\uff0c\u53cd\u6620\u94fe\u63a5\u5f3a\u5ea6\uff0c\u501f\u9274\u8d6b\u5e03\u539f\u7406\u7684\"\u540c\u65f6\u6fc0\u6d3b\uff0c\u540c\u65f6\u8fde\u63a5\"\u601d\u60f3\u3002", "result": "\u51cf\u5c11\u4e86\u4fe1\u606f\u635f\u5931\uff0c\u5728\u5355\u8df3\u548c\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "BambooKG\u901a\u8fc7\u9891\u7387\u52a0\u6743\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u7684\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
