<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 13]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: 提出了FILO2^x并行共享内存架构，用于协同优化带容量约束的车辆路径问题，无需显式分解且同步开销最小。这是FILO2算法的单轨迹并行适配版本，通过同时异步优化多个可能不相关的解区域来实现迭代级并行。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用可用计算资源，在保持类似最终解质量的同时，显著提升从数百到数十万客户规模实例的求解时间。

Method: 设计FILO2^x作为FILO2算法的并行版本，利用FILO2优化应用的局部性，多个求解器同时异步优化同一底层解的不同区域，形成迭代级并行。

Result: 计算结果表明，FILO2^x相比原始方法可以大幅提升求解时间，同时保持相似的最终解质量。

Conclusion: 尽管单线程FILO2算法已表现出高效率，但通过更好地利用计算资源，FILO2^x能够显著增强求解性能，适用于从数百到数十万客户规模的问题实例。

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [2] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: PSAP是一种预测性分片分配协议，通过动态智能分配账户和交易到分片来解决区块链分片中的负载不均衡问题，结合时间序列负载预测和安全约束强化学习，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 静态或启发式分片分配导致负载倾斜、拥塞和跨分片通信过多，削弱了分片的可扩展性优势。

Method: 集成时间序列负载预测模型和安全约束强化学习控制器，实现多块提前预测和自适应分片重配置，通过同步量化运行时和安全门控机制确保确定性推理。

Result: 在以太坊、NEAR和Hyperledger Fabric等异构数据集上，相比现有动态分片基线，吞吐量提升2倍，延迟降低35%，跨分片开销减少20%。

Conclusion: 预测性、确定性和安全感知的分片分配是下一代可扩展区块链系统的有前景方向。

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [3] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: AVS是一个为自动驾驶车辆设计的存储系统，通过计算与分层布局的协同设计，实现高效的数据存储和检索，包括模态感知的降维压缩、热冷数据分层存储和轻量级元数据索引。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆作为移动计算平台，每天产生大量异构数据（如14TB），需要支持第三方应用的高效数据存储和检索，而现有的车载数据记录器和存储系统无法满足这些需求。

Method: 设计了AVS系统，采用分层布局：模态感知的降维和压缩、热冷数据分层存储与每日归档、轻量级元数据层索引。在嵌入式硬件上使用真实L4自动驾驶轨迹进行验证。

Result: 原型系统实现了可预测的实时数据摄取、快速选择性检索，并在适度资源预算下显著减少了存储空间占用。

Conclusion: 该工作为将存储作为自动驾驶堆栈中的一等组件提供了观察和建议，并指出了向更可扩展和长期部署发展的下一步方向。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [4] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 本文提出了一个基于Julia的软件框架，能够自动动态生成静态调度和编译代码，通过扩展DAG调度理论并添加领域特定信息来实现优化，以量子电动力学中多粒子散射过程的矩阵元计算为例进行验证。


<details>
  <summary>Details</summary>
Motivation: 科学计算中的复杂问题通常包含具有不同计算需求的子任务，为了达到最优效率，需要分析每个子任务并将其调度到最适合的硬件上，同时考虑并行性、任务间依赖关系和设备间数据传输速度等因素。

Method: 使用有向无环图表示计算问题，扩展现有的DAG调度理论并添加领域特定计算信息，开发基于Julia的软件框架来自动动态生成静态调度和编译代码。

Result: 实现了能够利用给定机器上尽可能多硬件的调度系统，通过领域特定信息实现了原本不可能的优化。

Conclusion: 该框架通过结合理论扩展和领域特定信息，成功解决了复杂科学计算问题的优化调度问题，在量子电动力学计算示例中验证了方法的有效性。

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [5] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 本文提出了一个智能电网的骨干模型，用于测试电网的替代场景，通过分布式优化子系统实现生产和消费调度，同时保持灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 智能电网技术发展带来了复杂跨学科建模问题，传统计算方法难以解决，需要系统性的集成建模方法来模拟电力系统、能源市场、需求侧管理等资源。

Method: 采用分布式优化子系统的方法，构建智能电网骨干模型，模拟不同系统以在人类规模模型之前验证假设。

Result: 开发了一个能够测试电网替代场景的工具，实现了生产和消费调度，同时保持了系统的灵活性和可扩展性。

Conclusion: 该智能电网骨干模型为解决复杂电网仿真问题提供了有效工具，通过分布式优化方法成功实现了系统集成和调度功能。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [6] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 本文提出了一个集成EnergyPlus模拟、高性能计算和开放地理数据集的UBEM流程，用于估算意大利博洛尼亚建筑的能源需求。


<details>
  <summary>Details</summary>
Motivation: 城市建筑能源建模在理解和预测城市尺度能源消耗中发挥核心作用。

Method: 使用博洛尼亚开放数据门户的建筑足迹和高度信息，结合航空LiDAR测量；从区域建筑法规和欧洲TABULA数据库获取建筑材料、隔热特性和窗户性能等非几何属性；在Leonardo超级计算机上进行EnergyPlus模拟。

Result: 在30分钟内完成了约25,000栋建筑的模拟计算。

Conclusion: 该UBEM流程成功实现了大规模城市建筑能源需求的高效估算。

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [7] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 评估本地运行的小型语言模型(SLMs)在自动化事件分类中的可行性，与基于云的LLMs相比，解决了成本、延迟和机密性问题。


<details>
  <summary>Details</summary>
Motivation: SOC和CSIRT面临自动化事件分类的压力，但使用基于云的LLMs存在成本、延迟和机密性风险，因此研究本地执行的SLMs是否能满足需求。

Method: 评估了21个参数从1B到20B的模型，通过改变温度超参数，测量不同架构下的执行时间和精度。

Result: 温度对性能影响很小，而参数数量和GPU容量是决定性因素。

Conclusion: 本地执行的SLMs在自动化事件分类中具有可行性，性能主要取决于模型参数规模和计算资源。

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [8] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 该论文探讨了在太空中构建可扩展的机器学习计算系统，使用配备太阳能电池板、光学卫星间链路的卫星群和谷歌TPU芯片，旨在利用太阳能源为AI计算提供动力。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算需求的持续增长，需要寻找可持续的能源解决方案。太阳是太阳系中最大的能源，因此研究如何高效利用太阳能为未来AI基础设施提供动力具有重要意义。

Method: 提出使用卫星群构建太空计算系统，包括：配备太阳能电池板的卫星、使用自由空间光学的卫星间链路、谷歌TPU加速器芯片。卫星以紧密编队飞行（如81颗卫星组成1公里半径的集群），并采用高精度ML模型控制大规模星座。

Result: Trillium TPU经过辐射测试，在相当于5年任务寿命的总电离剂量下无永久性故障，仅出现比特翻转错误。发射成本分析显示到2030年代中期，LEO发射成本可能降至≤200美元/公斤。

Conclusion: 太空中的太阳能驱动AI计算系统是可行的，通过卫星群、光学通信和辐射硬化TPU的协同工作，可为未来AI基础设施提供可持续的能源解决方案。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [9] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 提出一个在混合高性能计算和云环境中高效运行的联邦学习框架，解决系统异构性、通信开销和资源调度等关键挑战，同时保持模型准确性和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和隐私感知AI系统的需求增长，联邦学习成为有前景的解决方案，同时高性能计算和云基础设施的结合带来了新的复杂性，特别是在处理异构硬件、通信限制和非均匀数据时。

Method: 开发了一个联邦学习框架，专门设计用于在混合HPC和云环境中高效运行，解决了系统异构性、通信开销和资源调度等关键挑战。

Result: 在混合测试平台上进行的实验表明，即使在非独立同分布数据分布和不同硬件条件下，系统在可扩展性、容错性和收敛性方面表现出色。

Conclusion: 这些结果突显了联邦学习作为在现代分布式计算环境中构建可扩展AI系统的实用方法的潜力。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [10] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: nFlows是一个NUMA感知的工作流执行运行时系统，用于在NUMA架构的HPC系统上建模、执行、模拟和验证数据密集型工作流的调度算法。


<details>
  <summary>Details</summary>
Motivation: 现代HPC系统普遍采用NUMA架构，包含多个NUMA域和异构内存区域，这增加了数据访问延迟的变异性，使任务和数据放置复杂化。然而，大多数工作流调度策略是为Grid或Cloud环境设计的，很少考虑NUMA感知。

Method: 开发了nFlows系统，支持构建模拟模型并在物理系统上直接执行，能够研究NUMA对调度的影响、设计NUMA感知算法、分析数据移动行为、识别性能瓶颈，并探索内存内工作流执行。

Result: 提出了nFlows系统的设计、实现和验证方法，该系统能够对NUMA架构HPC系统上的数据密集型工作流进行建模和调度算法研究。

Conclusion: nFlows填补了NUMA感知工作流调度在HPC系统中的空白，为研究NUMA效应、设计优化算法和分析性能提供了有效工具。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [11] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: PolarStore是一个用于云原生关系数据库的压缩共享存储系统，通过软硬件结合的压缩机制在保持性能的同时显著降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 云原生RDBMS虽然提供了弹性计算资源，但存储成本仍然是关键问题。现有压缩方法存在性能开销大或灵活性不足的权衡问题。

Method: 采用双层级压缩机制：硬件层使用PolarCSD进行存储内压缩，软件层进行轻量级压缩，并结合数据库导向的优化和压缩感知调度方案。

Result: 已在数千台存储服务器上部署，管理超过100PB数据，压缩比达到3.55，存储成本降低约60%，同时保持与未压缩集群相当的性能。

Conclusion: PolarStore成功解决了云原生RDBMS中压缩性能与灵活性的权衡问题，实现了显著的存储成本节约而不牺牲性能。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [12] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: MTMC是一个分层框架，通过将优化策略与实现细节解耦来解决GPU内核生成的挑战，使用强化学习指导轻量级LLM探索优化策略，并利用通用LLM逐步实现优化方案。


<details>
  <summary>Details</summary>
Motivation: 开发高性能GPU内核对AI和科学计算至关重要，但现有LLM方法在正确性和效率方面存在冲突限制，需要探索极其庞大的优化空间和实现代码空间。

Method: 提出Macro Thinking Micro Coding框架：Macro Thinking使用强化学习指导轻量级LLM学习语义优化策略；Micro Coding利用通用LLM逐步实现优化提案，避免全内核生成错误。

Result: 在KernelBench上，MTMC在Levels 1-2达到近100%准确率，Level 3达到70%准确率，比现有最佳方法提升50%以上，速度比LLM快7.3倍，比专家优化的PyTorch Eager内核快2.2倍。在TritonBench上达到59.64%准确率和34倍加速。

Conclusion: MTMC通过分层方法有效导航庞大的优化空间和复杂的实现细节，使LLM能够生成高性能GPU内核，在准确性和运行时间方面均表现出优越性能。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [13] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: Beluga是一种基于CXL技术的新型内存架构，使GPU和CPU能够通过CXL交换机访问共享的大规模内存池，显著降低LLM推理中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: LLM模型规模快速增长和对长上下文推理的需求使内存成为GPU加速服务系统的关键瓶颈。现有RDMA解决方案存在高延迟、复杂通信协议和同步开销等问题。

Method: 提出Beluga内存架构，利用CXL交换机实现GPU和CPU对共享内存池的原生load/store访问语义，并基于此设计Beluga-KVCache系统来管理LLM推理中的大规模KVCache。

Result: 与基于RDMA的解决方案相比，Beluga-KVCache在vLLM推理引擎中实现了89.6%的首令牌时间减少和7.35倍的吞吐量提升。

Conclusion: Beluga是首个通过CXL交换机使GPU直接访问大规模内存池的系统，为实现GPU低延迟共享访问海量内存资源迈出了重要一步。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [14] [Trust-Based Social Learning for Communication (TSLEC) Protocol Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.19562)
*Abraham Itzhak Weinberg*

Main category: cs.MA

TL;DR: TSLEC框架通过信任关系调节的显式社会学习，将多智能体系统中的涌现通信收敛时间减少23.9%，同时产生组合性协议并保持动态目标下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中的涌现通信通常通过独立学习实现，导致收敛缓慢且可能产生次优协议。

Method: 引入TSLEC框架，智能体通过学习的信任关系调节向同伴显式教授成功策略。

Result: 信任基社会学习将收敛所需回合数减少23.9%，产生组合性协议，在动态目标下保持高解码准确率，信任分数与教学质量强相关。

Conclusion: 显式社会学习从根本上加速了多智能体协调中的涌现通信。

Abstract: Emergent communication in multi-agent systems typically occurs through independent learning, resulting in slow convergence and potentially suboptimal protocols. We introduce TSLEC (Trust-Based Social Learning with Emergent Communication), a framework where agents explicitly teach successful strategies to peers, with knowledge transfer modulated by learned trust relationships. Through experiments with 100 episodes across 30 random seeds, we demonstrate that trust-based social learning reduces episodes-to-convergence by 23.9% (p < 0.001, Cohen's d = 1.98) compared to independent emergence, while producing compositional protocols (C = 0.38) that remain robust under dynamic objectives (Phi > 0.867 decoding accuracy). Trust scores strongly correlate with teaching quality (r = 0.743, p < 0.001), enabling effective knowledge filtering. Our results establish that explicit social learning fundamentally accelerates emergent communication in multi-agent coordination.

</details>


### [15] [An Adaptive, Data-Integrated Agent-Based Modeling Framework for Explainable and Contestable Policy Design](https://arxiv.org/abs/2511.19726)
*Roberto Garrone*

Main category: cs.MA

TL;DR: 本文提出了一个通用的自适应多智能体学习框架，整合了动态机制、信息论诊断、结构因果模型、先验生成方法和无监督行为识别，用于分析学习智能体和自适应控制如何共同影响系统轨迹。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统通常在反馈、适应和非平稳环境下运行，但许多模拟研究仍使用静态决策规则和固定控制参数，需要更全面的分析框架。

Method: 开发了包含四种动态机制（静态vs自适应智能体、固定vs自适应参数）、信息论诊断（熵率、统计复杂性、预测信息）、结构因果模型、先验生成方法和无监督行为识别方法的综合框架。

Result: 提供了一个领域无关的架构，用于系统比较非平衡、振荡或漂移动态中的稳定性、性能和可解释性。

Conclusion: 该框架为开发可解释和可争议的多智能体决策过程提供了结构化方法，包括数学定义、计算算子和实验设计模板。

Abstract: Multi-agent systems often operate under feedback, adaptation, and non-stationarity, yet many simulation studies retain static decision rules and fixed control parameters. This paper introduces a general adaptive multi-agent learning framework that integrates: (i) four dynamic regimes distinguishing static versus adaptive agents and fixed versus adaptive system parameters; (ii) information-theoretic diagnostics (entropy rate, statistical complexity, and predictive information) to assess predictability and structure; (iii) structural causal models for explicit intervention semantics; (iv) procedures for generating agent-level priors from aggregate or sample data; and (v) unsupervised methods for identifying emergent behavioral regimes. The framework offers a domain-neutral architecture for analyzing how learning agents and adaptive controls jointly shape system trajectories, enabling systematic comparison of stability, performance, and interpretability across non-equilibrium, oscillatory, or drifting dynamics. Mathematical definitions, computational operators, and an experimental design template are provided, yielding a structured methodology for developing explainable and contestable multi-agent decision processes.

</details>


### [16] [Complex Instruction Following with Diverse Style Policies in Football Games](https://arxiv.org/abs/2511.19885)
*Chenglu Sun,Shuo Shen,Haonan Hu,Wei Zhou,Chen Chen*

Main category: cs.MA

TL;DR: 本文提出了语言控制多样化风格策略（LCDSP），这是一种专门为复杂多智能体环境设计的新型LC-RL范式，通过多样化风格训练方法和风格解释器，能够理解并执行高级抽象指令。


<details>
  <summary>Details</summary>
Motivation: 尽管语言控制强化学习在基础领域和简单指令方面取得进展，但在复杂多智能体环境中理解和执行高级抽象指令（如足球比赛中的战术指令）仍面临重大挑战。

Method: LCDSP包含两个关键组件：多样化风格训练方法（DST）通过风格参数调节智能体行为来训练单一策略，风格解释器（SI）将高级语言指令快速准确地转换为相应的风格参数。

Result: 在复杂的5v5足球环境中进行的广泛实验表明，LCDSP能够有效理解抽象战术指令，并准确执行所需的多样化行为风格。

Conclusion: LCDSP展示了在复杂现实世界应用中的潜力，能够有效处理高级语言指令并生成多样化行为策略。

Abstract: Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探索使用可穿戴设备和AI方法监测慢性疼痛和阿片类药物使用障碍患者的疼痛峰值，发现机器学习模型预测准确率较高，但大语言模型在此领域表现有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和阿片类药物使用障碍是常见且相互关联的慢性疾病，目前缺乏基于证据的综合治疗方法。可穿戴设备有潜力监测复杂患者信息，为OUD和CP患者开发治疗方案。

Method: 使用可穿戴设备监测患者数据，应用多种AI方法（包括机器学习模型和大语言模型）分析疼痛峰值的临床相关性。

Result: 机器学习模型在预测疼痛峰值方面达到相对较高的准确率（>0.7），而大语言模型在提供疼痛峰值洞察方面表现有限。

Conclusion: 可穿戴设备的实时监测结合先进AI模型可以促进疼痛峰值的早期检测，支持个性化干预，但需要开发能够提供可操作洞察的大语言模型。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [18] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: 本文提出HeaRT推理引擎，用于AMS设计自动化，实现智能、自适应、类人设计优化，在40电路基准测试中准确率>97%，性能>98%，且运行成本仅为SOTA基线的0.5倍。


<details>
  <summary>Details</summary>
Motivation: 传统AI驱动的AMS设计自动化算法受限于对高质量数据集的依赖、跨架构可移植性差以及缺乏自适应机制。

Method: 提出HeaRT基础推理引擎，作为自动化循环的核心组件，实现智能自适应设计优化。

Result: HeaRT在40电路基准测试中推理准确率>97%，Pass@1性能>98%，运行成本仅为SOTA基线的0.5倍，在尺寸和拓扑设计适应任务中实现>3倍更快的收敛速度。

Conclusion: HeaRT是迈向智能自适应设计优化的重要一步，能够保持先前的设计意图，显著提升设计效率。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [19] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: FISCAL框架通过生成金融事实核查的合成数据，训练出轻量级验证器MiniCheck-FISCAL，在多个金融数据集上表现优异，接近大模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在金融应用中存在事实可靠性不足和计算效率低下的问题，需要开发更准确且高效的解决方案。

Method: 提出FISCAL框架生成金融合成数据，并训练MiniCheck-FISCAL轻量级验证器来核查数值金融声明。

Result: MiniCheck-FISCAL在多个金融数据集上超越GPT-3.5 Turbo和同类开源模型，接近Mixtral-8x22B等大模型的准确性，在外部数据集上可与GPT-4o和Claude-3.5相媲美。

Conclusion: 领域特定的合成数据结合高效微调，可使紧凑模型在金融AI中实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [20] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型(LLMs)在加速教育评估项目与内容标准对齐过程中的应用效果，通过三个实验测试了GPT模型在不同任务中的表现，发现LLMs能够显著减少人工审查负担并保持对齐准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的人工对齐审查虽然准确但速度慢且劳动密集，特别是在大型项目库中。本研究旨在探索LLMs是否能在不牺牲准确性的前提下加速这一过程。

Method: 使用超过12,000个项目-技能对，在K-5年级测试了三种LLMs(GPT-3.5 Turbo、GPT-4o-mini和GPT-4o)，涵盖三个任务：识别未对齐项目、从完整标准集中选择正确技能、在分类前缩小候选列表。

Result: GPT-4o-mini在识别对齐状态方面达到83-94%的准确率；数学领域表现强劲但阅读领域较低；预过滤候选技能显著改善结果，正确技能出现在前五建议中的概率超过95%。

Conclusion: LLMs特别是结合候选过滤策略时，能够显著减少项目审查的人工负担同时保持对齐准确性。建议开发混合流程，将基于LLM的筛选与模糊情况下的人工审查相结合，为持续项目验证和教学对齐提供可扩展解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [21] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: KOM是一个用于膝骨关节炎自动化评估、风险预测和治疗处方的多智能体系统，能够辅助临床医生并生成个性化管理计划，在实验中表现出优于通用大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎影响全球6亿多人，但个性化多学科干预需要大量医疗资源，在资源有限环境中难以实施。

Method: 开发KOM多智能体系统，自动化KOA评估、风险预测和治疗处方，基于患者个体特征、疾病状态、风险因素和禁忌症生成定制管理计划。

Result: 基准实验显示KOM在影像分析和处方生成方面优于多个通用大语言模型；随机三臂模拟研究表明KOM与临床医生协作可将诊断和规划时间减少38.5%，并提高治疗质量。

Conclusion: KOM有助于促进KOA的自动化管理，整合到临床工作流程中有潜力提高护理效率，其模块化架构也为开发其他慢性病的AI辅助管理系统提供了宝贵见解。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [22] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于评估指导的提示优化方法，通过建立系统化的提示评估框架和训练执行无关的评估器，实现可解释的、查询相关的提示重写，在多个数据集和模型上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要优化静态模板，在复杂动态场景中效果有限；查询相关方法依赖不稳定的文本反馈或黑盒奖励模型，提供弱且不可解释的优化信号；提示质量本身缺乏统一系统定义，导致评估信号碎片化不可靠。

Method: 首先建立性能导向的系统化提示评估框架，开发并微调执行无关的评估器直接预测多维度质量分数；评估器指导指标感知的优化器诊断失败模式并以可解释方式重写提示。

Result: 评估器在预测提示性能方面达到最强准确率；评估指导的优化在8个数据集和3个骨干模型上持续超越静态模板和查询相关基线方法。

Conclusion: 提出了统一的、基于指标的提示质量视角，证明评估指导的优化管道能在多样化任务中提供稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [23] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 本文提出了一种结合ω-正则目标与显式约束的强化学习方法，解决了传统标量奖励在表达复杂行为属性和安全性能权衡方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，表达能力有限，无法准确表达时间性、条件性或安全关键目标，且容易导致奖励黑客行为。同时，单一标量性能度量掩盖了在可接受风险水平下的安全性能权衡问题。

Method: 开发了一种基于线性规划的模型强化学习算法，将ω-正则目标与约束分开处理，在满足ω-正则约束阈值的前提下最大化目标满足概率，并建立了到约束极限平均问题的转换。

Result: 该算法在极限情况下能够生成一个策略，在满足ω-正则约束阈值的同时最大化ω-正则目标的满足概率，并保持了最优性保证。

Conclusion: 通过结合ω-正则目标和显式约束，该方法能够同时解决传统强化学习在表达能力和安全性能权衡方面的局限性，为复杂行为规范和安全关键应用提供了更强大的框架。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [24] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个用于创建轻量级交互式教育模拟的AI驱动框架，支持快速生成、跨平台嵌入和无代码定制，解决了传统模拟开发成本高、技术复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 传统教育模拟开发需要大量资源和技术专长，限制了其广泛应用。MicroSims旨在通过AI辅助生成、通用嵌入和无代码定制，降低教育模拟的创建门槛，促进教育公平。

Method: 采用标准化设计模式实现AI辅助生成，基于iframe架构提供通用嵌入和安全沙箱，使用透明可修改代码支持定制和教学透明度。

Result: 研究表明交互式模拟相比传统教学可将概念理解提升30-40%。MicroSims在保持这些优势的同时，解决了成本、技术复杂性和平台依赖等长期障碍。

Conclusion: MicroSims框架为教育公平和低成本智能交互式教科书开辟了新途径，使全球教育工作者能够按需创建定制化、与课程对齐的模拟，并为基于AI的自适应学习系统奠定基础。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [25] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 本研究将通用自我效能感量表(GSES)应用于10个大型语言模型，在四种任务条件下评估其模拟自我评估能力。结果显示模型自我评估高度稳定但普遍低于人类水平，且自我评估与真实能力不匹配，存在轻微高估倾向。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要关注任务准确性，而忽视了自我评估这一可靠智能的关键方面。本研究旨在系统评估LLMs的自我效能感及其与任务表现的关系。

Method: 采用10项通用自我效能感量表(GSES)，在10个LLMs上进行四种条件测试：无任务、计算推理、社会推理和摘要任务。分析自我评估的稳定性、准确性以及与任务表现的关系。

Result: 模型自我评估在不同条件下存在显著差异，总体得分低于人类标准。所有模型在计算和社会推理任务上表现完美，但摘要任务表现差异很大。自我评估不能可靠反映实际能力，存在轻微高估倾向。

Conclusion: 心理测量提示为理解LLM沟通行为提供了结构化洞察，但不能提供校准的性能估计。自我效能感较高的模型表现出更自信、拟人化的推理风格，而得分较低的模型则采用谨慎、去拟人化的解释方式。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [26] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练过程奖励模型，同时利用沙箱执行反馈定位和纠正错误步骤，在减少15%令牌消耗的同时超越现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于树搜索的代码生成方法难以有效评估中间算法步骤、无法定位和及时纠正错误步骤的问题，这些限制导致生成错误代码和增加计算成本。

Method: 提出RPM-MCTS方法：1) 使用知识检索作为过程奖励模型评估中间步骤；2) 在扩展阶段采用相似性过滤去除冗余节点；3) 利用沙箱执行反馈定位错误步骤并进行针对性纠正。

Result: 在四个公共代码生成基准测试上的广泛实验表明，RPM-MCTS优于当前最先进方法，同时实现约15%的令牌消耗减少。使用RPM-MCTS构建的数据对基础模型进行全微调可显著提升其代码能力。

Conclusion: RPM-MCTS是一种有效的代码生成方法，通过知识检索和沙箱反馈机制解决了中间步骤评估和错误纠正问题，在性能和效率方面均有显著提升。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [27] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱生成基准数据集的新方法，用于评估LLM输出的语义相似性方法，解决了现有基准在生成成本、领域适用性和等价定义方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM生成文本语义相似性的方法存在缺陷：可能更关注语法或词汇形式而非语义内容，现有基准依赖主观人工判断导致生成成本高，领域适用性有限，且语义等价定义不明确。

Method: 利用知识图谱生成语义相似或不同的自然语言陈述对，将不同对分为四个子类型，在四个不同领域（通用知识、生物医学、金融、生物学）生成基准数据集，并比较传统NLP评分和LLM作为评判者的语义相似性方法。

Result: 研究发现语义变化的子类型以及基准领域都会影响语义相似性方法的性能，没有一种方法始终表现最优。结果对使用LLM作为评判者检测文本语义内容具有重要启示。

Conclusion: 提出的基于知识图谱的基准生成方法有效解决了现有基准的局限性，揭示了语义相似性方法的性能受语义变化类型和领域影响，为LLM作为语义评判者的应用提供了重要参考。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [28] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个包含15种隐藏故障模式的系统级分类法，分析了LLM在真实应用中的失败模式，并探讨了评估与监控实践的差距，最后为构建可靠、可维护和成本感知的LLM系统提供了设计原则。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正被快速集成到决策支持工具和自动化工作流中，但它们在生产环境中的行为仍未被充分理解，其故障模式与传统机器学习模型有根本性差异。

Method: 提出了一个系统级的故障模式分类法，包含多步推理漂移、潜在不一致性、上下文边界退化、错误工具调用、版本漂移和成本驱动性能崩溃等15种隐藏故障模式。

Result: 分析了现有评估基准与生产需求之间的差距，发现现有基准主要测量知识和推理能力，但对稳定性、可重现性、漂移和工作流集成提供的信息有限。

Conclusion: 通过将LLM可靠性视为系统工程问题而非纯模型中心问题，为未来关于评估方法、AI系统鲁棒性和可靠LLM部署的研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [29] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 本文提出了一个简单有效的框架，在CPDC 2025竞赛中统一改进了GPU和API赛道。核心包括上下文工程（动态工具剪枝、角色裁剪）和GRPO训练（强化学习替代监督微调），最终在多个任务中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决对话系统中工具调用稳定性、执行可靠性和角色扮演指导的问题，同时缓解小样本过拟合，提升任务导向对话性能。

Method: 1. 上下文工程：动态工具剪枝和角色裁剪进行输入压缩，结合参数归一化和函数合并等后处理技术；2. GPU赛道采用GRPO训练，用强化学习替代监督微调，直接优化奖励信号。

Result: 团队在最终评估中排名：Task 2 API第1名，Task 1 API第2名，Task 3 API和GPU赛道均第3名，证明了方法的有效性。

Conclusion: 提出的框架通过上下文工程和GRPO训练有效提升了对话系统的性能，在CPDC 2025竞赛中取得了优异成绩，代码已公开。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [30] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个微导航经济测试平台，通过成本-收益分析评估自主配送机器人的商业可行性，揭示导航研究指标与商业部署之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准只关注任务成功率，忽视了商业部署所需的经济可行性，这对自主配送机器人的商业化至关重要。

Method: CostNav建模完整经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业参数，从缩小规模模拟扩展到现实配送场景。

Result: 基线模型达到43.0%的服务水平协议合规率，但商业上不可行：每次运行亏损30.009美元且无盈亏平衡点，碰撞导致的维护成本占运行成本的99.7%。

Conclusion: CostNav填补了导航研究与商业部署之间的鸿沟，为评估基于规则的导航、模仿学习和成本感知强化学习提供了基础，支持跨导航范式的经济权衡决策。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [31] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW框架通过构建和精炼经验学习知识库来优化LLM智能体，在保持计算效率的同时显著提升任务精度和减少API调用。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的智能体训练方法计算开销大，且生成的策略难以解释、适应或增量改进。需要一种更实用、可解释的智能体优化方法。

Method: 引入BREW框架，通过知识库构建和精炼来优化智能体。采用有效的记忆分区方法提高检索效率，使用任务评分器和行为准则学习洞察，并利用状态空间搜索确保鲁棒性。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等真实世界基准测试中，BREW实现了10-20%的任务精度提升，10-15%的API/工具调用减少，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: BREW将知识库确立为模块化、可控的智能体优化基础，提供了一种透明、可解释和可扩展的行为塑造机制，与将记忆视为静态上下文的先前工作形成鲜明对比。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [32] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文旨在澄清主动推理的概念，将其与自由能原理分离，提出在离散状态空间中实现主动推理的优化问题可以表述为约束散度最小化问题，并可通过标准平均场方法求解。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理与自由能原理之间的关系，展示主动推理可以在不依赖期望自由能概念的情况下实现。

Method: 将主动推理的优化问题重新表述为约束散度最小化问题，使用标准平均场方法求解，提出的感知/行动散度准则在建模感知时与变分自由能一致，在建模行动时与期望自由能泛函相差一个熵正则化项。

Result: 证明了主动推理可以在不诉诸期望自由能概念的情况下实现，提出的方法在离散状态空间中有效。

Conclusion: 主动推理可以与自由能原理分离，通过约束散度最小化方法实现，为理解主动推理提供了新的理论框架。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [33] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过CLASP对比学习框架建立3D几何与声学信号之间的因果对应关系，为物理一致的多模态学习提供基准。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习框架缺乏物理一致性，忽视了物体几何、材料、振动模式和产生声音之间的内在因果关系。需要建立基于物理定律而非统计相关性的感知模型。

Method: 构建VibraVerse数据集，包含3D模型的物理属性（密度、杨氏模量、泊松比）和体积几何，计算模态特征频率和特征向量进行撞击声音合成。使用CLASP对比学习框架实现跨模态对齐。

Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上，基于VibraVerse训练的模型展现出更高的准确性、可解释性和跨模态泛化能力。

Conclusion: VibraVerse为物理一致和因果可解释的多模态学习建立了基准，为声音引导的具身感知和物理世界理解提供了基础，数据集将开源。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>
