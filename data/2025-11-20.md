<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 15]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 提出了一种名为RemoteOptiGraph的分布式优化建模抽象，扩展了Plasmo.jl中的OptiGraph模型，支持在分布式内存环境中处理优化问题，通过Benders分解实现了7.5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为分布式内存环境中的优化问题提供统一建模方法，避免定制化建模方式，并为开发通用元算法提供基础。

Method: 扩展OptiGraph模型，引入InterWorkerEdges管理跨工作器的链接约束，在Plasmo.jl中实现该抽象。

Result: 在美国西部混合整数容量扩展模型上测试，包含超过1200万个变量和约束，使用RemoteOptiGraph和Benders分解比无分解方法快7.5倍。

Conclusion: RemoteOptiGraph抽象为分布式优化问题提供了通用建模框架，显著提升了大规模优化问题的求解效率。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [2] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle是一个双层共识架构，通过BB-Core层实现低延迟高吞吐量，BB-Guard层提供去中心化时间戳和恶意行为检测，在保持强安全性和活跃性的同时实现亚秒级最终性。


<details>
  <summary>Details</summary>
Motivation: 区块链共识面临安全、延迟和去中心化的三难困境，高吞吐量系统通常需要牺牲去中心化或对强对手的鲁棒性，而高度去中心化和安全的系统往往性能较低。

Method: 提出BlueBottle双层共识架构：BB-Core层采用n=5f+1协议，以部分容错性换取低最终性延迟；BB-Guard层提供去中心化时间戳、主动恶意行为检测和同步恢复路径。

Result: 实验显示BB-Core相比Mysticeti降低延迟20-25%，在中等同步假设下实现亚秒级最终性和高吞吐量，同时保持强安全性和活跃性。

Conclusion: BlueBottle的双层架构成功解决了区块链共识三难问题，在保持去中心化和安全性的同时实现了高性能。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 本文介绍了FERMAT强化学习环境，用于自动化发现数学理论和概念，并探索了自动评估数学对象有趣性的方法，使用基于LLM的进化算法在初等数论和有限域领域取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论发现这一重大挑战，通过构建符号化概念发现和定理证明的环境，探索自动评估数学对象有趣性的方法。

Method: 引入FERMAT强化学习环境，采用基于LLM的进化算法，结合函数抽象技术来合成非平凡的有趣性度量方法。

Result: 在初等数论和有限域领域，该方法相比硬编码基线取得了显著改进，成功发现了更有趣的数学对象和理论。

Conclusion: FERMAT环境为数学理论发现提供了有效的RL框架，基于LLM的进化算法在自动评估数学对象有趣性方面表现出色，为自动化数学发现开辟了新途径。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [4] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录回放交互、查询信念理据和注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛，提供可重复的方法来观察和测试这些动态过程，这在人类专家中是不可能的。

Method: 使用多智能体医疗案例模拟器，配备共享记忆（时间戳电子病历）和持有真实实验室结果的预言机智能体。通过大型语言模型智能体扮演不同医学专家角色，在顺序或并行交互中写入共享医疗记录并与调解员互动。

Result: 智能体信念往往反映现实世界的学科立场，包括过度依赖经典研究和抵制反证据，这些信念可以以人类专家无法实现的方式进行追踪和质询。

Conclusion: Ask WhAI通过使这些动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了可重复的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [5] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本文提出了一个完全自动化的LLM辅助工作流，用于处理灾害数据库中的非结构化位置信息，通过GPT-4o清理文本位置数据，并交叉验证三个独立的地理信息库来分配几何形状。


<details>
  <summary>Details</summary>
Motivation: 灾害事件的位置数据对于风险评估和减灾至关重要，但现有数据库如EM-DAT通常以非结构化文本形式报告位置，存在粒度不一致和拼写差异，难以与空间数据集集成。

Method: 使用GPT-4o处理清理文本位置信息，通过交叉验证GADM、OpenStreetMap和Wikidata三个独立地理信息库来分配几何形状，并为每个位置分配可靠性评分。

Result: 应用于2000-2024年EM-DAT数据集，该工作流对14,215个事件的17,948个独特位置进行了地理编码，无需人工干预，覆盖所有灾害类型。

Conclusion: 该方法展示了LLMs从非结构化文本中提取和结构化地理信息的潜力，为相关分析提供了可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [6] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel是一个行动研究项目，创建并追踪了名为Rachel So的完整AI学术身份，通过发表AI生成的研究论文来调查学术生态系统对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究AI作者身份对出版商、研究人员和科学系统的潜在影响，为关于超级人类、超能力AI系统未来学术交流的必要讨论提供实证数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10多篇AI生成的研究论文，并追踪其引用情况和同行评审邀请。

Result: Rachel So成功发表论文并获得引用，还收到了同行评审邀请，表明学术生态系统对AI作者身份存在一定程度的接受。

Conclusion: 这项研究为理解AI在学术交流中的角色提供了实证基础，强调了需要就AI作者身份对学术生态系统的影响进行深入讨论。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [7] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种概率方法来量化AI系统训练和测试数据集的代表性，通过比较场景套件特征与目标操作域特征的统计分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，产生区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信度和安全性，关键在于训练和测试数据集的数据相关安全属性，特别是代表性——场景数据反映系统设计安全操作条件（ODD）或预期遇到条件（TOD）的程度。

Method: 采用概率方法比较场景套件特征与TOD特征的统计分布，应用不精确贝叶斯方法处理有限数据和不确定先验，产生区间值、不确定性感知的代表性估计。

Result: 通过数值示例展示了在依赖性和先验不确定性下，跨操作类别（天气、道路类型、时间等）的场景套件与推断TOD分布的比较，估计了局部和全局代表性区间。

Conclusion: 提出的方法能够量化数据集的代表性，考虑真实TOD分布未知的情况，通过不精确贝叶斯框架提供不确定性感知的代表性评估。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [8] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为MAQ的人类化强化学习框架，通过将人类演示转化为宏观动作，使强化学习智能体的行为更接近人类，同时在D4RL Adroit基准测试中显著提高了人类相似度。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域表现出色，但其行为往往与人类行为存在差异，这影响了可解释性和可信度。本文旨在设计能够产生更自然、人类化行为的强化学习智能体。

Method: 将人类相似度建模为轨迹优化问题，采用后退时域控制作为可实现的实现方法。提出宏观动作量化（MAQ）框架，使用向量量化变分自编码器从人类演示中提取宏观动作。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了人类相似度，增加了轨迹相似度得分，并在人类评估研究中获得了所有强化学习智能体中最高的人类相似度排名。

Conclusion: MAQ框架可以轻松集成到各种现成的强化学习算法中，为学习人类化强化学习智能体开辟了有前景的研究方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [9] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个开源的多智能体框架，通过模块化设计和智能体专业化改进了GeneGPT，在基因组问答任务中性能相当或更优，同时降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决GeneGPT依赖专有模型带来的可扩展性、运营成本、数据隐私和泛化性问题，探索开源模型在基因组问答中的潜力。

Method: 采用模块化多智能体框架，引入工具路由、查询生成和响应验证的专业智能体，实现协调推理和基于角色的任务执行。

Result: 在90%以上的基准任务中匹配或优于GeneGPT，Gene-Turing平均得分0.849，GeneHop平均得分0.830，延迟降低40-50%。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，能够在不牺牲性能的情况下显著提高效率。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [10] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLMs解决动作和变化推理问题，通过提取动作和问题、逐步执行动作推导最终状态，然后评估查询来获得答案。


<details>
  <summary>Details</summary>
Motivation: 解决动作和变化推理问题，利用LLMs的能力来处理这类复杂的推理任务。

Method: 提取RAC基本元素（动作和问题），逐步执行每个动作推导最终状态，然后评估查询与推进状态的匹配度。

Result: 在多个RAC基准测试中表现出色，在不同基准、领域、LLM主干和RAC任务类型上都取得了强劲性能。

Conclusion: ProRAC框架在动作和变化推理问题上具有强大的性能和广泛的适用性。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [11] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估安全风险，包括风险分类分级、细粒度输出分析和人类安全对齐验证。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链提高了答案质量，但这种能力也引入了新的安全风险：有害内容可能在推理过程中被微妙注入、逐渐显现或被误导性理由合理化。现有安全评估主要关注输出层面判断，很少捕捉推理过程中的动态风险。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，明确考虑受影响群体和严重程度；2) 细粒度输出分析：引入微思维分块机制将长推理轨迹分割成语义连贯单元；3) 人类安全对齐：基于人类标注验证LLM评估结果。

Result: 在19个大型推理模型上的评估表明，SafeRBench能够进行详细的多维度安全评估，从多个角度提供风险和防护机制的见解。

Conclusion: SafeRBench为大型推理模型提供了全面的安全评估框架，能够识别传统输出层面评估难以捕捉的动态推理过程安全风险。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [12] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中版权内容的框架，通过将LLM的过度自信转化为优势，识别训练数据和非训练数据的模式差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练时可能使用了受版权保护的内容，现有成员推理攻击方法因LLM的过度自信、缺乏真实训练数据和依赖经验阈值而存在局限。

Method: 采用双重策略：(1)将文件分割成小片段以减少对大规模训练数据的依赖；(2)使用不确定性引导的无监督聚类避免经验阈值调优。

Result: 在LLaMA 7b和LLaMA2 7b上分别达到90.1%和91.6%的平均平衡准确率，相比SOTA基线有超过90%的相对提升，最高达到93.8%平衡准确率，在GPT-J 6B上也有良好泛化性。

Conclusion: 这是首个将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [13] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 本文认为仅靠效率无法实现可持续的推理AI，需要将明确限制嵌入到这类系统的优化和治理中


<details>
  <summary>Details</summary>
Motivation: AI研究正转向复杂问题解决，推理AI缺乏需求饱和点，性能随计算投资指数级增长，而效率改进正接近物理极限

Method: 讨论研究和政策方向，将明确限制嵌入推理AI系统的优化和治理框架

Result: 效率本身不足以实现可持续的推理AI，需要系统性的限制机制

Conclusion: 需要在推理AI的优化和治理中嵌入明确限制，以实现可持续发展

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [14] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文分析了AI研究中存在的两种基本智力观：智力现实主义认为智力是单一可测量的普遍能力，智力多元主义认为智力是多样化的情境依赖能力。这两种观点在方法论、解释和AI风险评估方面产生根本不同的研究路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中存在根本性的分歧，但这些分歧背后的智力观念往往隐含不显。作者希望通过明确这些基本假设来澄清AI研究中的争议。

Method: 通过分析当前AI研究中的辩论，揭示智力现实主义和多元主义如何在不同领域影响实证证据的解释。重点关注方法论、解释性和风险评估三个维度。

Result: 发现两种智力观在模型选择、基准设计、实验验证等方面产生不同方法论；对相同经验现象（如能力涌现、系统局限）产生矛盾解读；在AI风险评估上形成截然不同的观点。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的学术对话。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [15] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合性挑战环境，旨在同时测试强化学习智能体在部分可观测性、信用分配、表示学习、巨大动作空间等多个挑战上的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估智能体在独立任务间的切换能力，而非深度推理能力。作者希望创建一个能同时呈现多个相互关联挑战的单一环境，以测试智能体的综合理解能力。

Method: 开发了Terra Nova环境，灵感来源于《文明V》游戏，该环境将多个经典RL挑战整合到一个统一的框架中，要求智能体进行长期、综合的决策。

Result: 提出了一个新的综合性挑战环境定义，区别于简单的多任务聚合基准，强调环境内部变量的相互作用和整合推理需求。

Conclusion: Terra Nova为评估强化学习智能体在复杂、交互式环境中的深度推理能力提供了新的测试平台，填补了现有基准的不足。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [16] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文提出IPR（交互式物理推理器）框架，通过世界模型推演来评估和强化VLM策略，引入PhysCode物理中心动作编码，在1000+游戏中预训练后，在生存、好奇心和实用性三个层次上实现稳健性能，与GPT-5相当并在好奇心方面超越。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能够通过交互学习获得类似人类的推理能力，并随着经验积累持续改进。探索在包含多样化物理和因果机制的游戏中，从原始直觉到目标驱动推理的人类化学习过程。

Method: 提出IPR框架，使用世界模型推演来评分和强化VLM策略；引入PhysCode物理中心动作编码，将语义意图与动力学对齐，为预测和推理提供共享动作空间。在1000+异构游戏上进行预训练。

Result: IPR在三个推理层次上表现稳健，整体性能与GPT-5相当，在好奇心方面超越GPT-5。性能随训练游戏数量和交互步骤增加而提升，并能零样本迁移到未见过的游戏中。

Conclusion: 物理中心的交互是持续改进物理推理能力的有效路径，证明了通过游戏交互学习物理推理的可行性。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [17] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了TIM框架，通过基于扎根理论的DeFi意图分类法和多智能体LLM系统来推断用户交易意图，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi交易中理解用户意图至关重要但具有挑战性，现有方法缺乏深度语义洞察，需要解决复杂智能合约交互、多因素影响和模糊日志等问题。

Method: TIM框架包含：DeFi意图分类法、多智能体LLM系统、元级规划器动态协调领域专家、问题求解器处理多模态数据、认知评估器减轻幻觉并确保可验证性。

Result: 实验表明TIM显著优于机器学习模型、单一LLM和单一智能体基线，能够更可靠地理解DeFi用户动机。

Conclusion: 该工作为理解DeFi用户动机提供了更可靠的方法，为复杂区块链活动提供情境感知解释，并分析了意图推断中的核心挑战。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>
