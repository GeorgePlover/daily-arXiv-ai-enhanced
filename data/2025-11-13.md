<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bridging Natural Language and ASP: A Hybrid Approach Using LLMs and AMR Parsing](https://arxiv.org/abs/2511.08715)
*Connar Hite,Sean Saud,Raef Taha,Nayim Rahman,Tanvir Atahary,Scott Douglass,Tarek Taha*

Main category: cs.AI

TL;DR: 提出了一种使用LLM和AMR图将无约束英语翻译成ASP程序的新方法，用于解决逻辑谜题，生成完整的ASP程序来代表和解决问题。


<details>
  <summary>Details</summary>
Motivation: ASP是一种强大的组合问题解决工具，但需要学习语法。随着非编程人员与代码交互的需求增加，需要一种方法让用户能够用自然语言描述问题并自动生成ASP程序。

Method: 使用LLM简化自然语言句子、识别关键词和生成简单事实，然后从简化语言解析AMR图，系统性地生成ASP约束，最小化LLM的作用。

Result: 系统成功创建了完整的ASP程序来解决组合逻辑问题，展示了在示例逻辑谜题上的能力。

Conclusion: 这是创建轻量级、可解释的自然语言到复杂逻辑问题解决系统的重要第一步。

Abstract: Answer Set Programming (ASP) is a declarative programming paradigm based on logic programming and non-monotonic reasoning. It is a tremendously powerful tool for describing and solving combinatorial problems. Like any other language, ASP requires users to learn how it works and the syntax involved. It is becoming increasingly required for those unfamiliar with programming languages to interact with code. This paper proposes a novel method of translating unconstrained English into ASP programs for logic puzzles using an LLM and Abstract Meaning Representation (AMR) graphs. Everything from ASP rules, facts, and constraints is generated to fully represent and solve the desired problem. Example logic puzzles are used to demonstrate the capabilities of the system. While most current methods rely entirely on an LLM, our system minimizes the role of the LLM only to complete straightforward tasks. The LLM is used to simplify natural language sentences, identify keywords, and generate simple facts. The AMR graphs are then parsed from simplified language and used to generate ASP constraints systematically. The system successfully creates an entire ASP program that solves a combinatorial logic problem. This approach is a significant first step in creating a lighter-weight, explainable system that converts natural language to solve complex logic problems.

</details>


### [2] [Vector Symbolic Algebras for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.08747)
*Isaac Joffe,Chris Eliasmith*

Main category: cs.AI

TL;DR: 提出了一种基于向量符号代数(VSA)的认知合理ARC-AGI求解器，结合系统1直觉和系统2推理，通过面向对象的程序合成方法解决ARC-AGI基准测试。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI是人类轻松解决但对AI系统极其困难的基准测试，受神经科学和心理学中人类智能建模方法的启发，旨在开发认知合理的求解器。

Method: 使用神经符号方法，基于向量符号代数(VSA)集成系统1直觉和系统2推理，采用面向对象的程序合成，利用VSA表示抽象对象、指导解决方案搜索并实现样本高效的神经学习。

Result: 在ARC-AGI-1-Train上得分10.8%，在ARC-AGI-1-Eval上得分3.0%；在Sort-of-ARC上得分94.5%，在1D-ARC上得分83.1%（以极小计算成本超越GPT-4）。

Conclusion: 这是首个将VSA应用于ARC-AGI的方法，开发了迄今为止最认知合理的ARC-AGI求解器，其独特方法在计算效率和性能方面表现出色。

Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.

</details>


### [3] [UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://arxiv.org/abs/2511.08873)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: 提出了单向认知优化（UCO）方法，通过多轮交互式强化学习解决LLM作为智能导师时的动态适应问题，包含两个协同奖励函数来评估学生认知进步和识别最近发展区。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育场景中从答案提供者转向智能导师，但现有监督微调方法只能学习表面教学模式，缺乏动态适应能力。强化学习方法面临两个关键挑战：无法区分学生是否真正理解还是重复教师答案，以及无法实时感知学生认知状态变化。

Method: UCO方法采用多轮交互式强化学习范式，包含两个协同奖励函数：进步奖励捕捉学生认知进步，评估学生是否从困惑转向理解；支架奖励动态识别学生的最近发展区，鼓励教师在该区域内保持有效教学。

Result: 在BigMath和MathTutorBench基准测试中，UCO模型优于所有同等规模的基线模型，性能与先进的闭源模型相当。

Conclusion: UCO方法通过创新的奖励函数设计，有效解决了LLM作为智能导师时的动态适应问题，在数学辅导任务中表现出色。

Abstract: Large language models (LLMs) are shifting from answer providers to intelligent tutors in educational settings, yet current supervised fine-tuning methods only learn surface teaching patterns without dynamic adaptation capabilities. Recent reinforcement learning approaches address this limitation but face two critical challenges. First, they evaluate teaching effectiveness solely based on whether students produce correct outputs, unable to distinguish whether students genuinely understand or echo teacher-provided answers during interaction. Second, they cannot perceive students' evolving cognitive states in real time through interactive dialogue, thus failing to adapt teaching strategies to match students' cognitive levels dynamically. We propose the Unidirectional Cognitive Optimization (UCO) method to address these challenges. UCO uses a multi-turn interactive reinforcement learning paradigm where the innovation lies in two synergistic reward functions: the Progress Reward captures students' cognitive advancement, evaluating whether students truly transition from confusion to comprehension, while the Scaffold Reward dynamically identifies each student's Zone of Proximal Development (ZPD), encouraging teachers to maintain productive teaching within this zone. We evaluate UCO by comparing it against 11 baseline models on BigMath and MathTutorBench benchmarks. Experimental results demonstrate that our UCO model outperforms all models of equivalent scale and achieves performance comparable to advanced closed-source models. The code and data are available at https://github.com/Mind-Lab-ECNU/UCO.

</details>


### [4] [Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds](https://arxiv.org/abs/2511.08892)
*Weihao Tan,Xiangyang Li,Yunhao Fang,Heyuan Yao,Shi Yan,Hao Luo,Tenglong Ao,Huihui Li,Hongbin Ren,Bairen Yi,Yujia Qin,Bo An,Libin Liu,Guang Shi*

Main category: cs.AI

TL;DR: Lumine是首个能够在3D开放世界环境中实时完成数小时复杂任务的通用智能体，采用端到端的视觉语言模型统一感知、推理和行动，在《原神》中训练后能够完成5小时蒙德主线剧情，并展示了强大的跨游戏零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发能够在复杂3D开放世界环境中执行长时间任务的通用智能体，解决现有方法在处理实时、长时程任务方面的局限性。

Method: 采用端到端的视觉语言模型，以5Hz处理原始像素并生成30Hz的键盘鼠标动作，仅在必要时进行推理，在《原神》游戏中训练。

Result: 成功完成《原神》5小时蒙德主线剧情，达到人类效率水平；在《鸣潮》中完成100分钟任务，在《崩坏：星穹铁道》中完成5小时第一章，展示了强大的跨游戏泛化能力。

Conclusion: Lumine在开放世界环境和不同交互动态中表现出色，标志着向开放环境通用智能体迈出了具体一步。

Abstract: We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.

</details>


### [5] [AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines](https://arxiv.org/abs/2511.09005)
*Alvin Chauhan*

Main category: cs.AI

TL;DR: 本文提出了一个系统框架，将LLM推理视为受控的增量搜索，并通过多智能体管道实现渐进、增量和顺序（GIS）的搜索空间遍历。实验表明，采用递归精炼层的复杂模型在推理质量上显著优于简单线性模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出卓越的流畅性，但研究者仍在努力从中提取更强的推理能力。本文基于搜索导向的LLM计算解释，旨在系统化理解LLM推理和优化。

Method: 采用递归精炼（RR）作为实现GIS搜索的实用方法，包括自我批评、对抗性压力测试和整合关键反馈的迭代过程。设计了简单线性管道与复杂结构化管道的对比实验，复杂管道包含递归精炼层，并使用RAG增强的语料库构建反映美国开国元勋历史人物的多智能体模型。

Result: 复杂模型在所有九个测试案例中始终优于简单模型，平均仲裁分数为88.3对71.7。复杂模型的论证在分析深度、结构细微差别和战略框架方面更优越。

Conclusion: 递归精炼是通过GIS搜索增强LLM推理的强大架构特征，结构化多智能体管道能够实现更高质量的推理输出。

Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.

</details>


### [6] [A Research on Business Process Optimisation Model Integrating AI and Big Data Analytics](https://arxiv.org/abs/2511.08934)
*Di Liao,Ruijia Liang,Ziyi Ye*

Main category: cs.AI

TL;DR: 本研究构建了融合人工智能与大数据的业务流程优化模型，采用三层架构实现流程全生命周期的智能管理，实验验证显示该模型能显著缩短处理时间、提高资源利用率并降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 随着数字化转型的深入，业务流程优化成为提升企业竞争力的关键，需要构建智能化的流程管理解决方案。

Method: 采用包含数据处理、AI算法和业务逻辑的三层架构模型，结合分布式计算和深度学习技术，实现实时流程监控与优化。

Result: 实验验证显示：流程处理时间缩短42%，资源利用率提高28%，运营成本降低35%，系统在高并发负载下保持99.9%的可用性。

Conclusion: 研究成果对企业数字化转型具有重要理论价值和实践意义，为提升企业运营效率提供了新思路。

Abstract: With the deepening of digital transformation, business process optimisation has become the key to improve the competitiveness of enterprises. This study constructs a business process optimisation model integrating artificial intelligence and big data to achieve intelligent management of the whole life cycle of processes. The model adopts a three-layer architecture incorporating data processing, AI algorithms, and business logic to enable real-time process monitoring and optimization. Through distributed computing and deep learning techniques, the system can handle complex business scenarios while maintaining high performance and reliability. Experimental validation across multiple enterprise scenarios shows that the model shortens process processing time by 42%, improves resource utilisation by 28%, and reduces operating costs by 35%. The system maintained 99.9% availability under high concurrent loads. The research results have important theoretical and practical value for promoting the digital transformation of enterprises, and provide new ideas for improving the operational efficiency of enterprises.

</details>


### [7] [AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting](https://arxiv.org/abs/2511.08947)
*Xiaohan Zhang,Tian Gao,Mingyue Cheng,Bokai Pan,Ze Guo,Yaguo Liu,Xiaoyu Tao*

Main category: cs.AI

TL;DR: AlphaCast是一个结合人类智慧与大型语言模型智能的协同推理框架，将时间序列预测重新定义为交互式过程，通过自动化预测准备和生成推理与反思优化两个阶段，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测方法大多将预测视为静态的一次性映射任务，缺乏人类专家的交互、推理和适应性，限制了在复杂现实环境中的实用性。

Method: 采用两阶段框架：1)自动化预测准备阶段构建多源认知基础，包括特征集、领域知识库、上下文存储库和案例库；2)生成推理与反思优化阶段整合统计特征、先验知识、上下文信息和预测策略，触发元推理循环进行持续自我校正和策略优化。

Result: 在短期和长期数据集上的广泛实验表明，AlphaCast在预测准确性方面始终优于最先进的基线方法。

Conclusion: AlphaCast通过人类智慧与LLM智能的协同推理，成功将时间序列预测转变为交互式过程，显著提升了预测性能和在复杂环境中的实用性。

Abstract: Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .

</details>


### [8] [Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs](https://arxiv.org/abs/2511.09032)
*Dingji Wang,You Lu,Bihuan Chen,Shuo Hao,Haowen Jiang,Yifan Tian,Xin Peng*

Main category: cs.AI

TL;DR: 提出了一个名为Argus的运行时韧性导向框架，用于增强端到端自动驾驶系统的安全性，通过持续监控轨迹和危险缓解机制来防止安全违规。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在公共道路上部署时会面临各种驾驶危险，需要具备持续监控和自适应响应的韧性能力来确保安全。

Method: Argus框架持续监控自动驾驶系统生成的轨迹，当检测到车辆处于不安全状态时，通过危险缓解器无缝接管控制。

Result: 与TCP、UniAD和VAD三个先进端到端自动驾驶系统集成测试显示，Argus能将驾驶评分平均提升150.30%，防止64.38%的违规行为，且时间开销很小。

Conclusion: Argus框架能有效提升自动驾驶系统的韧性，显著改善驾驶性能并预防安全违规，具有实际应用价值。

Abstract: End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.
  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.

</details>


### [9] [Advancing Autonomous Emergency Response Systems: A Generative AI Perspective](https://arxiv.org/abs/2511.09044)
*Yousef Emami,Radha Reddy,Azadeh Pourkabirian,Miguel Gutierrez Gaitan*

Main category: cs.AI

TL;DR: 本文综述了下一代自动驾驶车辆在紧急服务中的优化策略，重点分析了从传统强化学习向扩散模型增强强化学习和大型语言模型辅助上下文学习的转变，为理解基于生成AI的自主应急响应系统提供了框架。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆有望通过更快、更安全、更高效的响应来革新紧急服务，但传统强化学习方法在动态紧急场景中存在样本效率低和适应性差的问题，需要新的优化策略。

Method: 分析了从传统强化学习到扩散模型增强强化学习的转变（通过合成数据生成增强策略鲁棒性），以及新兴的大型语言模型辅助上下文学习范式（提供轻量级、可解释的即时适应能力）。

Result: 扩散模型增强强化学习提高了策略鲁棒性但增加了计算成本，而大型语言模型辅助上下文学习提供了无需重新训练的快速适应能力，为自主应急响应系统提供了互补的解决方案。

Conclusion: 通过综述自动驾驶智能、扩散模型增强强化学习和大型语言模型辅助上下文学习的最新技术，本文为理解下一代基于生成AI的自主应急响应系统提供了关键框架。

Abstract: Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.

</details>


### [10] [OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.09092)
*Zezhen Ding,Zhen Tan,Jiheng Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: OR-R1是一个数据高效的训练框架，用于自动化优化建模和求解，通过监督微调和测试时组相对策略优化两阶段设计，仅需1/10的合成数据即可达到67.7%的平均求解准确率，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的优化建模方法需要大量标注或合成数据，导致成本高且可扩展性差，需要开发数据效率更高的自动化解决方案。

Method: 采用两阶段训练框架：首先使用监督微调从有限标注数据中学习问题建模和代码生成的推理模式，然后通过测试时组相对策略优化提升能力和一致性。

Result: OR-R1仅需1/10的合成数据就达到67.7%的平均求解准确率，比ORLM方法高出4.2%，在仅100个合成样本时仍能超越ORLM 2.4%以上，TGRPO贡献了3.1%-6.4%的额外准确率提升。

Conclusion: OR-R1提供了一个稳健、可扩展且成本效益高的自动化运筹优化问题建模和求解解决方案，显著降低了工业应用的专家知识和数据壁垒。

Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.

</details>


### [11] [Efficient Reasoning via Reward Model](https://arxiv.org/abs/2511.09158)
*Yuhao Wang,Xiaopeng Li,Cheng Gong,Ziru Liu,Suiyun Zhang,Rui Liu,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 提出了训练简洁性奖励模型（CRM）的流程和简洁性奖励函数（CRF），通过减少推理路径中的冗余内容来解决大型推理模型中的过度思考问题，在数学基准数据集上实现了准确率提升和响应长度减少。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如DeepSeek-R1和OpenAI o1）经常生成包含冗余或无关推理步骤的冗长响应（过度思考现象），这会显著增加计算成本。现有的长度惩罚方法存在长度崩溃和训练崩溃问题。

Method: 提出训练简洁性奖励模型（CRM）来对推理路径的简洁性进行评分，并引入新颖的简洁性奖励函数（CRF），在结果奖励和简洁性评分之间建立显式依赖关系。

Result: 在五个数学基准数据集上的实验表明，该方法在Qwen2.5-7B上实现了8.1%的准确率提升和19.9%的响应token长度减少，并且在Llama和Mistral等其他LLM上也有良好泛化性能。

Conclusion: 该方法从理论上证明了新奖励在方差减少和改进收敛性方面的优越性，实践上有效解决了过度思考问题，提高了推理效率和效果。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been shown to enhance the reasoning capabilities of large language models (LLMs), enabling the development of large reasoning models (LRMs). However, LRMs such as DeepSeek-R1 and OpenAI o1 often generate verbose responses containing redundant or irrelevant reasoning step-a phenomenon known as overthinking-which substantially increases computational costs. Prior efforts to mitigate this issue commonly incorporate length penalties into the reward function, but we find they frequently suffer from two critical issues: length collapse and training collapse, resulting in sub-optimal performance. To address them, we propose a pipeline for training a Conciseness Reward Model (CRM) that scores the conciseness of reasoning path. Additionally, we introduce a novel reward formulation named Conciseness Reward Function (CRF) with explicit dependency between the outcome reward and conciseness score, thereby fostering both more effective and more efficient reasoning. From a theoretical standpoint, we demonstrate the superiority of the new reward from the perspective of variance reduction and improved convergence properties. Besides, on the practical side, extensive experiments on five mathematical benchmark datasets demonstrate the method's effectiveness and token efficiency, which achieves an 8.1% accuracy improvement and a 19.9% reduction in response token length on Qwen2.5-7B. Furthermore, the method generalizes well to other LLMs including Llama and Mistral. The implementation code and datasets are publicly available for reproduction: https://anonymous.4open.science/r/CRM.

</details>


### [12] [Perspectives on a Reliability Monitoring Framework for Agentic AI Systems](https://arxiv.org/abs/2511.09178)
*Niclas Flehmig,Mary Ann Lundteigen,Shen Yin*

Main category: cs.AI

TL;DR: 本文提出了一个双层可靠性监控框架，用于解决智能AI系统在运行中的可靠性不足问题，包括离群检测层和AI透明度层，为人类操作员提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 智能AI系统在医疗和流程工业等高风险领域应用时，由于可靠性不足而存在风险，需要开发缓解技术来应对运行中的意外行为。

Method: 提出双层可靠性监控框架：第一层用于检测新颖输入的离群分布，第二层通过AI透明度揭示内部操作，为人类操作员提供决策支持。

Result: 该框架为开发缓解技术奠定了基础，能够减少因运行中可靠性不确定而产生的风险。

Conclusion: 双层监控框架能够有效支持人类操作员判断输出是否可靠并适时干预，提升智能AI系统在关键应用中的可靠性。

Abstract: The implementation of agentic AI systems has the potential of providing more helpful AI systems in a variety of applications. These systems work autonomously towards a defined goal with reduced external control. Despite their potential, one of their flaws is the insufficient reliability which makes them especially unsuitable for high-risk domains such as healthcare or process industry. Unreliable systems pose a risk in terms of unexpected behavior during operation and mitigation techniques are needed. In this work, we derive the main reliability challenges of agentic AI systems during operation based on their characteristics. We draw the connection to traditional AI systems and formulate a fundamental reliability challenge during operation which is inherent to traditional and agentic AI systems. As our main contribution, we propose a two-layered reliability monitoring framework for agentic AI systems which consists of a out-of-distribution detection layer for novel inputs and AI transparency layer to reveal internal operations. This two-layered monitoring approach gives a human operator the decision support which is needed to decide whether an output is potential unreliable or not and intervene. This framework provides a foundation for developing mitigation techniques to reduce risk stemming from uncertain reliability during operation.

</details>


### [13] [MedFuse: Multiplicative Embedding Fusion For Irregular Clinical Time Series](https://arxiv.org/abs/2511.09247)
*Yi-Hsien Hsieh,Ta-Jung Chien,Chun-Kai Huang,Shao-Hua Sun,Che Lin*

Main category: cs.AI

TL;DR: MedFuse是一个用于不规则临床时间序列的框架，通过乘法融合模块MuFuse将特征值和特征身份嵌入进行乘法调制，在三个真实世界数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床时间序列存在不规则性、异步采样、缺失值和异质特征动态等问题，现有嵌入策略通常通过加法操作结合特征身份和值嵌入，限制了捕捉值依赖特征交互的能力。

Method: 提出MedFuse框架，核心是MuFuse模块，通过乘法调制融合值和特征嵌入，保留特征特定信息的同时建模跨特征的高阶依赖关系。

Result: 在涵盖重症和慢性护理的三个真实世界数据集实验中，MedFuse在关键预测任务上一致优于最先进的基线方法。学习表示分析显示乘法融合增强了表达能力并支持跨数据集预训练。

Conclusion: MedFuse为建模不规则临床时间序列提供了一个可泛化的方法。

Abstract: Clinical time series derived from electronic health records (EHRs) are inherently irregular, with asynchronous sampling, missing values, and heterogeneous feature dynamics. While numerical laboratory measurements are highly informative, existing embedding strategies usually combine feature identity and value embeddings through additive operations, which constrains their ability to capture value-dependent feature interactions. We propose MedFuse, a framework for irregular clinical time series centered on the MuFuse (Multiplicative Embedding Fusion) module. MuFuse fuses value and feature embeddings through multiplicative modulation, preserving feature-specific information while modeling higher-order dependencies across features. Experiments on three real-world datasets covering both intensive and chronic care show that MedFuse consistently outperforms state-of-the-art baselines on key predictive tasks. Analysis of the learned representations further demonstrates that multiplicative fusion enhances expressiveness and supports cross-dataset pretraining. These results establish MedFuse as a generalizable approach for modeling irregular clinical time series.

</details>


### [14] [HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting](https://arxiv.org/abs/2511.09275)
*Minlan Shao,Zijian Zhang,Yili Wang,Yiwei Dai,Xu Shen,Xin Wang*

Main category: cs.AI

TL;DR: 本文提出HyperD框架，通过将交通数据分解为周期性和残差分量来解决交通预测中的空间依赖性和多尺度周期性模式挑战，在四个真实数据集上实现了最先进的预测精度。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临两大挑战：复杂空间依赖性和多尺度周期性模式与不规则波动的共存。现有方法难以同时有效处理这些因素。

Method: 提出HyperD框架，包含混合周期表示模块（处理周期性分量）和频率感知残差表示模块（处理非周期性波动），并引入双视图对齐损失来强制语义分离。

Result: 在四个真实交通数据集上的实验表明，HyperD实现了最先进的预测精度，在干扰下具有更好的鲁棒性和更高的计算效率。

Conclusion: HyperD通过解耦周期性和残差分量，有效解决了交通预测中的关键挑战，为智能交通系统提供了可靠的预测解决方案。

Abstract: Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.

</details>


### [15] [From Model Training to Model Raising - A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development](https://arxiv.org/abs/2511.09287)
*Roland Aydin,Christian Cyron,Steve Bachelor,Ashton Anderson,Robert West*

Main category: cs.AI

TL;DR: 论文提出从"模型训练"转向"模型培育"的范式转变，将价值观对齐融入模型开发全过程，通过重新设计训练语料库来实现早期价值观承诺。


<details>
  <summary>Details</summary>
Motivation: 当前AI训练方法只在模型核心能力建立后才进行价值观对齐，导致模型容易失准且缺乏深层次价值体系。随着大语言模型能力逐渐超越人类，这种深层价值观整合变得至关重要。

Method: 重新设计训练语料库：采用第一人称视角重构训练数据、将信息重新情境化为生活经验、模拟社会互动、以及搭建训练数据排序的脚手架。

Result: 预期这种训练语料库的重新设计将实现从第一个训练标记开始的早期价值观承诺，使知识、技能和价值观内在难以分离。

Conclusion: 在大型语言模型能力开始超越人类能力的生态系统中，这种从模型训练转向模型培育的范式转变是至关重要的需求。

Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.

</details>


### [16] [Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI](https://arxiv.org/abs/2511.09325)
*Stine Beltoft,Lukas Galke*

Main category: cs.AI

TL;DR: 本文主张开发专门为定性研究设计的AI系统，以弥补当前AI在定性研究中的不足，强调系统需要具备透明性、可重复性和隐私友好性。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要推动了定量方法的发展，但定性研究领域被忽视。研究人员被迫使用通用AI工具如ChatGPT，但这些工具存在偏见、不透明、不可重复和隐私问题，无法满足定性研究的专业需求。

Method: 通过文献回顾分析现有自动化发现流程如何通过增强定性能力来改进，并识别安全定性AI在多学科和混合方法研究中的关键机会。

Result: 发现AI在定性研究领域存在严重缺口，现有工具无法满足定性研究的专业要求，需要专门设计的系统。

Conclusion: 需要从零开始构建专门用于解释性研究的定性AI系统，这些系统必须透明、可重复且隐私友好，以推动多学科和混合方法研究的发展。

Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.

</details>


### [17] [The 2025 Planning Performance of Frontier Large Language Models](https://arxiv.org/abs/2511.09378)
*Augusto B. Corrêa,André G. Pereira,Jendrik Seipp*

Main category: cs.AI

TL;DR: 评估了2025年三个前沿大语言模型（DeepSeek R1、Gemini 2.5 Pro、GPT-5）在PDDL规划和推理任务上的表现，发现GPT-5在标准PDDL领域与LAMA规划器竞争，但在混淆测试中所有模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 评估前沿大语言模型在端到端规划任务中的推理能力，了解它们与专业规划器的性能差距。

Method: 使用国际规划竞赛学习轨道的PDDL领域子集，对三个前沿LLM进行标准PDDL和混淆PDDL任务的规划性能测试，并与LAMA规划器对比。

Result: GPT-5在标准PDDL任务中与LAMA规划器表现相当，所有LLM在混淆测试中性能下降但程度比之前模型轻，显示相比前代LLM有显著改进。

Conclusion: 前沿LLM在规划任务上取得了实质性进步，缩小了与专业规划器在挑战性基准测试中的性能差距。

Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.

</details>


### [18] [What We Don't C: Representations for scientific discovery beyond VAEs](https://arxiv.org/abs/2511.09433)
*Brian Rogers,Micah Bowles,Chris J. Lintott,Steve Croft*

Main category: cs.AI

TL;DR: 提出了一种基于潜在流匹配和分类器自由引导的新方法，通过显式分离条件信息与残差表示来解耦潜在子空间，在多个实验中展示了该方法能够访问高维数据的有意义特征。


<details>
  <summary>Details</summary>
Motivation: 在科学发现中访问学习表示中的信息对于高维领域至关重要，需要一种机制来分析、控制和重新利用潜在表示。

Method: 基于潜在流匹配与分类器自由引导的方法，显式分离条件信息与残差表示，实现潜在子空间的解耦。

Result: 在合成2D高斯玩具问题、彩色MNIST和Galaxy10天文数据集三个实验中，该方法成功访问了高维数据的有意义特征。

Conclusion: 该方法为使用生成模型进行科学探索提供了一条简单而强大的途径，能够分析我们未捕获、考虑或编目的信息。

Abstract: Accessing information in learned representations is critical for scientific discovery in high-dimensional domains. We introduce a novel method based on latent flow matching with classifier-free guidance that disentangles latent subspaces by explicitly separating information included in conditioning from information that remains in the residual representation. Across three experiments -- a synthetic 2D Gaussian toy problem, colored MNIST, and the Galaxy10 astronomy dataset -- we show that our method enables access to meaningful features of high dimensional data. Our results highlight a simple yet powerful mechanism for analyzing, controlling, and repurposing latent representations, providing a pathway toward using generative models for scientific exploration of what we don't capture, consider, or catalog.

</details>


### [19] [CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?](https://arxiv.org/abs/2511.09483)
*Peiyu Li,Xiaobao Huang,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: CrochetBench是一个评估多模态大语言模型在钩针编织领域进行细粒度、低层次程序推理能力的基准测试，强调从描述转向实际操作，要求模型识别针法、选择结构适当的指令并生成可编译的钩针程序。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注高层次描述或视觉问答，缺乏对实际操作能力的评估。CrochetBench旨在填补这一空白，通过评估模型在真实创造性领域中的程序能力，揭示表面理解与可执行精度之间的差距。

Method: 采用CrochetPARADE DSL作为中间表示，支持结构验证和通过执行进行功能评估。基准测试涵盖针法分类、指令接地以及自然语言和图像到DSL的翻译任务。

Result: 在所有任务中，当评估从表面相似性转向可执行正确性时，模型性能急剧下降，暴露出在长距离符号推理和3D感知程序合成方面的局限性。

Conclusion: CrochetBench为评估多模态模型的程序能力提供了新视角，并突显了在真实创造性领域中表面理解与可执行精度之间的显著差距。

Abstract: We present CrochetBench, a benchmark for evaluating the ability of multimodal large language models to perform fine-grained, low-level procedural reasoning in the domain of crochet. Unlike prior benchmarks that focus on high-level description or visual question answering, CrochetBench shifts the emphasis from describing to doing: models are required to recognize stitches, select structurally appropriate instructions, and generate compilable crochet procedures. We adopt the CrochetPARADE DSL as our intermediate representation, enabling structural validation and functional evaluation via execution. The benchmark covers tasks including stitch classification, instruction grounding, and both natural language and image-to-DSL translation. Across all tasks, performance sharply declines as the evaluation shifts from surface-level similarity to executable correctness, exposing limitations in long-range symbolic reasoning and 3D-aware procedural synthesis. CrochetBench offers a new lens for assessing procedural competence in multimodal models and highlights the gap between surface-level understanding and executable precision in real-world creative domains. Code is available at https://github.com/Peiyu-Georgia-Li/crochetBench.

</details>


### [20] [Consensus Sampling for Safer Generative AI](https://arxiv.org/abs/2511.09493)
*Adam Tauman Kalai,Yael Tauman Kalai,Or Zamir*

Main category: cs.AI

TL;DR: 提出一种基于多模型聚合的AI安全方法，通过共识采样算法从k个模型中选择最安全的s个子集，在模型间达成足够共识时输出结果，否则弃权。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全方法依赖模型输出或激活检查，但某些风险无法通过检查单独检测，需要架构无关的补充方法。

Method: 使用共识采样算法，利用模型计算输出概率的能力，当足够多模型安全且达成共识时输出结果，否则弃权。

Result: 该方法能够实现与k个模型中最安全的s个模型平均风险相当的安全性，并在模型间缺乏共识时弃权。

Conclusion: 提供了一种新的模型无关AI安全方法，通过聚合未知安全子集中的模型来增强单一可靠模型的安全性保证。

Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.

</details>


### [21] [Fundamentals of Physical AI](https://arxiv.org/abs/2511.09497)
*Vahid Salehi*

Main category: cs.AI

TL;DR: 本文从科学和系统角度阐述了物理人工智能的基本原理，旨在为智能系统的物理体现、感知、行动、学习和情境敏感性建立统一的理论框架，将智能视为身体、环境和经验之间真实交互的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 传统AI依赖符号处理和数据驱动模型，而物理AI将智能理解为身体、环境和经验之间真实交互的涌现现象，需要建立描述物理体现、感知、行动、学习和情境敏感性的统一理论框架。

Method: 提出六个基本原理：体现、感知、运动行动、学习、自主性和情境敏感性，这些原理构成一个封闭的控制循环，能量、信息、控制和情境在其中持续交互。

Result: 理论模型通过康复诊所中的自适应辅助机器人实例说明，物理智能不是来自抽象计算，而是来自直接的、具身化的体验，六个基本原理在实际系统中相互作用。

Conclusion: 物理AI将学习理解为智能体与环境之间结构耦合的变化，而非参数调整，这一范式转变将智能理解为物理具身化过程，意义生成来自物理体验而非数据库。

Abstract: This work will elaborate the fundamental principles of physical artificial intelligence (Physical AI) from a scientific and systemic perspective. The aim is to create a theoretical foundation that describes the physical embodiment, sensory perception, ability to act, learning processes, and context sensitivity of intelligent systems within a coherent framework. While classical AI approaches rely on symbolic processing and data driven models, Physical AI understands intelligence as an emergent phenomenon of real interaction between body, environment, and experience. The six fundamentals presented here are embodiment, sensory perception, motor action, learning, autonomy, and context sensitivity, and form the conceptual basis for designing and evaluating physically intelligent systems. Theoretically, it is shown that these six principles do not represent loose functional modules but rather act as a closed control loop in which energy, information, control, and context are in constant interaction. This circular interaction enables a system to generate meaning not from databases, but from physical experience, a paradigm shift that understands intelligence as an physical embodied process. Physical AI understands learning not as parameter adjustment, but as a change in the structural coupling between agents and the environment. To illustrate this, the theoretical model is explained using a practical scenario: An adaptive assistant robot supports patients in a rehabilitation clinic. This example illustrates that physical intelligence does not arise from abstract calculation, but from immediate, embodied experience. It shows how the six fundamentals interact in a real system: embodiment as a prerequisite, perception as input, movement as expression, learning as adaptation, autonomy as regulation, and context as orientation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [22] [Introduction to Automated Negotiation](https://arxiv.org/abs/2511.08659)
*Dave de Jonge*

Main category: cs.MA

TL;DR: 这是一本面向计算机科学学生的自动谈判入门教材，不需要预备知识，包含基于Python的简单谈判框架。


<details>
  <summary>Details</summary>
Motivation: 为计算机科学学生提供自动谈判的入门教材，降低学习门槛，让没有预备知识的学生也能快速上手。

Method: 使用Python实现了一个简单的玩具世界谈判框架，读者可以基于此实现自己的谈判算法并进行实验。

Result: 提供了一个简单易用的谈判框架，学生可以快速理解自动谈判的基本概念和实现方法。

Conclusion: 本书成功地为计算机科学学生提供了一个完整的自动谈判学习路径，从理论到实践，框架设计简单易用，便于移植到其他编程语言。

Abstract: This book is an introductory textbook targeted towards computer science students who are completely new to the topic of automated negotiation. It does not require any prerequisite knowledge, except for elementary mathematics and basic programming skills.
  This book comes with an simple toy-world negotiation framework implemented in Python that can be used by the readers to implement their own negotiation algorithms and perform experiments with them. This framework is small and simple enough that any reader who does not like to work in Python should be able to re-implement it very quickly in any other programming language of their choice.

</details>


### [23] [Convergence dynamics of Agent-to-Agent Interactions with Misaligned objectives](https://arxiv.org/abs/2511.08710)
*Romain Cosentino,Sarath Shekkizhar,Adam Earle*

Main category: cs.MA

TL;DR: 本文提出了一个多智能体场景中智能体间交互的理论框架，分析语言模型智能体在目标不一致时的迭代梯度更新动态，揭示了会导致有偏均衡和可预测的残差误差。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中语言模型智能体在目标不一致时的交互动态，为理解、预测和防御多智能体系统提供理论框架。

Method: 建立理论框架，分析两个语言模型智能体使用对方输出作为输入进行迭代梯度更新，通过实验使用训练好的transformer模型和GPT-5进行上下文线性回归任务验证理论。

Result: 当智能体目标不一致时，会产生有偏均衡，双方都无法达到目标，残差误差可从目标差距和提示几何预测；建立了非对称收敛条件并提供了可证明实现单边成功的算法。

Conclusion: 该框架为研究、预测和防御多智能体系统提供了设置，明确将提示设计和交互设置与稳定性、偏差和鲁棒性联系起来。

Abstract: We develop a theoretical framework for agent-to-agent interactions in multi-agent scenarios. We consider the setup in which two language model based agents perform iterative gradient updates toward their respective objectives in-context, using the output of the other agent as input. We characterize the generation dynamics associated with the interaction when the agents have misaligned objectives, and show that this results in a biased equilibrium where neither agent reaches its target - with the residual errors predictable from the objective gap and the geometry induced by the prompt of each agent. We establish the conditions for asymmetric convergence and provide an algorithm that provably achieves an adversarial result, producing one-sided success. Experiments with trained transformer models as well as GPT$5$ for the task of in-context linear regression validate the theory. Our framework presents a setup to study, predict, and defend multi-agent systems; explicitly linking prompt design and interaction setup to stability, bias, and robustness.

</details>


### [24] [Achieving Equilibrium under Utility Heterogeneity: An Agent-Attention Framework for Multi-Agent Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2511.08926)
*Zhuhui Li,Chunbo Luo,Liming Huang,Luyu Qi,Geyong Min*

Main category: cs.MA

TL;DR: 本文提出了一种基于智能体注意力的多智能体多目标强化学习框架（AA-MAMORL），通过隐式学习其他智能体的效用函数和策略，在保持分散执行的同时近似贝叶斯纳什均衡，显著提升了多智能体多目标系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体多目标系统优化方法在处理异构目标和效用函数设置时面临挑战，由于私有效用函数和相关策略导致训练非平稳性加剧。本文旨在解决在分散执行约束下如何获取全局效用函数以实现贝叶斯纳什均衡的问题。

Method: 提出AA-MAMORL框架，在集中训练期间隐式学习其他智能体的效用函数和相关策略的联合信念，将全局状态和效用映射到每个智能体的策略。在执行阶段，每个智能体基于局部观察和私有效用函数独立选择动作，无需智能体间通信。

Result: 在自定义MAMO粒子环境和标准MOMALand基准测试中进行的实验表明，访问全局偏好和提出的AA-MAMORL方法显著提高了性能，并持续优于最先进的方法。

Conclusion: 理论证明在分散执行约束下需要直接访问或结构化建模全局效用函数才能实现贝叶斯纳什均衡。AA-MAMORL框架通过隐式学习联合信念，在保持分散执行的同时有效近似BNE，为多智能体多目标系统提供了可行的解决方案。

Abstract: Multi-agent multi-objective systems (MAMOS) have emerged as powerful frameworks for modelling complex decision-making problems across various real-world domains, such as robotic exploration, autonomous traffic management, and sensor network optimisation. MAMOS offers enhanced scalability and robustness through decentralised control and more accurately reflects inherent trade-offs between conflicting objectives. In MAMOS, each agent uses utility functions that map return vectors to scalar values. Existing MAMOS optimisation methods face challenges in handling heterogeneous objective and utility function settings, where training non-stationarity is intensified due to private utility functions and the associated policies. In this paper, we first theoretically prove that direct access to, or structured modeling of, global utility functions is necessary for the Bayesian Nash Equilibrium under decentralised execution constraints. To access the global utility functions while preserving the decentralised execution, we propose an Agent-Attention Multi-Agent Multi-Objective Reinforcement Learning (AA-MAMORL) framework. Our approach implicitly learns a joint belief over other agents' utility functions and their associated policies during centralised training, effectively mapping global states and utilities to each agent's policy. In execution, each agent independently selects actions based on local observations and its private utility function to approximate a BNE, without relying on inter-agent communication. We conduct comprehensive experiments in both a custom-designed MAMO Particle environment and the standard MOMALand benchmark. The results demonstrate that access to global preferences and our proposed AA-MAMORL significantly improve performance and consistently outperform state-of-the-art methods.

</details>


### [25] [Learning Efficient Communication Protocols for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.09171)
*Xinren Zhang,Jiadong Yu,Zixin Zhong*

Main category: cs.MA

TL;DR: 本文提出了一个学习多轮通信协议的通用框架，通过三个通信效率指标来优化多智能体强化学习中的通信效率和协作性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，通信虽然能促进协调，但可能导致冗余或非必要的信息交换。现有研究缺乏对通信协议（通信拓扑和消息）的适当定义和优化的深入研究。

Method: 提出了一个学习多轮通信协议的通用框架，引入了三个通信效率指标：信息熵效率指数和专业化效率指数用于效率增强优化，拓扑效率指数用于显式评估。将前两个指标整合为调整损失函数，促进信息丰富的消息传递和角色专业化。

Result: 通过综合实验证明，学习到的通信协议能显著提高通信效率，并在改进成功率的同时实现更好的协作性能。

Conclusion: 该框架能够有效优化多智能体系统中的通信协议，在保证任务性能的同时提升通信效率。

Abstract: Multi-Agent Systems (MAS) have emerged as a powerful paradigm for modeling complex interactions among autonomous entities in distributed environments. In Multi-Agent Reinforcement Learning (MARL), communication enables coordination but can lead to inefficient information exchange, since agents may generate redundant or non-essential messages. While prior work has focused on boosting task performance with information exchange, the existing research lacks a thorough investigation of both the appropriate definition and the optimization of communication protocols (communication topology and message). To fill this gap, we introduce a generalized framework for learning multi-round communication protocols that are both effective and efficient. Within this framework, we propose three novel Communication Efficiency Metrics (CEMs) to guide and evaluate the learning process: the Information Entropy Efficiency Index (IEI) and Specialization Efficiency Index (SEI) for efficiency-augmented optimization, and the Topology Efficiency Index (TEI) for explicit evaluation. We integrate IEI and SEI as the adjusted loss functions to promote informative messaging and role specialization, while using TEI to quantify the trade-off between communication volume and task performance. Through comprehensive experiments, we demonstrate that our learned communication protocol can significantly enhance communication efficiency and achieves better cooperation performance with improved success rates.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [26] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 本文提出了首个通过MLIR中的OpenMP目标指令实现选择性代码卸载到FPGA的方法，结合OpenMP方言和高级综合方言，提供面向FPGA的可移植编译流程。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律放缓，FPGA等异构计算平台在加速HPC工作负载方面越来越受关注，需要更灵活的FPGA加速方法。

Method: 将MLIR OpenMP方言与高级综合方言结合，利用现有MLIR构建模块，支持任何MLIR兼容的前端（如Flang），通过标准OpenMP指令手动优化卸载内核。

Result: 成功实现了基于指令的FPGA加速方法，显著减少了开发工作量，展示了MLIR生态系统的可组合性优势。

Conclusion: 该方法为基于指令的FPGA加速建立了灵活且可扩展的路径，并集成在MLIR生态系统中。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [27] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: 数据中心通过能源资源解耦电力容量和电网负载，创造灵活性以降低碳排放，优化分布和管理可实现显著碳减排和经济可行性。


<details>
  <summary>Details</summary>
Motivation: AI和云数据中心的高能耗加剧了碳足迹问题，其恒定电力需求与波动性可再生能源发电存在冲突，需要通过负载适应性来改善电网可再生能源吸收。

Method: 定义和计算数据中心负载解耦的功率和能量需求，评估解耦资源的分布和管理方法，包括站点差异和电网合作。

Result: 优化分布可实现98%潜在碳减排，仅需70%解耦资源；电网合作管理比单向信息共享多实现1.4倍碳减排；经济上收益大于本地成本。

Conclusion: 数据中心负载解耦是降低碳排放的有效方法，但站点间差异可能需要电网干预以确保公平性。

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [28] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 本文评估了虚拟化云基础设施中HPC风格的CPU性能和成本，使用SPEC ACCEL套件中的OpenMP工作负载子集比较了AWS、Azure、GCP和OCI四大云服务提供商在Intel、AMD和ARM通用实例类型下的表现。


<details>
  <summary>Details</summary>
Motivation: 研究云服务提供商在不同CPU架构（Intel、AMD、ARM）上的性能和成本差异，为HPC工作负载选择合适的云实例提供指导。

Method: 使用SPEC ACCEL套件中的OpenMP工作负载子集，在AWS、Azure、GCP和OCI四大云服务提供商的Intel、AMD和ARM通用实例类型上进行测试，比较按需定价和一年折扣定价两种模式。

Result: AWS在所有三种实例类型中始终提供最短运行时间，但收费较高；OCI在所有CPU系列中最为经济，但运行速度较慢；Azure性能和成本处于中等水平；GCP从Intel切换到AMD时性能显著提升，但其ARM实例比自身AMD实例慢两倍以上且更昂贵；AWS的ARM实例在运行时间上比其Intel和AMD实例快达49%。

Conclusion: 实例选择和云服务提供商选择会在运行时间和价格上产生显著差异，应根据工作负载优先级（原始速度或成本最小化）来指导实例类型决策。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [29] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 本文提出了企业级隐私保护联邦学习框架APPFL，旨在解决从本地原型到分布式部署的扩展性问题，提供可扩展的本地模拟、无缝部署过渡、多基础设施支持、多级抽象和全面隐私保护等关键能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在科学领域具有重要应用价值，但构建既用户友好又具备隐私保护的企业级框架仍面临挑战，特别是在跨越异构客户端计算基础设施的本地原型与分布式部署之间的差距。

Method: 基于构建APPFL框架的经验，提出企业级隐私保护联邦学习框架的愿景，包括可扩展本地模拟、无缝部署过渡、分布式部署、多级抽象和隐私安全技术（差分隐私、安全聚合、认证和机密计算）等关键能力。

Result: 提出了实现企业级联邦学习框架的架构设计，能够桥接研究原型与企业级部署之间的差距。

Conclusion: 该框架旨在实现可扩展、可靠且隐私保护的科学AI，为联邦学习从研究到企业级应用提供完整解决方案。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [30] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG是一个纯软件框架，通过将MIG的一对一分配模型改为一对多分配模型，并支持跨MIG实例的主机共享内存集合通信，显著提升了GPU集群的利用率和效率。


<details>
  <summary>Details</summary>
Motivation: 多租户环境下的GPU集群存在利用率低的问题，NVIDIA MIG虽然提供硬件级隔离，但其硬件刚性和传统一对一分配模型导致严重的资源碎片化和集群利用率低下。

Method: Flex-MIG采用纯软件方法，将MIG的操作模型重新设计为软件协调层，实现一对多分配模型，并支持跨MIG实例的主机共享内存集合通信，无需硬件修改。

Result: Flex-MIG消除了需要排空的重配置需求，减少了碎片化，在多样化跟踪测试中将makespan提高了高达17%。

Conclusion: 将MIG的操作模型重新思考为软件协调层可以显著提高集群效率，Flex-MIG展示了在不修改硬件的情况下通过软件创新解决GPU资源共享问题的可行性。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [31] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: 本文提出了一种新的用户空间任务调度方法CES，通过结合交换调度来优化协程等用户空间任务的同步性能，避免了传统用户空间同步原语引入的不必要延迟。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言广泛支持协程用于高并行或异步应用，用户空间同步避免了重量级系统调用，但现有用户空间同步原语仍从内核级调度视角处理同步，导致关键路径上出现不必要延迟，限制吞吐量。

Method: 开发了Combine-and-Exchange Scheduling (CES)方法，确保竞争临界区保持在同一个执行线程上，同时将可并行工作均匀分布到其他线程。

Result: 该方法可应用于多种现有语言和库，在应用基准测试中实现3倍性能提升，在微基准测试中实现8倍性能提升。

Conclusion: CES为完全在用户空间调度的任务重新思考了同步机制，显著提高了用户空间任务的性能表现。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [32] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 本文提出了一种名为循环内存保护(CMP)的无协调队列，在保持严格FIFO语义、无界容量和无锁进度的同时恢复了简单性。CMP通过有界保护窗口提供实际的重用保证，性能比现有无锁队列提升1.72-4倍。


<details>
  <summary>Details</summary>
Motivation: 传统无锁队列实现为了防范ABA、释放后使用等风险而引入复杂的协调机制，这些保护机制的设计复杂度往往超过了队列本身。在AI时代，训练和推理管道涉及数百到数千个并发线程，保护开销变得不可忽视。

Method: 采用循环内存保护(CMP)机制，通过有界保护窗口提供实际的重用保证，避免了无限保护带来的复杂性和开销，同时保持严格FIFO语义和无锁进度。

Result: 实验证明CMP在高竞争条件下比最先进的无锁队列性能提升1.72-4倍，并能扩展到数百个线程。通过线性化性和有界重用分析证明了严格FIFO和安全性。

Conclusion: 高度并发的队列可以在不削弱队列语义的情况下回归基本简单性，CMP展示了通过实用主义方法而非理论完美主义来实现高性能并发数据结构的可行性。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [33] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA是一种针对空间数据流架构的编程语言，通过抽象底层架构细节，提供对数据放置、数据流模式和异步操作的精确控制，显著简化了Cerebras等空间数据流架构的编程难度。


<details>
  <summary>Details</summary>
Motivation: 现有的FPGA和CGRA编程模型主要关注循环调度，但忽略了空间数据流架构的独特能力，特别是高效的数据流管理和复杂路由控制，导致编程这些架构仍然具有挑战性。

Method: 提出SPADA编程语言，引入严格的数据流语义框架来定义路由正确性、数据竞争和死锁，并设计实现针对Cerebras CSL的多级降低编译器。

Result: SPADA能够用比CSL少6-8倍的代码表达复杂的并行模式，包括流水线归约和多维模板，并在三个数量级上实现接近理想的弱扩展性。

Conclusion: SPADA通过统一空间数据流架构的编程模型，推动了这些新兴高性能计算平台的理论基础和实践可用性。

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [34] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 本文使用CSP进程代数对Python联邦学习测试床中的第三个通用算法（通用TDM通信）进行形式化验证，通过模型检查器PAT自动证明其死锁自由性和成功终止性。


<details>
  <summary>Details</summary>
Motivation: 在前一篇论文中已经对前两个通用算法进行了形式化验证，本文旨在使用相同的方法验证第三个通用算法，确保其正确性。

Method: 采用两阶段方法：第一阶段构建忠实反映Python代码的CSP模型；第二阶段使用模型检查器PAT自动验证算法的安全性和活性属性。

Result: 成功证明了第三个通用算法的死锁自由性（安全性属性）和成功终止性（活性属性）。

Conclusion: 通过形式化验证方法确认了Python联邦学习测试床中第三个通用算法的正确性，为边缘系统中的联邦学习算法提供了可靠保证。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>
