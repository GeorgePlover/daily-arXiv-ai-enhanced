{"id": "2512.09277", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09277", "abs": "https://arxiv.org/abs/2512.09277", "authors": ["Yanpeng Yu", "Haiyue Ma", "Krish Agarwal", "Nicolai Oswald", "Qijing Huang", "Hugo Linsenmaier", "Chunhui Mei", "Ritchie Zhao", "Ritika Borkar", "Bita Darvish Rouhani", "David Nellans", "Ronny Krashinsky", "Anurag Khandelwal"], "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens", "comment": null, "summary": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.\n  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.", "AI": {"tldr": "METRO\u662f\u4e00\u79cd\u9488\u5bf9\u5185\u5b58\u53d7\u9650\u573a\u666f\u7684MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u670d\u52a1\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861GPU\u4e0a\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u4e13\u5bb6\u5e76\u884c\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u5404GPU\u5904\u7406\u7684\u4ee4\u724c\u6570\u91cf\u6765\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u4f46\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\u573a\u666f\uff08\u7279\u522b\u662f\u89e3\u7801\u9636\u6bb5\uff09\u4e2d\uff0c\u8fd9\u79cd\u505a\u6cd5\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u589e\u52a0\u4e86\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\uff0c\u52a0\u5267\u4e86\u5185\u5b58\u538b\u529b\u3002", "method": "\u63d0\u51faMETRO\uff08Minimum Expert Token ROuting\uff09\u7b97\u6cd5\uff0c\u5728\u5185\u5b58\u53d7\u9650\u573a\u666f\u4e0b\u5e73\u8861\u6bcf\u4e2aGPU\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\u3002\u91c7\u7528\u65b0\u9896\u7684allGather\u65b9\u6848\u6536\u96c6\u5168\u5c40top-k\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684allToAll\u5177\u6709\u6700\u5c0f\u5f00\u9500\u3002\u7b97\u6cd5\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548cGPU\u5e76\u884c\u5904\u7406\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u7cfb\u7edf\uff088\u4e2aA100 GPU\u4e0a\u7684vLLM\uff09\u548c\u4e13\u6709\u6a21\u62df\u5668\uff088-16\u4e2aB200 GPU\uff09\u4e0a\u8bc4\u4f30\uff0cMETRO\u76f8\u6bd4EPLB\u5c06\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e11-22%\uff0cQwen3\u548cDeepSeek-V3\u670d\u52a1\u7684\u603b\u4ee4\u724c\u541e\u5410\u91cf\u63d0\u9ad83-21%\u3002\u5728\u56fa\u5b9a\u89e3\u7801SLO\u4e0b\uff0cMETRO\u7684\u89e3\u7801\u541e\u5410\u91cf\u6bd4EPLB\u63d0\u9ad8\u6700\u591a4.11\u500d\u3002", "conclusion": "\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\u573a\u666f\u4e2d\uff0c\u5e73\u8861\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\u662f\u66f4\u6709\u6548\u7684\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u3002METRO\u7b97\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u4e13\u5bb6\u6fc0\u6d3b\u548c\u9ad8\u6548\u7684\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u5e76\u884cMoE\u670d\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2512.09309", "categories": ["cs.DC", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09309", "abs": "https://arxiv.org/abs/2512.09309", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "comment": "16 pages, 7 figures. Published in the Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing (SEC '25), Dec 3-6, 2025, Washington, D.C., USA", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eVision Transformers\u7684\u5206\u5e03\u5f0f\u5206\u5c42\u5378\u8f7d\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5272\u89c6\u89c9\u6570\u636e\u5e76\u5206\u53d1\u5230\u591a\u4e2a\u72ec\u7acb\u4e91\u670d\u52a1\u5668\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u9632\u6b62\u5b8c\u6574\u56fe\u50cf\u91cd\u5efa\u3002", "motivation": "\u89c6\u89c9\u667a\u80fd\u5de5\u5177\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u8d85\u51fa\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u80fd\u529b\u3002\u4f20\u7edf\u4e91\u5378\u8f7d\u65b9\u6848\u5728\u4f20\u8f93\u548c\u670d\u52a1\u5668\u7aef\u8ba1\u7b97\u65f6\u5b58\u5728\u663e\u8457\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u9700\u8981\u8bbe\u8ba1\u9690\u79c1\u4fdd\u62a4\u7684\u8fb9\u7f18-\u4e91\u534f\u540c\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u672c\u5730\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u624b\u673a\u6216Nvidia Jetson\uff09\u4f5c\u4e3a\u8fb9\u7f18\u534f\u8c03\u5668\uff0c\u5c06\u7528\u6237\u89c6\u89c9\u6570\u636e\u5206\u5272\u6210\u5c0f\u90e8\u5206\u5e76\u5206\u53d1\u5230\u591a\u4e2a\u72ec\u7acb\u4e91\u670d\u52a1\u5668\uff0c\u786e\u4fdd\u6ca1\u6709\u5355\u4e2a\u5916\u90e8\u670d\u52a1\u5668\u62e5\u6709\u5b8c\u6574\u56fe\u50cf\uff0c\u6700\u7ec8\u6570\u636e\u5408\u5e76\u548c\u805a\u5408\u8ba1\u7b97\u4ec5\u5728\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u3002", "result": "\u4ee5Segment Anything Model (SAM)\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u5206\u5272\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5bb9\u91cd\u5efa\u548c\u7528\u6237\u6570\u636e\u66b4\u9732\u7684\u98ce\u9669\uff0c\u76f8\u6bd4\u4f20\u7edf\u4e91\u65b9\u6cd5\u5927\u5e45\u589e\u5f3a\u4e86\u5185\u5bb9\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9632\u6b62\u4e86\u5b8c\u6574\u6570\u636e\u91cd\u5efa\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u5378\u8f7d\u9700\u6c42\u4e0e\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\u3002"}}
{"id": "2512.09331", "categories": ["cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09331", "abs": "https://arxiv.org/abs/2512.09331", "authors": ["Nam Anh Dang", "Ben Landrum", "Ken Birman"], "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN", "comment": "12 pages, 14 figures, submitted to VLDB 2026", "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.", "AI": {"tldr": "BatANN\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u67e5\u8be2\u72b6\u6001\u5b8c\u6574\u53d1\u9001\u5230\u6570\u636e\u6240\u5728\u673a\u5668\u6765\u4fdd\u6301\u5355\u5168\u5c40\u56fe\u7684\u641c\u7d22\u6548\u7387\uff0c\u5b9e\u73b0\u8fd1\u7ebf\u6027\u541e\u5410\u6269\u5c55\u3002", "motivation": "\u968f\u7740\u6570\u636e\u96c6\u6269\u5c55\u5230\u6570\u5341\u4ebf\u5411\u91cf\uff0c\u78c1\u76d8\u5411\u91cf\u641c\u7d22\u6210\u4e3a\u5b9e\u7528\u65b9\u6848\uff0c\u4f46\u672a\u6765\u9700\u8981\u5904\u7406\u5355\u4e2a\u670d\u52a1\u5668\u65e0\u6cd5\u5bb9\u7eb3\u7684\u8d85\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u9700\u8981\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u78c1\u76d8\u8fd1\u4f3c\u6700\u8fd1\u90bb\u7cfb\u7edf\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5f53\u8bbf\u95ee\u5b58\u50a8\u5728\u5176\u4ed6\u673a\u5668\u4e0a\u7684\u90bb\u57df\u65f6\uff0c\u5c06\u67e5\u8be2\u7684\u5b8c\u6574\u72b6\u6001\u53d1\u9001\u5230\u8be5\u673a\u5668\u7ee7\u7eed\u6267\u884c\uff0c\u4ee5\u63d0\u9ad8\u5c40\u90e8\u6027\u3002", "result": "\u57281\u4ebf\u548c10\u4ebf\u70b9\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752810\u53f0\u670d\u52a1\u5668\uff0c\u57280.95\u53ec\u56de\u7387\u4e0b\uff0c\u541e\u5410\u91cf\u5206\u522b\u8fbe\u5230\u57fa\u7ebf\u65b9\u6cd5\u76846.21-6.49\u500d\u548c2.5-5.10\u500d\uff0c\u5e73\u5747\u5ef6\u8fdf\u4f4e\u4e8e6\u6beb\u79d2\u3002", "conclusion": "BatANN\u662f\u9996\u4e2a\u57fa\u4e8e\u5355\u5168\u5c40\u56fe\u7684\u5f00\u6e90\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\uff0c\u5728\u6807\u51c6TCP\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u6570\u641c\u7d22\u6548\u7387\u548c\u8fd1\u7ebf\u6027\u541e\u5410\u6269\u5c55\u3002"}}
{"id": "2512.09472", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09472", "abs": "https://arxiv.org/abs/2512.09472", "authors": ["Chiheng Lou", "Sheng Qi", "Rui Kang", "Yong Zhang", "Chen Sun", "Pengcheng Wang", "Bingyang Liu", "Xuanzhe Liu", "Xin Jin"], "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving", "comment": null, "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.", "AI": {"tldr": "WarmServe\u662f\u4e00\u4e2a\u591aLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u77e5\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\uff0c\u4f7f\u7528\u901a\u7528GPU\u5de5\u4f5c\u8282\u70b9\u5b9e\u73b0\u4e00\u5bf9\u591aGPU\u9884\u70ed\uff0c\u663e\u8457\u63d0\u5347\u9996\u4ee4\u724c\u65f6\u95f4\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u591aLLM\u670d\u52a1\u7cfb\u7edf\u5728\u5171\u4eabGPU\u96c6\u7fa4\u4e2d\u4f18\u5316GPU\u5229\u7528\u7387\u65f6\uff0c\u5f80\u5f80\u4ee5\u727a\u7272\u63a8\u7406\u6027\u80fd\u4e3a\u4ee3\u4ef7\uff0c\u7279\u522b\u662f\u9996\u4ee4\u724c\u65f6\u95f4(TTFT)\u8868\u73b0\u8f83\u5dee\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u6298\u8877\u7684\u6839\u672c\u539f\u56e0\u662f\u8fd9\u4e9b\u7cfb\u7edf\u5bf9\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u7f3a\u4e4f\u8ba4\u77e5\uff0c\u800c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u5468\u671f\u6027\u548c\u957f\u671f\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u63d0\u51fa\u901a\u7528GPU\u5de5\u4f5c\u8282\u70b9\u5b9e\u73b0\u4e00\u5bf9\u591aGPU\u9884\u70ed\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1WarmServe\u7cfb\u7edf\uff1a1) \u91c7\u7528\u9a71\u9010\u611f\u77e5\u6a21\u578b\u653e\u7f6e\u7b56\u7565\u7f13\u89e3\u96c6\u7fa4\u8303\u56f4\u7684\u9884\u70ed\u5e72\u6270\uff1b2) \u901a\u8fc7\u4e3b\u52a8\u9884\u70ed\u63d0\u524d\u51c6\u5907\u901a\u7528GPU\u5de5\u4f5c\u8282\u70b9\uff1b3) \u4f7f\u7528\u96f6\u5f00\u9500\u5185\u5b58\u5207\u6362\u673a\u5236\u7ba1\u7406GPU\u5185\u5b58\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\uff0cWarmServe\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u81ea\u52a8\u6269\u5c55\u7684\u7cfb\u7edf\uff0cTTFT\u63d0\u5347\u9ad8\u8fbe50.8\u500d\uff0c\u540c\u65f6\u76f8\u6bd4GPU\u5171\u4eab\u7cfb\u7edf\u80fd\u591f\u670d\u52a1\u591a\u8fbe2.5\u500d\u7684\u8bf7\u6c42\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5468\u671f\u6027\u548c\u53ef\u9884\u6d4b\u6027\uff0cWarmServe\u6210\u529f\u89e3\u51b3\u4e86\u591aLLM\u670d\u52a1\u4e2d\u8d44\u6e90\u5229\u7528\u7387\u4e0e\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9ad8\u8d44\u6e90\u6548\u7387\u7684\u7edf\u4e00\u3002"}}
{"id": "2512.09088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09088", "abs": "https://arxiv.org/abs/2512.09088", "authors": ["Adrian Ryser", "Florian Allwein", "Tim Schlippe"], "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study", "comment": null, "summary": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Bl\u00f6baum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u4f1a\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\uff0c\u4f46\u4e0d\u4f1a\u5bfc\u81f4\u5168\u9762\u4e0d\u4fe1\u4efb\uff0c\u800c\u662f\u5f15\u53d1\u60c5\u5883\u5316\u7684\u4fe1\u4efb\u6821\u51c6\u3002\u7814\u7a76\u53d1\u73b0\u7528\u6237\u76f8\u5173\u56e0\u7d20\uff08\u671f\u671b\u3001\u5148\u524d\u7ecf\u9a8c\u3001\u4e13\u4e1a\u77e5\u8bc6\uff09\u548c\u76f4\u89c9\u5f71\u54cd\u4fe1\u4efb\uff0c\u60c5\u5883\u56e0\u7d20\uff08\u611f\u77e5\u98ce\u9669\u3001\u51b3\u7b56\u98ce\u9669\uff09\u4e5f\u8d77\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7684\u5e7b\u89c9\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9LLM\u7684\u4fe1\u4efb\u4ee5\u53ca\u7528\u6237\u4e0eLLM\u7684\u4e92\u52a8\u3002\u5e7b\u89c9\u662fLLM\u8f93\u51fa\u7684\u4e8b\u5b9e\u9519\u8bef\u4f46\u770b\u4f3c\u5408\u7406\u7684\u5185\u5bb9\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u9519\u8bef\u5982\u4f55\u5f71\u54cd\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u4fe1\u4efb\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9192\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u7814\u7a76\uff0c\u63a2\u7d22\u65e5\u5e38\u4f7f\u7528\u4e2d\u5e7b\u89c9\u5bf9\u4fe1\u4efb\u7684\u5f71\u54cd\u3002\u57fa\u4e8eLee & See\u7684\u6821\u51c6\u4fe1\u4efb\u6a21\u578b\u548cAfroogh\u7b49\u4eba\u7684\u4fe1\u4efb\u76f8\u5173\u56e0\u7d20\u7406\u8bba\u6846\u67b6\u3002", "result": "\u5e7b\u89c9\u4e0d\u4f1a\u5bfc\u81f4\u5168\u9762\u4e0d\u4fe1\u4efb\uff0c\u800c\u662f\u5f15\u53d1\u60c5\u5883\u654f\u611f\u7684\u4fe1\u4efb\u6821\u51c6\u3002\u786e\u8ba4\u4e86\u671f\u671b\u3001\u5148\u524d\u7ecf\u9a8c\u3001\u7528\u6237\u4e13\u4e1a\u77e5\u8bc6\u548c\u9886\u57df\u77e5\u8bc6\u4f5c\u4e3a\u7528\u6237\u76f8\u5173\u4fe1\u4efb\u56e0\u7d20\uff0c\u5e76\u53d1\u73b0\u76f4\u89c9\u662f\u5e7b\u89c9\u68c0\u6d4b\u7684\u989d\u5916\u56e0\u7d20\u3002\u4fe1\u4efb\u52a8\u6001\u8fd8\u53d7\u60c5\u5883\u56e0\u7d20\u5f71\u54cd\uff0c\u7279\u522b\u662f\u611f\u77e5\u98ce\u9669\u548c\u51b3\u7b56\u98ce\u9669\u3002\u9a8c\u8bc1\u4e86Bl\u00f6baum\u7684\u9012\u5f52\u4fe1\u4efb\u6821\u51c6\u8fc7\u7a0b\uff0c\u5e76\u6269\u5c55\u4e86\u76f4\u89c9\u4f5c\u4e3a\u7528\u6237\u76f8\u5173\u4fe1\u4efb\u56e0\u7d20\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u8d1f\u8d23\u4efb\u548c\u53cd\u601d\u6027LLM\u4f7f\u7528\u7684\u5b9e\u7528\u5efa\u8bae\u3002\u7814\u7a76\u6269\u5c55\u4e86\u4fe1\u4efb\u6821\u51c6\u7406\u8bba\uff0c\u5f3a\u8c03\u4e86\u76f4\u89c9\u5728\u68c0\u6d4b\u5e7b\u89c9\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u7406\u89e3\u7528\u6237\u5982\u4f55\u9002\u5e94LLM\u7684\u4e0d\u5b8c\u7f8e\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2512.09187", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09187", "abs": "https://arxiv.org/abs/2512.09187", "authors": ["Mrinal Agarwal", "Saad Rana", "Theo Sundoro", "Hermela Berhe", "Spencer Kim", "Vasu Sharma", "Sean O'Brien", "Kevin Zhu"], "title": "WOLF: Werewolf-based Observations for LLM Deception and Falsehoods", "comment": "Spotlight Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop at NeurIPS 2025", "summary": "Deception is a fundamental challenge for multi-agent reasoning: effective systems must strategically conceal information while detecting misleading behavior in others. Yet most evaluations reduce deception to static classification, ignoring the interactive, adversarial, and longitudinal nature of real deceptive dynamics. Large language models (LLMs) can deceive convincingly but remain weak at detecting deception in peers. We present WOLF, a multi-agent social deduction benchmark based on Werewolf that enables separable measurement of deception production and detection. WOLF embeds role-grounded agents (Villager, Werewolf, Seer, Doctor) in a programmable LangGraph state machine with strict night-day cycles, debate turns, and majority voting. Every statement is a distinct analysis unit, with self-assessed honesty from speakers and peer-rated deceptiveness from others. Deception is categorized via a standardized taxonomy (omission, distortion, fabrication, misdirection), while suspicion scores are longitudinally smoothed to capture both immediate judgments and evolving trust dynamics. Structured logs preserve prompts, outputs, and state transitions for full reproducibility. Across 7,320 statements and 100 runs, Werewolves produce deceptive statements in 31% of turns, while peer detection achieves 71-73% precision with ~52% overall accuracy. Precision is higher for identifying Werewolves, though false positives occur against Villagers. Suspicion toward Werewolves rises from ~52% to over 60% across rounds, while suspicion toward Villagers and the Doctor stabilizes near 44-46%. This divergence shows that extended interaction improves recall against liars without compounding errors against truthful roles. WOLF moves deception evaluation beyond static datasets, offering a dynamic, controlled testbed for measuring deceptive and detective capacity in adversarial multi-agent interaction.", "AI": {"tldr": "WOLF\u662f\u4e00\u4e2a\u57fa\u4e8e\u72fc\u4eba\u6740\u7684\u591a\u667a\u80fd\u4f53\u793e\u4ea4\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u5206\u79bb\u6d4b\u91cf\u6b3a\u9a97\u4ea7\u751f\u548c\u68c0\u6d4b\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6e38\u620f\u673a\u5236\u548c\u6807\u51c6\u5316\u5206\u7c7b\u5b9e\u73b0\u52a8\u6001\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5c06\u6b3a\u9a97\u7b80\u5316\u4e3a\u9759\u6001\u5206\u7c7b\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u6b3a\u9a97\u52a8\u6001\u7684\u4ea4\u4e92\u6027\u3001\u5bf9\u6297\u6027\u548c\u957f\u671f\u6027\u3002\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u6709\u8bf4\u670d\u529b\u7684\u6b3a\u9a97\uff0c\u4f46\u5728\u68c0\u6d4b\u540c\u4f34\u6b3a\u9a97\u65b9\u9762\u8868\u73b0\u8f83\u5f31\u3002", "method": "\u57fa\u4e8e\u72fc\u4eba\u6740\u6e38\u620f\u6784\u5efa\u591a\u667a\u80fd\u4f53\u793e\u4ea4\u63a8\u7406\u57fa\u51c6\uff0c\u4f7f\u7528\u53ef\u7f16\u7a0bLangGraph\u72b6\u6001\u673a\uff0c\u5305\u542b\u4e25\u683c\u7684\u663c\u591c\u5faa\u73af\u3001\u8fa9\u8bba\u8f6e\u6b21\u548c\u591a\u6570\u6295\u7968\u673a\u5236\u3002\u6bcf\u4e2a\u9648\u8ff0\u4f5c\u4e3a\u72ec\u7acb\u5206\u6790\u5355\u5143\uff0c\u4f7f\u7528\u6807\u51c6\u5316\u5206\u7c7b\u6cd5\uff08\u9057\u6f0f\u3001\u626d\u66f2\u3001\u634f\u9020\u3001\u8bef\u5bfc\uff09\u5bf9\u6b3a\u9a97\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u7eb5\u5411\u5e73\u6ed1\u7684\u6000\u7591\u5206\u6570\u6355\u6349\u4fe1\u4efb\u52a8\u6001\u53d8\u5316\u3002", "result": "\u57287,320\u4e2a\u9648\u8ff0\u548c100\u6b21\u8fd0\u884c\u4e2d\uff0c\u72fc\u4eba\u572831%\u7684\u56de\u5408\u4e2d\u4ea7\u751f\u6b3a\u9a97\u6027\u9648\u8ff0\uff0c\u540c\u4f34\u68c0\u6d4b\u8fbe\u523071-73%\u7684\u7cbe\u786e\u7387\u548c\u7ea652%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u5bf9\u72fc\u4eba\u7684\u6000\u7591\u4ece\u7ea652%\u4e0a\u5347\u5230\u8d85\u8fc760%\uff0c\u800c\u5bf9\u6751\u6c11\u548c\u533b\u751f\u7684\u6000\u7591\u7a33\u5b9a\u572844-46%\u5de6\u53f3\u3002", "conclusion": "WOLF\u5c06\u6b3a\u9a97\u8bc4\u4f30\u4ece\u9759\u6001\u6570\u636e\u96c6\u6269\u5c55\u5230\u52a8\u6001\u3001\u53d7\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u80fd\u591f\u6d4b\u91cf\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u6b3a\u9a97\u548c\u68c0\u6d4b\u80fd\u529b\uff0c\u6269\u5c55\u4e92\u52a8\u63d0\u9ad8\u4e86\u5bf9\u8bf4\u8c0e\u8005\u7684\u53ec\u56de\u7387\uff0c\u800c\u4e0d\u4f1a\u589e\u52a0\u5bf9\u8bda\u5b9e\u89d2\u8272\u7684\u9519\u8bef\u3002"}}
{"id": "2512.09114", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09114", "abs": "https://arxiv.org/abs/2512.09114", "authors": ["Pamela Gupta"], "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance", "comment": "47 pages", "summary": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.", "AI": {"tldr": "AI TIPS 2.0\u6846\u67b6\u89e3\u51b3\u73b0\u6709AI\u6cbb\u7406\u6846\u67b6\u7684\u4e09\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\u7528\u4f8b\u98ce\u9669\u8bc4\u4f30\u4e0d\u8db3\u3001\u539f\u5219\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u6027\u63a7\u5236\u3001\u89c4\u6a21\u5316\u5b9e\u65bd\u673a\u5236\u7f3a\u5931", "motivation": "\u5f53\u524dAI\u6cbb\u7406\u6846\u67b6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u7f3a\u4e4f\u9488\u5bf9\u5177\u4f53\u7528\u4f8b\u7684\u98ce\u9669\u8bc4\u4f30\uff08\u5982Humana\u96c6\u4f53\u8bc9\u8bbc\u6848\u4f8b\uff09\uff1b2) \u73b0\u6709\u6846\u67b6\u5982ISO 42001\u548cNIST AI RMF\u505c\u7559\u5728\u6982\u5ff5\u5c42\u9762\uff0c\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u63a7\u5236\uff1b3) \u7ec4\u7ec7\u7f3a\u4e4f\u89c4\u6a21\u5316\u5b9e\u65bd\u6cbb\u7406\u7684\u673a\u5236", "method": "\u63d0\u51faAI TIPS 2.0\uff08\u4eba\u5de5\u667a\u80fd\u4fe1\u4efb\u96c6\u6210\u652f\u67f1\u53ef\u6301\u7eed\u6027\u6846\u67b6\uff09\uff0c\u8fd9\u662f2019\u5e74\u5f00\u53d1\u7684\u5168\u9762\u64cd\u4f5c\u6846\u67b6\u7684\u66f4\u65b0\u7248\u672c\uff0c\u6bd4NIST AI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\u65e9\u56db\u5e74\uff0c\u76f4\u63a5\u9488\u5bf9\u4e0a\u8ff0\u6311\u6218\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848", "result": "AI TIPS 2.0\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6cbb\u7406\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6cbb\u7406\u65b9\u6cd5\uff0c\u652f\u6301\u4ece\u8463\u4e8b\u4f1a\u5230\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u89d2\u8272\u5316\u53ef\u89c1\u6027", "conclusion": "\u9700\u8981\u65b0\u7684AI\u6cbb\u7406\u6846\u67b6\u6765\u586b\u8865\u73b0\u6709\u6846\u67b6\u7684\u7a7a\u767d\uff0cAI TIPS 2.0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u64cd\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u51b3\u7528\u4f8b\u98ce\u9669\u8bc4\u4f30\u3001\u53ef\u64cd\u4f5c\u6027\u63a7\u5236\u548c\u89c4\u6a21\u5316\u5b9e\u65bd\u7b49\u5173\u952e\u6311\u6218"}}
{"id": "2512.09568", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09568", "abs": "https://arxiv.org/abs/2512.09568", "authors": ["Zhi Zhao", "Hang Xiao", "Wei Rang"], "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing", "comment": "24 pages,5 figures", "summary": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e15\u7d2f\u6258\u7684\u6df7\u5408\u9cb8\u9c7c-\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5(PHWSOA)\uff0c\u7528\u4e8e\u89e3\u51b3\u4e91\u8ba1\u7b97\u4e2d\u7684\u591a\u76ee\u6807\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u6267\u884c\u65f6\u95f4\u3001\u865a\u62df\u673a\u8d1f\u8f7d\u5e73\u8861\u548c\u7ecf\u6d4e\u6210\u672c\u4e09\u4e2a\u76ee\u6807\u3002", "motivation": "\u4e91\u8ba1\u7b97\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u662f\u4e00\u4e2a\u5173\u952e\u7814\u7a76\u6311\u6218\u3002\u73b0\u6709\u8c03\u5ea6\u65b9\u6848\u5927\u591a\u53ea\u4f18\u5316\u5355\u4e00\u6216\u6709\u9650\u6307\u6807\uff08\u5982\u6267\u884c\u65f6\u95f4\u6216\u8d44\u6e90\u5229\u7528\u7387\uff09\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPHWSOA\u7b97\u6cd5\uff0c\u7ed3\u5408\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5(WOA)\u548c\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5(SOA)\u7684\u4f18\u52bf\uff0c\u5f25\u8865\u5404\u81ea\u5728\u5c40\u90e8\u5f00\u53d1\u548c\u5168\u5c40\u63a2\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u91c7\u7528Halton\u5e8f\u5217\u521d\u59cb\u5316\u63d0\u9ad8\u79cd\u7fa4\u591a\u6837\u6027\uff0c\u5e15\u7d2f\u6258\u5f15\u5bfc\u53d8\u5f02\u673a\u5236\u9632\u6b62\u65e9\u719f\u6536\u655b\uff0c\u5e76\u884c\u5904\u7406\u52a0\u901f\u6536\u655b\uff0c\u5e76\u96c6\u6210\u52a8\u6001\u865a\u62df\u673a\u8d1f\u8f7d\u91cd\u5206\u914d\u673a\u5236\u3002", "result": "\u5728CloudSim\u6a21\u62df\u5668\u4e0a\u4f7f\u7528NASA-iPSC\u548cHPC2N\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5b9e\u9a8c\uff0cPHWSOA\u5b9e\u73b0\u4e86\uff1a\u6267\u884c\u65f6\u95f4\u6700\u591a\u51cf\u5c1172.1%\uff0c\u865a\u62df\u673a\u8d1f\u8f7d\u5e73\u8861\u6539\u558436.8%\uff0c\u6210\u672c\u8282\u770123.5%\u3002\u6027\u80fd\u663e\u8457\u4f18\u4e8eWOA\u3001GA\u3001PEWOA\u548cGCWOA\u7b49\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "PHWSOA\u7b97\u6cd5\u5728\u4e91\u8ba1\u7b97\u4efb\u52a1\u8c03\u5ea6\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u591a\u76ee\u6807\u4f18\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u4e91\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.09117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09117", "abs": "https://arxiv.org/abs/2512.09117", "authors": ["Luciano Floridi", "Yiyang Jia", "Fernando Tohm\u00e9"], "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem", "comment": null, "summary": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u8303\u7574\u6846\u67b6\uff0c\u5206\u6790\u4eba\u7c7b\u548cLLM\u5982\u4f55\u5c06\u5185\u5bb9\u8f6c\u5316\u4e3a\u5173\u4e8e\u53ef\u80fd\u4e16\u754c\u72b6\u6001\u7a7a\u95f4W\u7684\u771f\u503c\u547d\u9898\uff0c\u8bba\u8bc1LLM\u5e76\u975e\u89e3\u51b3\u800c\u662f\u7ed5\u8fc7\u4e86\u7b26\u53f7\u63a5\u5730\u95ee\u9898", "motivation": "\u7814\u7a76\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u5185\u5bb9\u8f6c\u5316\u4e3a\u771f\u503c\u547d\u9898\u65f6\u7684\u8ba4\u77e5\u5dee\u5f02\uff0c\u7279\u522b\u662f\u63a2\u8ba8LLM\u662f\u5426\u771f\u6b63\u89e3\u51b3\u4e86\u7b26\u53f7\u63a5\u5730\u95ee\u9898", "method": "\u4f7f\u7528\u5f62\u5f0f\u5316\u8303\u7574\u7406\u8bba\u6846\u67b6\uff0c\u6784\u5efa\u57fa\u4e8e\u53ef\u80fd\u4e16\u754c\u72b6\u6001\u7a7a\u95f4W\u7684\u5206\u6790\u6a21\u578b\uff0c\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u5728\u547d\u9898\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5dee\u5f02", "result": "LLM\u901a\u8fc7\u7edf\u8ba1\u6a21\u5f0f\u800c\u975e\u771f\u5b9e\u8bed\u4e49\u7406\u89e3\u6765\u751f\u6210\u547d\u9898\uff0c\u5b9e\u9645\u4e0a\u7ed5\u8fc7\u4e86\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u800c\u975e\u89e3\u51b3\u5b83", "conclusion": "LLM\u7f3a\u4e4f\u771f\u6b63\u7684\u7b26\u53f7\u63a5\u5730\u80fd\u529b\uff0c\u5176\u547d\u9898\u751f\u6210\u673a\u5236\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u8fd9\u5bf9\u7406\u89e3AI\u7684\u8bed\u4e49\u5904\u7406\u80fd\u529b\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2512.09548", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09548", "abs": "https://arxiv.org/abs/2512.09548", "authors": ["Ioana Giurgiu", "Michael E. Nidd"], "title": "Supporting Dynamic Agentic Workloads: How Data and Agents Interact", "comment": null, "summary": "The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u667a\u80fd\u4f53\u7684\u6570\u636e\u67b6\u6784\uff0c\u89e3\u51b3\u4f20\u7edf\u6570\u636e\u5e93\u65e0\u6cd5\u9002\u5e94LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u9a71\u52a8\u3001\u534f\u4f5c\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u95ee\u9898", "motivation": "\u4f20\u7edf\u6570\u636e\u5e93\u67b6\u6784\u8bbe\u8ba1\u7528\u4e8e\u9759\u6001\u3001\u5b9a\u4e49\u660e\u786e\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8868\u73b0\u51fa\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u9a71\u52a8\u3001\u534f\u4f5c\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5bfc\u81f4\u4f20\u7edf\u67e5\u8be2\u4f18\u5316\u5668\u548c\u7f13\u5b58\u673a\u5236\u96be\u4ee5\u5e94\u5bf9", "method": "\u63d0\u51fa\u667a\u80fd\u4f53\u4e2d\u5fc3\u6570\u636e\u67b6\u6784\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u6570\u636e\u68c0\u7d22\u3001\u8bed\u4e49\u5fae\u7f13\u5b58\u3001\u9884\u6d4b\u6027\u6570\u636e\u9884\u53d6\u548c\u57fa\u4e8e\u6cd5\u5b9a\u4eba\u6570\u7684\u6570\u636e\u670d\u52a1\u7b49\u673a\u5236", "result": "\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u5730\u8bbf\u95ee\u4ee3\u8868\u6027\u6570\u636e\uff0c\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u67e5\u8be2\u3001\u6570\u636e\u79fb\u52a8\u548c\u7cfb\u7edf\u95f4\u63a8\u7406\u8d1f\u8f7d", "conclusion": "\u5c06\u6570\u636e\u7cfb\u7edf\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u9002\u5e94\u534f\u4f5c\u8005\u800c\u975e\u9759\u6001\u6267\u884c\u5668\uff0c\u4e3a\u884c\u4e3a\u54cd\u5e94\u5f0f\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u5f00\u8f9f\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u7f13\u5b58\u3001\u63a2\u6d4b\u548c\u7f16\u6392\u5b9e\u73b0\u52a8\u6001\u63a8\u7406\u9a71\u52a8\u667a\u80fd\u4f53\u95f4\u7684\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6570\u636e\u4ea4\u6362"}}
{"id": "2512.09664", "categories": ["cs.DC", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09664", "abs": "https://arxiv.org/abs/2512.09664", "authors": ["Antonio Terpin", "Alan Bonomi", "Francesco Banelli", "Raffaello D'Andrea"], "title": "SynthPix: A lightspeed PIV images generator", "comment": "Code: https://github.com/antonioterpin/synthpix", "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "AI": {"tldr": "SynthPix\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u5b9e\u73b0\u7684\u9ad8\u6027\u80fd\u5e76\u884c\u5408\u6210\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u7c92\u5b50\u56fe\u50cf\u6d4b\u901f\uff08PIV\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u5b9e\u73b0\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5f00\u53d1SynthPix\u7684\u4e3b\u8981\u52a8\u673a\u662f\u4e3a\u4e86\u652f\u6301\u6570\u636e\u9700\u6c42\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6d41\u573a\u4f30\u8ba1\u4e2d\u7684\u8bad\u7ec3\uff0c\u5e76\u51cf\u5c11\u5feb\u901f\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u5f00\u53d1\u65f6\u7684\u8fed\u4ee3\u65f6\u95f4\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5b9e\u65f6PIV\u53cd\u9988\u7684\u4e3b\u52a8\u6d41\u4f53\u63a7\u5236\u7814\u7a76\u4e2d\u3002", "method": "\u57fa\u4e8eJAX\u6846\u67b6\u5b9e\u73b0\uff0c\u5145\u5206\u5229\u7528\u52a0\u901f\u5668\u7684\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\uff0c\u652f\u6301\u4e0e\u73b0\u6709\u5de5\u5177\u76f8\u540c\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u4f46\u901a\u8fc7\u5e76\u884c\u5316\u67b6\u6784\u5927\u5e45\u63d0\u5347\u56fe\u50cf\u5bf9\u751f\u6210\u901f\u5ea6\u3002", "result": "SynthPix\u5728\u6bcf\u79d2\u56fe\u50cf\u5bf9\u751f\u6210\u541e\u5410\u91cf\u4e0a\u6bd4\u73b0\u6709\u5de5\u5177\u9ad8\u51fa\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901fPIV\u76f8\u5173\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "SynthPix\u662f\u4e00\u4e2a\u5bf9\u6d41\u4f53\u52a8\u529b\u5b66\u793e\u533a\u6709\u7528\u7684\u8f6f\u4ef6\u5305\uff0c\u80fd\u591f\u652f\u6301\u6570\u636e\u5bc6\u96c6\u578b\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u5b9e\u65f6PIV\u53cd\u9988\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2512.09142", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09142", "abs": "https://arxiv.org/abs/2512.09142", "authors": ["Sergio Burdisso", "S\u00e9verin Baroudi", "Yanis Labrak", "David Grunert", "Pawel Cyrta", "Yiyang Chen", "Srikanth Madikeri", "Esa\u00fa Villatoro-Tello", "Thomas Schaaf", "Ricard Marxer", "Petr Motlicek"], "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation", "comment": "Pre-print submitted to EACL System Demonstration (under review)", "summary": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.", "AI": {"tldr": "SDialog\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u96c6\u6210\u4e86\u5bf9\u8bdd\u751f\u6210\u3001\u8bc4\u4f30\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u6784\u5efa\u548c\u5206\u6790\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u7aef\u5230\u7aef\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bdd\u7cfb\u7edf\u5f00\u53d1\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u96be\u4ee5\u7cfb\u7edf\u5730\u8fdb\u884c\u751f\u6210\u3001\u8bc4\u4f30\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002SDialog\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u96c6\u6210\u7684\u5de5\u5177\u6765\u66f4\u7cfb\u7edf\u5730\u6784\u5efa\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u7406\u89e3\u5bf9\u8bdd\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u6807\u51c6\u5316\u7684Dialog\u8868\u793a\uff0cSDialog\u63d0\u4f9b\uff1a1\uff09\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u4e0e\u53ef\u7ec4\u5408\u7f16\u6392\uff1b2\uff09\u7ed3\u5408\u8bed\u8a00\u6307\u6807\u3001LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u548c\u529f\u80fd\u6b63\u786e\u6027\u9a8c\u8bc1\u5668\u7684\u7efc\u5408\u8bc4\u4f30\uff1b3\uff09\u901a\u8fc7\u7279\u5f81\u6d88\u878d\u548c\u8bf1\u5bfc\u8fdb\u884c\u6fc0\u6d3b\u68c0\u67e5\u548c\u5f15\u5bfc\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff1b4\uff09\u5305\u542b3D\u623f\u95f4\u5efa\u6a21\u548c\u9ea6\u514b\u98ce\u6548\u679c\u7684\u5b8c\u6574\u58f0\u5b66\u6a21\u62df\u97f3\u9891\u751f\u6210\u3002\u5de5\u5177\u5305\u4e0e\u6240\u6709\u4e3b\u8981LLM\u540e\u7aef\u96c6\u6210\u3002", "result": "SDialog\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u8bdd\u751f\u6210\u3001\u8bc4\u4f30\u548c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u6df7\u5408\u540e\u7aef\u5b9e\u9a8c\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6784\u5efa\u548c\u5206\u6790\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u3001\u8bc4\u4f30\u548c\u53ef\u89e3\u91ca\u6027\u8026\u5408\u5728\u5bf9\u8bdd\u4e2d\u5fc3\u67b6\u6784\u4e2d\uff0cSDialog\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u66f4\u7cfb\u7edf\u5730\u6784\u5efa\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u7406\u89e3\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u63a8\u52a8\u4e86\u5bf9\u8bddAI\u7814\u7a76\u7684\u7cfb\u7edf\u5316\u53d1\u5c55\u3002"}}
{"id": "2512.09831", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.09831", "abs": "https://arxiv.org/abs/2512.09831", "authors": ["Chainarong Amornbunchornvej"], "title": "Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning", "comment": "The first draft of cognitive geometry model", "summary": "This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.\n  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-\"the No-Null-Space Leadership Condition\"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.\n  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u8ba4\u77e5\u5f02\u6784\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4fe1\u5ff5\u3001\u52a8\u673a\u548c\u5f71\u54cd\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u7531\u4e2a\u6027\u5316\u4ef7\u503c\u7a7a\u95f4\u8868\u793a\uff0c\u4fe1\u5ff5\u88ab\u5f62\u5f0f\u5316\u4e3a\u7ed3\u6784\u5316\u5411\u91cf\uff0c\u5176\u4f20\u64ad\u901a\u8fc7\u7ebf\u6027\u89e3\u91ca\u6620\u5c04\u8fdb\u884c\u3002\u4fe1\u5ff5\u53ea\u6709\u5728\u907f\u514d\u8fd9\u4e9b\u6620\u5c04\u7684\u96f6\u7a7a\u95f4\u65f6\u624d\u80fd\u5728\u4ea4\u6d41\u4e2d\u5b58\u6d3b\uff0c\u8fd9\u4e3a\u53ef\u7406\u89e3\u6027\u3001\u8bef\u89e3\u548c\u4fe1\u5ff5\u6d88\u4ea1\u63d0\u4f9b\u4e86\u7ed3\u6784\u6807\u51c6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5206\u6790\u8ba4\u77e5\u5f02\u6784\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4fe1\u5ff5\u52a8\u6001\uff0c\u89e3\u51b3\u4fe1\u5ff5\u4f20\u64ad\u3001\u626d\u66f2\u548c\u6d88\u4ea1\u7684\u7ed3\u6784\u6027\u539f\u56e0\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5171\u4eab\u4fe1\u606f\u6216\u7406\u6027\u5047\u8bbe\uff0c\u800c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u548c\u4ee3\u6570\u7ea6\u675f\u6765\u7406\u89e3\u4fe1\u5ff5\u5728\u4e0d\u540c\u8ba4\u77e5\u7cfb\u7edf\u95f4\u7684\u4f20\u64ad\u673a\u5236\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u6bcf\u4e2a\u667a\u80fd\u4f53\u5efa\u6a21\u4e3a\u4e2a\u6027\u5316\u4ef7\u503c\u7a7a\u95f4\uff08\u5411\u91cf\u7a7a\u95f4\uff09\uff1b2) \u5c06\u4fe1\u5ff5\u5f62\u5f0f\u5316\u4e3a\u7ed3\u6784\u5316\u5411\u91cf\uff08\u62bd\u8c61\u5b58\u5728\uff09\uff1b3) \u4f7f\u7528\u7ebf\u6027\u89e3\u91ca\u6620\u5c04\u6765\u5efa\u6a21\u4fe1\u5ff5\u4f20\u64ad\uff1b4) \u5206\u6790\u96f6\u7a7a\u95f4\u6761\u4ef6\u5bf9\u4fe1\u5ff5\u5b58\u6d3b\u7684\u5f71\u54cd\uff1b5) \u63a8\u5bfc\u51fa\"\u65e0\u96f6\u7a7a\u95f4\u9886\u5bfc\u6761\u4ef6\"\u7b49\u4ee3\u6570\u7ea6\u675f\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a1) \u4fe1\u5ff5\u626d\u66f2\u3001\u52a8\u673a\u6f02\u79fb\u3001\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u548c\u76f8\u4e92\u7406\u89e3\u7684\u9650\u5236\u90fd\u6e90\u4e8e\u7eaf\u4ee3\u6570\u7ea6\u675f\uff1b2) \"\u65e0\u96f6\u7a7a\u95f4\u9886\u5bfc\u6761\u4ef6\"\u5c06\u9886\u5bfc\u529b\u7279\u5f81\u5316\u4e3a\u8868\u793a\u53ef\u8fbe\u6027\u800c\u975e\u8bf4\u670d\u6216\u6743\u5a01\uff1b3) \u89e3\u91ca\u4e86\u62bd\u8c61\u5b58\u5728\u5982\u4f55\u5728\u591a\u6837\u5316\u8ba4\u77e5\u51e0\u4f55\u4e2d\u4f20\u64ad\u3001\u53d8\u5f02\u6216\u6d88\u5931\uff1b4) \u4e3a\u4fe1\u5ff5\u52a8\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u7840\u3002", "conclusion": "\u8be5\u8ba4\u77e5\u51e0\u4f55\u89c6\u89d2\u901a\u8fc7\u5c06\u610f\u4e49\u4fdd\u5b58\u5efa\u7acb\u5728\u7ed3\u6784\u517c\u5bb9\u6027\u800c\u975e\u5171\u4eab\u4fe1\u606f\u6216\u7406\u6027\u7684\u57fa\u7840\u4e0a\uff0c\u7edf\u4e00\u4e86\u6982\u5ff5\u7a7a\u95f4\u3001\u793e\u4f1a\u8ba4\u8bc6\u8bba\u548cAI\u4ef7\u503c\u5bf9\u9f50\u7684\u89c1\u89e3\u3002\u8fd9\u4e00\u6846\u67b6\u9610\u660e\u4e86\u4eba\u7c7b\u548c\u4eba\u5de5\u7cfb\u7edf\u4e2d\u5f71\u54cd\u7684\u8ba4\u77e5\u8fb9\u754c\uff0c\u4e3a\u5206\u6790\u5f02\u6784\u667a\u80fd\u4f53\u95f4\u7684\u4fe1\u5ff5\u52a8\u6001\u63d0\u4f9b\u4e86\u901a\u7528\u57fa\u7840\u3002"}}
{"id": "2512.09685", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09685", "abs": "https://arxiv.org/abs/2512.09685", "authors": ["Zeyu Zhang", "Haiying Shen"], "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs", "comment": null, "summary": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.", "AI": {"tldr": "STAR\u7cfb\u7edf\u901a\u8fc7\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\u548c\u8d44\u6e90\u91cd\u5206\u914d\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684straggler\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5c3d\u7ba1GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u5f88\u6d41\u884c\uff0c\u4f46straggler\u95ee\u9898\u7684\u666e\u904d\u6027\u3001\u539f\u56e0\u548c\u5f71\u54cd\u4ee5\u53ca\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u7814\u7a76\u53d1\u73b0straggler\u5e7f\u6cdb\u5b58\u5728\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4eceSSGD\u5207\u6362\u5230ASGD\uff09\u53ef\u80fd\u65e0\u6cd5\u6539\u5584\u8bad\u7ec3\u65f6\u95f4\u751a\u81f3\u4ea7\u751f\u66f4\u591astraggler\u3002", "method": "\u63d0\u51faSTAR\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\uff0c\u5c06\u5de5\u4f5c\u8005\u5206\u7ec4\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\uff1b2\uff09\u542f\u53d1\u5f0f\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u540c\u6b65\u6a21\u5f0f\u4ee5\u6700\u5c0f\u5316\u8bad\u7ec3\u65f6\u95f4\uff1b3\uff09\u8d44\u6e90\u91cd\u5206\u914d\u652f\u6301\u6240\u9009\u6a21\u5f0f\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u5171\u5b58\u4f5c\u4e1a\u7684\u5f71\u54cd\uff1b4\uff09\u901a\u8fc7\u907f\u514dCPU\u548c\u5e26\u5bbd\u8fc7\u8f7d\u6765\u4e3b\u52a8\u9884\u9632straggler\u3002", "result": "\u5728AWS\u4e0a\u7684trace\u9a71\u52a8\u8bc4\u4f30\u663e\u793a\uff0cSTAR\u5728PS\u67b6\u6784\u4e2d\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u964d\u4f4e48-84%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u5728all-reduce\u67b6\u6784\u4e2d\u964d\u4f4e51-70%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86SSGD\u7684\u6536\u655b\u7cbe\u5ea6\u3002", "conclusion": "STAR\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684straggler\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\u548c\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2512.09340", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09340", "abs": "https://arxiv.org/abs/2512.09340", "authors": ["Chethana Prasad Kabgere"], "title": "Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration", "comment": "12 pages, 3 figures. Research manuscript based on the final project for CS6795 (Introduction to Cognitive Science), Georgia Tech", "summary": "Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.", "AI": {"tldr": "\u5bf9\u6bd4\u4eba\u7c7b\u4e0eAI\u7cfb\u7edf\u5728\u6a21\u7cca\u89c6\u89c9\u523a\u6fc0\u4e0b\u7684\u56fe\u50cf\u6807\u6ce8\u8868\u73b0\uff0c\u5206\u6790\u4e24\u8005\u5728\u8868\u5f81\u3001\u63a8\u7406\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u9762\u7684\u5f02\u540c\uff0c\u4e3a\u672a\u6765\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u63d0\u4f9b\u8ba4\u77e5\u57fa\u7840", "motivation": "\u7814\u7a76\u4eba\u7c7b\u548cAI\u7cfb\u7edf\u5982\u4f55\u89e3\u91ca\u6a21\u7cca\u89c6\u89c9\u523a\u6fc0\uff0c\u8fd9\u80fd\u6df1\u5165\u7406\u89e3\u611f\u77e5\u3001\u63a8\u7406\u548c\u51b3\u7b56\u7684\u672c\u8d28\u3002\u901a\u8fc7\u5bf9\u6bd4\u4e24\u8005\u7684\u5904\u7406\u7b56\u7565\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u3001\u8ba4\u77e5\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u57fa\u7840", "method": "\u7ed3\u5408\u8ba1\u7b97\u8ba4\u77e5\u79d1\u5b66\u3001\u8ba4\u77e5\u67b6\u6784\u548c\u8fde\u63a5\u4e3b\u4e49-\u7b26\u53f7\u6df7\u5408\u6a21\u578b\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u7b56\u7565\uff08\u7c7b\u6bd4\u63a8\u7406\u3001\u5f62\u72b6\u8bc6\u522b\u3001\u7f6e\u4fe1\u5ea6\u8c03\u8282\uff09\u4e0eAI\u7684\u7279\u5f81\u5904\u7406\u3002\u57fa\u4e8eMarr\u7684\u4e09\u5c42\u6b21\u5047\u8bbe\u3001Simon\u7684\u6709\u9650\u7406\u6027\u548cThagard\u7684\u8868\u5f81\u60c5\u611f\u6846\u67b6\uff0c\u5206\u6790\u53c2\u4e0e\u8005\u53cd\u5e94\u4e0eGrad-CAM\u53ef\u89c6\u5316\u6a21\u578b\u6ce8\u610f\u529b\u3002\u4f7f\u7528ACT-R\u548cSoar\u8ba4\u77e5\u67b6\u6784\u89e3\u91ca\u4eba\u7c7b\u884c\u4e3a", "result": "\u63ed\u793a\u4e86\u751f\u7269\u4e0e\u4eba\u5de5\u7cfb\u7edf\u5728\u8868\u5f81\u3001\u63a8\u7406\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u9762\u7684\u5173\u952e\u76f8\u4f3c\u70b9\u548c\u5dee\u5f02\u3002\u4eba\u7c7b\u8868\u73b0\u51fa\u5206\u5c42\u548c\u542f\u53d1\u5f0f\u51b3\u7b56\u7b56\u7565\uff0c\u800cAI\u5219\u57fa\u4e8e\u7279\u5f81\u5904\u7406", "conclusion": "\u5206\u6790\u4e3a\u672a\u6765\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u63d0\u4f9b\u4e86\u52a8\u673a\uff0c\u8fd9\u79cd\u67b6\u6784\u5c06\u7ed3\u6784\u5316\u7b26\u53f7\u63a8\u7406\u4e0e\u8fde\u63a5\u4e3b\u4e49\u8868\u5f81\u76f8\u7edf\u4e00\uff0c\u7ed3\u5408\u5177\u8eab\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba4\u77e5\u5bf9\u9f50\u539f\u5219\uff0c\u6709\u671b\u6784\u5efa\u65e2\u9ad8\u6027\u80fd\u53c8\u53ef\u89e3\u91ca\u4e14\u5177\u6709\u8ba4\u77e5\u57fa\u7840\u7684AI\u7cfb\u7edf"}}
{"id": "2512.09710", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09710", "abs": "https://arxiv.org/abs/2512.09710", "authors": ["Hagit Attiya", "Panagiota Fatourou", "Eleftherios Kosmas", "Yuanhao Wei"], "title": "Recoverable Lock-Free Locks", "comment": null, "summary": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u540c\u65f6\u5b9e\u73b0\u65e0\u9501\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u8f6c\u6362\u4e3a\u53ef\u6062\u590d\u7684\u65e0\u9501\u5b9e\u73b0", "motivation": "\u73b0\u6709\u7cfb\u7edf\u9700\u8981\u5728\u9501\u673a\u5236\u548c\u65e0\u9501\u673a\u5236\u4e4b\u95f4\u505a\u51fa\u6743\u8861\uff0c\u7f3a\u4e4f\u540c\u65f6\u5177\u5907\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u65e0\u9501\u7b97\u6cd5\u901a\u5e38\u96be\u4ee5\u5b9e\u73b0\u53ef\u6062\u590d\u6027\uff0c\u800c\u57fa\u4e8e\u9501\u7684\u7cfb\u7edf\u5728\u6545\u969c\u6062\u590d\u65b9\u9762\u5b58\u5728\u9650\u5236\u3002", "method": "\u4ece\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u51fa\u53d1\uff0c\u4e3a\u9501\u83b7\u53d6\u548c\u9501\u91ca\u653e\u64cd\u4f5c\u63d0\u4f9b\u53ef\u6062\u590d\u3001\u65e0\u9501\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u8f6c\u6362\u652f\u6301\u5d4c\u5957\u9501\u4ee5\u786e\u4fdd\u901a\u7528\u6027\uff0c\u5e76\u5728\u4e0d\u635f\u5bb3\u539f\u59cb\u9501\u5b9e\u73b0\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\u786e\u4fdd\u53ef\u6062\u590d\u6027\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u540c\u65f6\u5177\u5907\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\uff0c\u652f\u6301\u5d4c\u5957\u9501\uff0c\u4fdd\u6301\u4e86\u539f\u59cb\u9501\u5b9e\u73b0\u7684\u6b63\u786e\u6027\uff0c\u4e3a\u5e76\u53d1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6545\u969c\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u8be5\u8f6c\u6362\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u9501\u7b97\u6cd5\u4e0e\u53ef\u6062\u590d\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4e3a\u5e76\u53d1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6b63\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2512.09566", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09566", "abs": "https://arxiv.org/abs/2512.09566", "authors": ["Junkai Ji", "Zhangfan Yang", "Dong Xu", "Ruibin Bai", "Jianqiang Li", "Tingjun Hou", "Zexuan Zhu"], "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search", "comment": "21 pages, 5 figures", "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.", "AI": {"tldr": "Trio\u662f\u4e00\u4e2a\u6574\u5408\u7247\u6bb5\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u5206\u5b50\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u95ed\u73af\u9776\u5411\u5206\u5b50\u8bbe\u8ba1\uff0c\u5728\u7ed3\u5408\u4eb2\u548c\u529b\u3001\u7c7b\u836f\u6027\u548c\u5408\u6210\u53ef\u884c\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u836f\u7269\u53d1\u73b0\u65b9\u6cd5\u8017\u65f6\u6602\u8d35\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3001\u8fc7\u5ea6\u5f3a\u8c03\u7ed3\u5408\u4eb2\u548c\u529b\u800c\u5ffd\u89c6\u5173\u952e\u836f\u7406\u5b66\u6027\u8d28\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u8f6c\u5316\u5e94\u7528\u4ef7\u503c\u3002", "method": "Trio\u6846\u67b6\u6574\u5408\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u7247\u6bb5\u7684\u5206\u5b50\u8bed\u8a00\u5efa\u6a21\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7247\u6bb5\u7ec4\u88c5\uff1b\u5f3a\u5316\u5b66\u4e60\u786e\u4fdd\u7269\u7406\u5316\u5b66\u548c\u5408\u6210\u53ef\u884c\u6027\uff1b\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u5e73\u8861\u65b0\u9896\u5316\u5b66\u578b\u63a2\u7d22\u548c\u7ed3\u5408\u53e3\u888b\u4e2d\u6709\u524d\u666f\u4e2d\u95f4\u4f53\u7684\u5229\u7528\u3002", "result": "Trio\u53ef\u9760\u5730\u751f\u6210\u5316\u5b66\u6709\u6548\u4e14\u836f\u7406\u5b66\u589e\u5f3a\u7684\u914d\u4f53\uff0c\u5728\u7ed3\u5408\u4eb2\u548c\u529b\uff08+7.85%\uff09\u3001\u7c7b\u836f\u6027\uff08+11.10%\uff09\u548c\u5408\u6210\u53ef\u884c\u6027\uff08+12.05%\uff09\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06\u5206\u5b50\u591a\u6837\u6027\u6269\u5c55\u56db\u500d\u4ee5\u4e0a\u3002", "conclusion": "Trio\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u95ed\u73af\u9776\u5411\u5206\u5b50\u8bbe\u8ba1\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.09727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09727", "abs": "https://arxiv.org/abs/2512.09727", "authors": ["Junlin Xiao", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions", "comment": null, "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6765\u805a\u5408\u591a\u7ebf\u7a0bMCTS\u7edf\u8ba1\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u57286\u4e2a\u4e0d\u540c\u9886\u57df\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u805a\u5408\u7b56\u7565\uff0c\u4ec5\u9700\u9002\u5ea6\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u5f53\u8ba1\u7b97\u65f6\u95f4\u6709\u9650\u4f46\u9700\u8981\u6700\u4f73\u6027\u80fd\u65f6\uff0c\u5982\u4f55\u6709\u6548\u805a\u5408\u4e0d\u540c\u7ebf\u7a0b\u7684\u7edf\u8ba1\u4fe1\u606f\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u6839\u5e76\u884cMCTS\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u7edf\u8ba1\u805a\u5408\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6765\u83b7\u53d6\u672a\u5728\u73af\u5883\u4e2d\u8bd5\u9a8c\u8fc7\u7684\u6709\u5e0c\u671b\u52a8\u4f5c\u7684\u4ef7\u503c\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u56de\u5f52\u6a21\u578b\u6765\u63a8\u65ad\u672a\u91c7\u6837\u52a8\u4f5c\u7684\u4ef7\u503c\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u805a\u5408\u591a\u7ebf\u7a0b\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u57286\u4e2a\u4e0d\u540c\u9886\u57df\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u805a\u5408\u7b56\u7565\uff0c\u540c\u65f6\u4ec5\u9700\u8981\u9002\u5ea6\u7684\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684\u7edf\u8ba1\u805a\u5408\u65b9\u6cd5\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u6839\u5e76\u884cMCTS\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5728\u7ebf\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09829", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09829", "abs": "https://arxiv.org/abs/2512.09829", "authors": ["Khurram Khalil", "Muhammad Mahad Khaliq", "Khaza Anuarul Hoque"], "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning", "comment": "Accepted in the IEEE DATE 2026 conference", "summary": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.", "AI": {"tldr": "RIFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u6545\u969c\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6700\u574f\u60c5\u51b5\u6545\u969c\u641c\u7d22\u8f6c\u5316\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901fAI\u52a0\u901f\u5668\u7684\u6545\u969c\u8bc4\u4f30\uff0c\u51cf\u5c11\u6d4b\u8bd5\u5411\u91cf\u9700\u6c4299%\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u786c\u4ef6\u4fdd\u62a4\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3AI\u52a0\u901f\u5668\u89c4\u6a21\u5de8\u5927\uff0c\u4f20\u7edf\u6545\u969c\u8bc4\u4f30\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u548c\u5173\u952e\u6545\u969c\u6a21\u5f0f\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6545\u969c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "RIFT\u5c06\u590d\u6742\u7684\u6700\u574f\u60c5\u51b5\u6545\u969c\u641c\u7d22\u8f6c\u5316\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408\u6df7\u5408\u7075\u654f\u5ea6\u5206\u6790\u8fdb\u884c\u641c\u7d22\u7a7a\u95f4\u526a\u679d\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u751f\u6210\u6700\u5c0f\u5316\u3001\u9ad8\u5f71\u54cd\u529b\u7684\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u5728\u57fa\u4e8eNVIDIA A100 GPU\u7684\u5341\u4ebf\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cRIFT\u76f8\u6bd4\u8fdb\u5316\u65b9\u6cd5\u5b9e\u73b02.2\u500d\u6545\u969c\u8bc4\u4f30\u52a0\u901f\uff0c\u76f8\u6bd4\u968f\u673a\u6545\u969c\u6ce8\u5165\u51cf\u5c1199%\u4ee5\u4e0a\u6d4b\u8bd5\u5411\u91cf\u9700\u6c42\uff0c\u540c\u65f6\u83b7\u5f97\u66f4\u4f18\u7684\u6545\u969c\u8986\u76d6\u7387\u3002RIFT\u6307\u5bfc\u7684\u9009\u62e9\u6027\u9519\u8bef\u6821\u6b63\u7801\u76f8\u6bd4\u7edf\u4e00\u4e09\u91cd\u6a21\u5757\u5197\u4f59\u4fdd\u62a4\uff0c\u6210\u672c\u6548\u76ca\uff08\u5355\u4f4d\u9762\u79ef\u8986\u76d6\u7387\uff09\u63d0\u534712.8\u500d\u3002", "conclusion": "RIFT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u53d1\u73b0\u6700\u5c0f\u5316\u3001\u9ad8\u5f71\u54cd\u529b\u7684\u6545\u969c\u573a\u666f\uff0c\u663e\u8457\u63d0\u9ad8AI\u52a0\u901f\u5668\u6545\u969c\u8bc4\u4f30\u6548\u7387\uff0c\u540c\u65f6\u751f\u6210\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u5546\u4e1aRTL\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u7a0b\u7684UVM\u517c\u5bb9\u9a8c\u8bc1\u5de5\u4ef6\u3002"}}
{"id": "2512.09895", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.09895", "abs": "https://arxiv.org/abs/2512.09895", "authors": ["Jane Greenberg", "Scott McClellan", "Addy Ireland", "Robert Sammarco", "Colton Gerber", "Christopher B. Rauch", "Mat Kelly", "John Kunze", "Yuan An", "Eric Toberer"], "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science", "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures", "summary": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.", "AI": {"tldr": "MatSci-YAMZ\u5e73\u53f0\u7ed3\u5408AI\u548c\u4eba\u5de5\u53c2\u4e0e\uff08\u5305\u62ec\u4f17\u5305\uff09\u6765\u652f\u6301\u5143\u6570\u636e\u8bcd\u6c47\u8868\u5f00\u53d1\uff0c\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86AI-\u4eba\u5de5\u534f\u540c\u6a21\u578b\u5728\u589e\u5f3a\u8bed\u4e49\u900f\u660e\u5ea6\u548c\u51cf\u5c11\u5171\u8bc6\u6784\u5efa\u65f6\u95f4\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5143\u6570\u636e\u8bcd\u6c47\u8868\u5bf9\u63a8\u8fdbFAIR\u548cFARR\u6570\u636e\u539f\u5219\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u53d1\u5c55\u53d7\u5230\u4eba\u529b\u8d44\u6e90\u6709\u9650\u548c\u6807\u51c6\u5316\u5b9e\u8df5\u4e0d\u4e00\u81f4\u7684\u5236\u7ea6\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u652f\u6301\u5143\u6570\u636e\u8bcd\u6c47\u8868\u5f00\u53d1\u3002", "method": "\u5f00\u53d1\u4e86MatSci-YAMZ\u5e73\u53f0\uff0c\u6574\u5408\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u5de5\u53c2\u4e0e\uff08\u5305\u62ec\u4f17\u5305\uff09\u6765\u652f\u6301\u5143\u6570\u636e\u8bcd\u6c47\u8868\u5f00\u53d1\u3002\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u8fdb\u884c\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c6\u540d\u53c2\u4e0e\u8005\u901a\u8fc7\u5e73\u53f0\u63d0\u4f9b\u672f\u8bed\u5b9a\u4e49\u548c\u793a\u4f8b\uff0c\u5f15\u5bfcAI\u5b9a\u4e49\u7cbe\u70bc\uff0c\u5f62\u6210\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u3002", "result": "\u6210\u529f\u751f\u6210\u4e8619\u4e2aAI\u751f\u6210\u7684\u5b9a\u4e49\uff0c\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u8bc1\u660e\u4e86AI-\u4eba\u5de5\u534f\u540c\u7cbe\u70bc\u7684\u53ef\u884c\u6027\u3002\u7814\u7a76\u786e\u8ba4\u4e86AI-\u4eba\u5de5\u534f\u540c\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5305\u62ec\uff1a1\uff09\u6210\u529f\u7684\u6982\u5ff5\u9a8c\u8bc1\uff1b2\uff09\u4e0eFAIR\u548c\u5f00\u653e\u79d1\u5b66\u539f\u5219\u7684\u4e00\u81f4\u6027\uff1b3\uff09\u6307\u5bfc\u672a\u6765\u7814\u7a76\u7684\u7814\u7a76\u534f\u8bae\uff1b4\uff09\u8de8\u9886\u57df\u6269\u5c55\u7684\u6f5c\u529b\u3002", "conclusion": "MatSci-YAMZ\u7684\u57fa\u7840\u6a21\u578b\u6709\u80fd\u529b\u589e\u5f3a\u8bed\u4e49\u900f\u660e\u5ea6\uff0c\u51cf\u5c11\u5171\u8bc6\u6784\u5efa\u548c\u5143\u6570\u636e\u8bcd\u6c47\u8868\u5f00\u53d1\u6240\u9700\u7684\u65f6\u95f4\uff0c\u4e3a\u8de8\u9886\u57df\u6269\u5c55\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2512.09897", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09897", "abs": "https://arxiv.org/abs/2512.09897", "authors": ["Haoye Lu", "Pavan Seshadri", "Kaheer Suleman"], "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments", "comment": null, "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.", "AI": {"tldr": "SCOPE\u662f\u4e00\u79cd\u4e00\u6b21\u6027\u5206\u5c42\u89c4\u5212\u5668\uff0c\u5229\u7528LLM\u751f\u6210\u7684\u5b50\u76ee\u6807\u4ec5\u521d\u59cb\u5316\u65f6\u9884\u8bad\u7ec3\u8f7b\u91cf\u5b66\u751f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4f46\u727a\u7272\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u90e8\u7f72\u6548\u7387\u4f4e\u3001LLM\u53c2\u6570\u56fa\u5b9a\u65e0\u6cd5\u9002\u5e94\u76ee\u6807\u4efb\u52a1\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSCOPE\u65b9\u6cd5\uff1a1\uff09\u4ec5\u5728\u521d\u59cb\u5316\u65f6\u4f7f\u7528LLM\u751f\u6210\u5b50\u76ee\u6807\uff1b2\uff09\u4ece\u793a\u4f8b\u8f68\u8ff9\u76f4\u63a5\u63a8\u5bfc\u5b50\u76ee\u6807\uff1b3\uff09\u9884\u8bad\u7ec3\u8f7b\u91cf\u5b66\u751f\u6a21\u578b\uff1b4\uff09\u907f\u514d\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u91cd\u590d\u67e5\u8be2LLM\u3002", "result": "\u5728TextCraft\u73af\u5883\u4e2d\uff0cSCOPE\u8fbe\u52300.56\u6210\u529f\u7387\uff0c\u4f18\u4e8eADaPT\u76840.52\uff1b\u63a8\u7406\u65f6\u95f4\u4ece164.4\u79d2\u5927\u5e45\u51cf\u5c11\u52303.0\u79d2\uff0c\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5b50\u76ee\u6807\u5373\u4f7f\u975e\u6700\u4f18\uff0c\u4ecd\u80fd\u4e3a\u57fa\u4e8e\u6587\u672c\u7684\u5206\u5c42\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u826f\u597d\u7684\u8d77\u70b9\uff0cSCOPE\u5728\u6548\u7387\u4e0e\u6027\u80fd\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
