<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 40]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-α Optimization](https://arxiv.org/abs/2511.09563)
*Qilong Yuan*

Main category: cs.AI

TL;DR: 本文提出了一种新的高效方法来解决大规模联合路由分配（JRA）优化问题，通过部分路径重构（PPR）求解器和全局大α约束，在保持高计算效率的同时实现近乎最优解。


<details>
  <summary>Details</summary>
Motivation: 现有精确方法虽然能保证最优性，但在大规模实例中计算效率低下；启发式方法虽然高效但偏离最优解约1%。需要开发既能保持高精度又具有计算效率的新方法。

Method: 提出部分路径重构（PPR）求解器，首先识别关键物品-占位符对形成简化子问题，然后高效求解以改进全局解。结合PJAR框架和全局大α约束，通过迭代优化路径获得高精度解。

Result: 在n=300、500和1000的基准数据集上，该方法平均偏差为0.00%，计算效率高，将初始启发式解的偏差减少了一半。

Conclusion: 所提出的方法能够为大规模JRA问题提供近乎最优的高精度解，且该框架具有扩展到TSP及相关优化问题的潜力。

Abstract: The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-α constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.

</details>


### [2] [Variable Neighborhood Search for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2511.09570)
*David Woller,Viktor Kozák,Miroslav Kulich,Libor Přeučil*

Main category: cs.AI

TL;DR: 本文介绍了在2020年IEEE世界计算智能大会上CEC-12竞赛中获胜的方法，该方法基于变邻域搜索元启发式算法，解决了电动汽车路径规划问题的最小化变体CGVRP，并在完整竞赛数据集上取得了最佳结果。


<details>
  <summary>Details</summary>
Motivation: 由于电动汽车在物流中的日益普及，电动汽车路径规划问题扩展了经典车辆路径问题。文献中考虑的各种约束使得比较不同问题变体的方法具有挑战性。

Method: 采用变邻域搜索元启发式算法来解决容量约束绿色车辆路径问题这一最小化变体。

Result: 该方法在完整竞赛数据集上取得了最佳结果，并且优于后续发布的更近期算法。

Conclusion: 基于变邻域搜索的方法在电动汽车路径规划问题的最小化变体上表现出色，证明了其有效性。

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the classical Vehicle Routing Problem (VRP) to reflect the growing use of electric and hybrid vehicles in logistics. Due to the variety of constraints considered in the literature, comparing approaches across different problem variants remains challenging. A minimalistic variant of the EVRP, known as the Capacitated Green Vehicle Routing Problem (CGVRP), was the focus of the CEC-12 competition held during the 2020 IEEE World Congress on Computational Intelligence. This paper presents the competition-winning approach, based on the Variable Neighborhood Search (VNS) metaheuristic. The method achieves the best results on the full competition dataset and also outperforms a more recent algorithm published afterward.

</details>


### [3] [Proceedings of the Second International Workshop on Next-Generation Language Models for Knowledge Representation and Reasoning (NeLaMKRR 2025)](https://arxiv.org/abs/2511.09575)
*Ha-Thanh Nguyen,Ken Satoh,Francesca Toni,Randy Goebel,Kostas Stathis*

Main category: cs.AI

TL;DR: 该研讨会旨在探索如何调和基于transformer的语言模型与基于逻辑的知识表示之间的推理能力，分析语言模型的推理能力，将逻辑推理能力注入语言模型，并形式化语言模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自然语言处理领域取得了巨大进展，但它们是否真正具备推理能力仍不明确。需要建立跨学科平台来探索语言模型与逻辑推理的融合，以提升在精确性和可靠性要求高的应用中的实用性。

Method: 通过跨学科合作，分析语言模型的推理能力与知识表示方法的对比，采用神经符号方法将逻辑推理能力注入语言模型，并形式化语言模型的推理过程。

Result: 研讨会旨在建立研究平台，促进不同学科背景的研究者共同探索语言模型推理能力的本质和提升方法。

Conclusion: 通过调和语言模型与逻辑推理，可以更好地理解语言模型的推理能力，并提升其在需要精确性和可靠性的应用中的实用性。

Abstract: Reasoning is an essential component of human intelligence in that it plays a fundamental role in our ability to think critically, support responsible decisions, and solve challenging problems. Traditionally, AI has addressed reasoning in the context of logic-based representations of knowledge. However, the recent leap forward in natural language processing, with the emergence of language models based on transformers, is hinting at the possibility that these models exhibit reasoning abilities, particularly as they grow in size and are trained on more and more data. Still, despite ongoing discussions about what reasoning is in language models, it is still not easy to articulate to what extent these models are actually capable of reasoning.
  The goal of this workshop is to create a platform for researchers from different disciplines and/or AI perspectives to explore approaches and techniques with the aim to reconcile reasoning between language models using transformers and logic-based representations. The specific objectives include analysing the reasoning abilities of language models measured alongside KR methods, injecting KR-style reasoning abilities into language models (including by neuro-symbolic means), and formalising the kind of reasoning language models carry out. This exploration aims to uncover how language models can effectively integrate and leverage knowledge and reasoning with it, thus improving their application and utility in areas where precision and reliability are key requirements.

</details>


### [4] [Cogent argument extensions are weakly admissible but not vice versa](https://arxiv.org/abs/2511.09600)
*Gustavo Bodanza*

Main category: cs.AI

TL;DR: 本文研究了非可容许论证框架语义学中cogent语义和弱可容许语义之间的关系，证明了cogent扩展是弱可容许的，但反之不成立。


<details>
  <summary>Details</summary>
Motivation: 探索两种非可容许论证框架语义学（cogent语义和弱可容许语义）之间的理论关系，澄清它们之间的包含关系。

Method: 通过理论证明的方法，分析cogent扩展和弱可容许扩展的定义特性，建立它们之间的逻辑关系。

Result: 证明了cogent扩展一定是弱可容许扩展，但存在弱可容许扩展不是cogent扩展的情况。

Conclusion: cogent语义是弱可容许语义的真子集，两者之间存在严格的包含关系，这为理解非可容许论证语义提供了重要的理论洞见。

Abstract: In this research note, we show the relationship between two non-admissible argumentation framework semantics: cogent and weakly admissible semantics. We prove that, while cogent extensions are weakly admissible, the converse is not true.

</details>


### [5] [Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models](https://arxiv.org/abs/2511.09682)
*Tiansheng Huang,Virat Shejwalkar,Oscar Chang,Milad Nasr,Ling Liu*

Main category: cs.AI

TL;DR: 本文研究了音频推理模型（ARMs）的安全性，发现标准推理训练可以防御普通音频越狱攻击，但无法防御更高级的攻击。作者提出了Rebellion方法，通过训练模型抵抗最坏情况的表示漂移来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着音频推理模型的普及，目前尚无研究关注其对抗越狱攻击的安全性。作者旨在填补这一空白，探索如何保护ARMs免受音频越狱攻击。

Method: 作者首先展示了标准推理训练的局限性，然后提出了Rebellion方法——一种鲁棒的推理训练方法，训练ARMs抵抗最坏情况的表示漂移。

Result: 实验结果表明，Rebellion能够有效防御高级音频越狱攻击，同时不影响良性任务的性能，并且在准确性与安全性权衡方面显著优于标准推理训练方法。

Conclusion: Rebellion方法为音频推理模型提供了一种有效的安全防护机制，能够在保持性能的同时显著提升模型对抗越狱攻击的能力。

Abstract: Instilling reasoning capabilities in large models (LMs) using reasoning training (RT) significantly improves LMs' performances. Thus Audio Reasoning Models (ARMs), i.e., audio LMs that can reason, are becoming increasingly popular. However, no work has studied the safety of ARMs against jailbreak attacks that aim to elicit harmful responses from target models. To this end, first, we show that standard RT with appropriate safety reasoning data can protect ARMs from vanilla audio jailbreaks, but cannot protect them against our proposed simple yet effective jailbreaks. We show that this is because of the significant representation drift between vanilla and advanced jailbreaks which forces the target ARMs to emit harmful responses. Based on this observation, we propose Rebellion, a robust RT that trains ARMs to be robust to the worst-case representation drift. All our results are on Qwen2-Audio; they demonstrate that Rebellion: 1) can protect against advanced audio jailbreaks without compromising performance on benign tasks, and 2) significantly improves accuracy-safety trade-off over standard RT method.

</details>


### [6] [Echoing: Identity Failures when LLM Agents Talk to Each Other](https://arxiv.org/abs/2511.09710)
*Sarath Shekkizhar,Romain Cosentino,Adam Earle,Silvio Savarese*

Main category: cs.AI

TL;DR: 研究发现LLM智能体在自主互动时会出现行为漂移现象，特别是回响效应，即智能体放弃自身角色而模仿对话伙伴，这在不同LLM提供商和领域中都普遍存在。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体自主交互的增加，出现了无法从单个智能体性能预测的新型失败模式——智能体间对话中的行为漂移，这需要专门研究。

Method: 通过60种AxA配置、3个领域和2000多次对话的实验，分析不同LLM模型中的回响现象，并测试结构化响应的缓解效果。

Result: 回响现象在三大LLM提供商中普遍存在，发生率从5%到70%不等，且随着对话轮数增加而加剧（7轮以上），结构化响应可将回响率降至9%。

Conclusion: 智能体间交互存在独特的行为漂移风险，回响效应是系统性而非提示工程问题，需要协议级别的解决方案来确保交互稳定性。

Abstract: As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\%$ to $70\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\%$.

</details>


### [7] [Massively Parallel Proof-Number Search for Impartial Games and Beyond](https://arxiv.org/abs/2511.10339)
*Tomáš Čížek,Martin Balko,Martin Schmid*

Main category: cs.AI

TL;DR: 本文提出了首个可在大规模CPU集群上高效扩展的并行Proof-Number Search算法，应用于Sprouts游戏求解，在1024核上实现332.9倍加速，性能提升4个数量级，并验证了Sprouts猜想的新位置。


<details>
  <summary>Details</summary>
Motivation: Proof-Number Search算法在游戏求解中应用广泛，但随着大规模计算集群的普及，现有并行版本在多核CPU上扩展性差，需要开发能高效利用大量CPU的并行算法。

Method: 采用两级并行化和工作者间信息共享机制，开发大规模并行Proof-Number Search版本，并结合Grundy数来缩减游戏树，应用于Sprouts游戏求解。

Result: 在1024核上实现332.9倍加速，性能比当前最优Sprouts求解器GLOP快4个数量级，能生成复杂度高1000倍的证明，验证了Sprouts猜想的42个新位置，使已知结果数量几乎翻倍。

Conclusion: 成功开发了首个可在大规模CPU集群上高效扩展的并行Proof-Number Search算法，显著提升了游戏求解能力，为复杂游戏树的分析提供了有效工具。

Abstract: Proof-Number Search is a best-first search algorithm with many successful applications, especially in game solving. As large-scale computing clusters become increasingly accessible, parallelization is a natural way to accelerate computation. However, existing parallel versions of Proof-Number Search are known to scale poorly on many CPU cores. Using two parallelized levels and shared information among workers, we present the first massively parallel version of Proof-Number Search that scales efficiently even on a large number of CPUs. We apply our solver, enhanced with Grundy numbers for reducing game trees, to the Sprouts game, a case study motivated by the long-standing Sprouts Conjecture. Our solver achieves a significantly improved 332.9$\times$ speedup when run on 1024 cores, enabling it to outperform the state-of-the-art Sprouts solver GLOP by four orders of magnitude in runtime and to generate proofs 1,000$\times$ more complex. Despite exponential growth in game tree size, our solver verified the Sprouts Conjecture for 42 new positions, nearly doubling the number of known outcomes.

</details>


### [8] [AI Annotation Orchestration: Evaluating LLM verifiers to Improve the Quality of LLM Annotations in Learning Analytics](https://arxiv.org/abs/2511.09785)
*Bakhtawar Ahtisham,Kirk Vanacore,Jinsook Lee,Zhuqian Zhou,Doug Pietrzak,Rene F. Kizilcec*

Main category: cs.AI

TL;DR: 该论文研究通过验证导向的编排方法（自我验证和交叉验证）来提高大语言模型在辅导话语定性编码中的可靠性，实验表明验证编排能显著提升与人类标注的一致性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地用于标注学习互动，但其可靠性问题限制了实用性。研究者希望测试验证导向的编排是否能改善辅导话语的定性编码质量。

Method: 使用30个一对一数学辅导会话的转录文本，比较三种前沿大语言模型（GPT、Claude、Gemini）在三种条件下的表现：无验证标注、自我验证和交叉验证。输出结果与盲审的人类裁决进行对比，使用Cohen's kappa衡量一致性。

Result: 验证编排使kappa值提高了58%。自我验证相比无验证基线几乎使一致性翻倍，在具有挑战性的辅导行为上提升最大。交叉验证平均提升37%，但存在配对和结构依赖效应：某些验证器-标注器组合优于自我验证，而其他组合降低一致性，反映了验证器严格度的差异。

Conclusion: 验证是构建可靠、可扩展的LLM辅助标注系统的原则性设计杠杆，为学习分析领域提供了实用的编排框架和标准化报告方法。

Abstract: Large Language Models (LLMs) are increasingly used to annotate learning interactions, yet concerns about reliability limit their utility. We test whether verification-oriented orchestration-prompting models to check their own labels (self-verification) or audit one another (cross-verification)-improves qualitative coding of tutoring discourse. Using transcripts from 30 one-to-one math sessions, we compare three production LLMs (GPT, Claude, Gemini) under three conditions: unverified annotation, self-verification, and cross-verification across all orchestration configurations. Outputs are benchmarked against a blinded, disagreement-focused human adjudication using Cohen's kappa. Overall, orchestration yields a 58 percent improvement in kappa. Self-verification nearly doubles agreement relative to unverified baselines, with the largest gains for challenging tutor moves. Cross-verification achieves a 37 percent improvement on average, with pair- and construct-dependent effects: some verifier-annotator pairs exceed self-verification, while others reduce alignment, reflecting differences in verifier strictness. We contribute: (1) a flexible orchestration framework instantiating control, self-, and cross-verification; (2) an empirical comparison across frontier LLMs on authentic tutoring data with blinded human "gold" labels; and (3) a concise notation, verifier(annotator) (e.g., Gemini(GPT) or Claude(Claude)), to standardize reporting and make directional effects explicit for replication. Results position verification as a principled design lever for reliable, scalable LLM-assisted annotation in Learning Analytics.

</details>


### [9] [Why Open Small AI Models Matter for Interactive Art](https://arxiv.org/abs/2511.09788)
*Mar Canet Sola,Varvara Guljajeva*

Main category: cs.AI

TL;DR: 本文主张开放小型AI模型对互动艺术创作独立性的重要性，它们可在本地部署，为艺术家提供对基础设施和代码的控制权，与主流封闭式大型企业系统形成对比。


<details>
  <summary>Details</summary>
Motivation: 当前主导的大型封闭式企业AI系统对互动艺术作品施加严重限制，包括内容过滤限制、保存问题、延迟增加和接口有限等技术挑战，需要为艺术家提供更多自主权和控制力。

Method: 通过使用可本地部署的开放小型AI模型，艺术家可以长期使用模型、创建自定义模型、修改代码集成新接口，或通过重新训练和微调使用新数据集。

Result: 开放小型AI模型赋予创作者更多自主权、控制力和可持续性，促进技术自决，减少对不适合互动艺术需求的企业AI的依赖。

Conclusion: 这种方法增强了艺术家的能力，支持包含AI组件的艺术作品的长期保存和展览，为互动艺术实践提供了更可持续的技术路径。

Abstract: This position paper argues for the importance of open small AI models in creative independence for interactive art practices. Deployable locally, these models offer artists vital control over infrastructure and code, unlike dominant large, closed-source corporate systems. Such centralized platforms function as opaque black boxes, imposing severe limitations on interactive artworks, including restrictive content filters, preservation issues, and technical challenges such as increased latency and limited interfaces. In contrast, small AI models empower creators with more autonomy, control, and sustainability for these artistic processes. They enable the ability to use a model as long as they want, create their own custom model, either by making code changes to integrate new interfaces, or via new datasets by re-training or fine-tuning the model. This fosters technological self-determination, offering greater ownership and reducing reliance on corporate AI ill-suited for interactive art's demands. Critically, this approach empowers the artist and supports long-term preservation and exhibition of artworks with AI components. This paper explores the practical applications and implications of using open small AI models in interactive art, contrasting them with closed-source alternatives.

</details>


### [10] [Thermally Activated Dual-Modal Adversarial Clothing against AI Surveillance Systems](https://arxiv.org/abs/2511.09829)
*Jiahuan Long,Tingsong Jiang,Hanqing Liu,Chao Ma,Wen Yao*

Main category: cs.AI

TL;DR: 提出一种热激活对抗可穿戴系统，通过热致变色染料和柔性加热单元在衣物表面产生动态对抗模式，在默认状态下呈现普通黑色T恤外观，加热后激活隐藏对抗图案以逃避可见光和红外监控检测。


<details>
  <summary>Details</summary>
Motivation: 现有对抗补丁在真实场景中部署困难，因其显眼外观难以实用。本文旨在开发一种适应性强、在复杂现实环境中有效的隐私保护方法。

Method: 集成热致变色染料与柔性加热单元，在衣物表面诱导视觉动态对抗模式。系统在默认状态下为普通黑色T恤，通过嵌入式热单元加热激活隐藏对抗图案。

Result: 物理实验显示对抗可穿戴系统在50秒内实现快速纹理激活，在多样化真实监控环境中保持80%以上的对抗成功率。

Conclusion: 这项工作展示了物理基础、用户可控反AI系统的新途径，强调了在AI监控无处不在时代，主动对抗技术对隐私保护日益重要。

Abstract: Adversarial patches have emerged as a popular privacy-preserving approach for resisting AI-driven surveillance systems. However, their conspicuous appearance makes them difficult to deploy in real-world scenarios. In this paper, we propose a thermally activated adversarial wearable designed to ensure adaptability and effectiveness in complex real-world environments. The system integrates thermochromic dyes with flexible heating units to induce visually dynamic adversarial patterns on clothing surfaces. In its default state, the clothing appears as an ordinary black T-shirt. Upon heating via an embedded thermal unit, hidden adversarial patterns on the fabric are activated, allowing the wearer to effectively evade detection across both visible and infrared modalities. Physical experiments demonstrate that the adversarial wearable achieves rapid texture activation within 50 seconds and maintains an adversarial success rate above 80\% across diverse real-world surveillance environments. This work demonstrates a new pathway toward physically grounded, user-controllable anti-AI systems, highlighting the growing importance of proactive adversarial techniques for privacy protection in the age of ubiquitous AI surveillance.

</details>


### [11] [EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services](https://arxiv.org/abs/2511.09894)
*Keshara Weerasinghe,Xueren Ge,Tessa Heick,Lahiru Nuwan Wijayasingha,Anthony Cortez,Abhishek Satpathy,John Stankovic,Homa Alemzadeh*

Main category: cs.AI

TL;DR: EgoEMS是首个端到端、高保真、多模态、多参与者的EMS数据集，包含233个模拟紧急场景的20多小时数据，用于开发AI认知助手支持急救医疗服务。


<details>
  <summary>Details</summary>
Motivation: 急救医疗服务中急救人员面临高认知负荷，AI认知助手可作为虚拟伙伴减轻负担，支持实时数据收集和决策制定。

Method: 开发了EgoEMS数据集，使用开源、低成本、可复制的数据采集系统，包含62名参与者（46名EMS专业人员）的233个模拟场景，标注了关键步骤、时间戳音频转录、动作质量指标和边界框分割掩码。

Result: 创建了首个从自我中心视角捕捉真实EMS活动的多模态数据集，强调现实性，包含反映真实紧急动态的响应者-患者互动。

Conclusion: EgoEMS数据集和基准测试套件有望推动智能EMS系统的研究，最终改善患者预后。

Abstract: Emergency Medical Services (EMS) are critical to patient survival in emergencies, but first responders often face intense cognitive demands in high-stakes situations. AI cognitive assistants, acting as virtual partners, have the potential to ease this burden by supporting real-time data collection and decision making. In pursuit of this vision, we introduce EgoEMS, the first end-to-end, high-fidelity, multimodal, multiperson dataset capturing over 20 hours of realistic, procedural EMS activities from an egocentric view in 233 simulated emergency scenarios performed by 62 participants, including 46 EMS professionals. Developed in collaboration with EMS experts and aligned with national standards, EgoEMS is captured using an open-source, low-cost, and replicable data collection system and is annotated with keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks. Emphasizing realism, the dataset includes responder-patient interactions reflecting real-world emergency dynamics. We also present a suite of benchmarks for real-time multimodal keystep recognition and action quality estimation, essential for developing AI support tools for EMS. We hope EgoEMS inspires the research community to push the boundaries of intelligent EMS systems and ultimately contribute to improved patient outcomes.

</details>


### [12] [Boosting In-Silicon Directed Evolution with Fine-Tuned Protein Language Model and Tree Search](https://arxiv.org/abs/2511.09900)
*Yaodong Yang,Yang Wang,Jinpeng Li,Pei Guo,Da Han,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.AI

TL;DR: AlphaDE是一个利用蛋白质语言模型指导蛋白质进化的新框架，通过微调预训练模型和使用蒙特卡洛树搜索进行测试时推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质定向进化算法主要关注搜索策略设计，但忽视了如何利用编码丰富进化模式的蛋白质语言模型来指导搜索过程。

Method: 1. 使用同源蛋白质序列通过掩码语言建模微调预训练蛋白质语言模型；2. 引入基于蒙特卡洛树搜索的测试时推理，利用微调后的模型指导蛋白质进化。

Result: 广泛的基准实验表明，AlphaDE即使使用少样本微调也能显著优于先前的最先进方法。案例研究显示其支持通过计算进化压缩蛋白质序列空间。

Conclusion: AlphaDE成功地将蛋白质语言模型与进化搜索相结合，为蛋白质工程提供了有效的计算进化框架。

Abstract: Protein evolution through amino acid sequence mutations is a cornerstone of life sciences. While current in-silicon directed evolution algorithms focus on designing search strategies, they overlook how to utilize the transformative protein language models, which encode rich evolutionary patterns, to guide search. To bridge this gap, we propose AlphaDE, a novel framework to evolve protein sequences by harnessing the innovative paradigms of large language models. First, AlphaDE fine-tunes pretrained protein language models using masked language modeling on homologous protein sequences to activate the evolutionary plausibility for the interested protein class. Second, AlphaDE introduces test-time inference based on Monte Carlo tree search, which effectively evolves proteins with evolutionary guidance from the fine-tuned protein language model. Extensive benchmark experiments show that AlphaDE remarkably outperforms previous state-of-the-art methods even with few-shot fine-tuning. An interesting case study further shows that AlphaDE supports condensing the protein sequence space through computational evolution.

</details>


### [13] [Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models](https://arxiv.org/abs/2511.09907)
*Yongxian Wei,Yilin Zhao,Li Shen,Xinrui Chen,Runxi Cheng,Sinan Du,Hao Yu,Gang Liu,Jiahong Yan,Chun Yuan,Dian Li*

Main category: cs.AI

TL;DR: 本文提出了一种能够推理规划问题方向并适应求解器能力的问题生成器，通过构建相关的问题对并添加推理模型生成的中间问题设计CoT，利用求解器反馈作为奖励信号来校准难度，在10个数学和通用推理基准上平均提升2.5%。


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法存在两个主要问题：(i) 不加区分地生成问题，忽略求解器能力导致低价值问题，或依赖复杂数据管道来平衡难度；(ii) 问题生成缺乏推理，导致问题变体浅显。

Method: 构建相关的问题对，并用推理模型生成的中间问题设计CoT进行增强。将求解器对合成问题的反馈作为奖励信号，使生成器能够校准难度并在求解器能力边缘产生补充问题。

Result: 在10个数学和通用推理基准上平均提升2.5%，并能泛化到语言和视觉语言模型。通过协同进化，进一步获得0.7%的性能增益。

Conclusion: 该方法通过显式推理规划和难度适应，有效提升了问题生成的质量和训练数据的价值，实现了生成器与求解器的协同进化。

Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.

</details>


### [14] [OIDA-QA: A Multimodal Benchmark for Analyzing the Opioid Industry Documents Archive](https://arxiv.org/abs/2511.09914)
*Xuan Shen,Brian Wingenroth,Zichao Wang,Jason Kuen,Wanrong Zhu,Ruiyi Zhang,Yiwei Wang,Lichun Ma,Anqi Liu,Hongfu Liu,Tong Sun,Kevin S. Hawkins,Kate Tasker,G. Caleb Alexander,Jiuxiang Gu*

Main category: cs.AI

TL;DR: 本文针对阿片类药物危机相关的复杂多模态文档，构建了一个包含40万训练文档和1万测试文档的基准数据集，开发了领域特定的多模态大语言模型，并通过历史QA对和页面引用等技术提高了文档信息提取和问答任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机揭示了监管系统、医疗实践、公司治理和公共政策等多方面的系统性缺陷，需要创新分析方法来处理UCSF-JHU阿片行业文档档案中的大量数据和文档。这些医疗相关法律和公司文档的复杂性、多模态特性和专业性要求针对特定数据类型和详细注释的先进方法。

Method: 1. 根据文档属性组织原始数据集，构建包含40万训练文档和1万测试文档的基准；2. 从每个文档中提取丰富的多模态信息（文本内容、视觉元素、布局结构）；3. 使用多个AI模型生成包含36万训练QA对和1万测试QA对的大规模数据集；4. 开发领域特定的多模态大语言模型；5. 引入历史QA对作为上下文基础，并在答案中纳入页面引用；6. 引入基于重要性的页面分类器。

Result: 初步结果表明，在文档信息提取和问答任务中，AI助手取得了改进。数据集和模型已在HuggingFace上公开可用。

Conclusion: 通过构建大规模多模态数据集和开发领域特定的多模态LLMs，结合历史上下文和页面引用技术，能够有效提升对复杂医疗法律文档的分析精度和专业性，为阿片类药物危机等公共卫生问题的系统性分析提供了创新解决方案。

Abstract: The opioid crisis represents a significant moment in public health that reveals systemic shortcomings across regulatory systems, healthcare practices, corporate governance, and public policy. Analyzing how these interconnected systems simultaneously failed to protect public health requires innovative analytic approaches for exploring the vast amounts of data and documents disclosed in the UCSF-JHU Opioid Industry Documents Archive (OIDA). The complexity, multimodal nature, and specialized characteristics of these healthcare-related legal and corporate documents necessitate more advanced methods and models tailored to specific data types and detailed annotations, ensuring the precision and professionalism in the analysis. In this paper, we tackle this challenge by organizing the original dataset according to document attributes and constructing a benchmark with 400k training documents and 10k for testing. From each document, we extract rich multimodal information-including textual content, visual elements, and layout structures-to capture a comprehensive range of features. Using multiple AI models, we then generate a large-scale dataset comprising 360k training QA pairs and 10k testing QA pairs. Building on this foundation, we develop domain-specific multimodal Large Language Models (LLMs) and explore the impact of multimodal inputs on task performance. To further enhance response accuracy, we incorporate historical QA pairs as contextual grounding for answering current queries. Additionally, we incorporate page references within the answers and introduce an importance-based page classifier, further improving the precision and relevance of the information provided. Preliminary results indicate the improvements with our AI assistant in document information extraction and question-answering tasks. The dataset and models are publicly available at: https://huggingface.co/opioidarchive

</details>


### [15] [Adaptive Hyperbolic Kernels: Modulated Embedding in de Branges-Rovnyak Spaces](https://arxiv.org/abs/2511.09921)
*Leping Si,Meimei Yang,Hui Xue,Shipeng Zhu,Pengfei Fang*

Main category: cs.AI

TL;DR: 本文提出了一种曲率感知的de Branges-Rovnyak空间，构建了自适应双曲核函数家族，在视觉和语言基准测试中优于现有双曲核方法。


<details>
  <summary>Details</summary>
Motivation: 双曲空间在嵌入层次结构方面具有优势，但现有双曲核方法存在几何失真或缺乏适应性的问题，需要改进。

Method: 引入曲率感知的de Branges-Rovnyak空间，设计可调节乘子来自适应选择对应任意曲率双曲空间的RKHS，构建包括自适应双曲径向核在内的自适应双曲核函数家族。

Result: 在视觉和语言基准测试上的广泛实验表明，所提出的核函数在建模层次依赖关系方面优于现有双曲核方法。

Conclusion: 通过曲率感知的RKHS和自适应双曲核函数，有效提升了双曲表示能力，在层次结构建模任务中取得了优越性能。

Abstract: Hierarchical data pervades diverse machine learning applications, including natural language processing, computer vision, and social network analysis. Hyperbolic space, characterized by its negative curvature, has demonstrated strong potential in such tasks due to its capacity to embed hierarchical structures with minimal distortion. Previous evidence indicates that the hyperbolic representation capacity can be further enhanced through kernel methods. However, existing hyperbolic kernels still suffer from mild geometric distortion or lack adaptability. This paper addresses these issues by introducing a curvature-aware de Branges-Rovnyak space, a reproducing kernel Hilbert space (RKHS) that is isometric to a Poincare ball. We design an adjustable multiplier to select the appropriate RKHS corresponding to the hyperbolic space with any curvature adaptively. Building on this foundation, we further construct a family of adaptive hyperbolic kernels, including the novel adaptive hyperbolic radial kernel, whose learnable parameters modulate hyperbolic features in a task-aware manner. Extensive experiments on visual and language benchmarks demonstrate that our proposed kernels outperform existing hyperbolic kernels in modeling hierarchical dependencies.

</details>


### [16] [ChEmREF: Evaluating Language Model Readiness for Chemical Emergency Response](https://arxiv.org/abs/2511.10027)
*Risha Surana,Qinyuan Ye,Swabha Swayamdipta*

Main category: cs.AI

TL;DR: 本文介绍了化学应急响应评估框架(ChEmREF)，用于评估语言模型在危险化学品事故应急响应中的表现，包括化学表示转换、应急响应生成和领域知识问答三个任务。


<details>
  <summary>Details</summary>
Motivation: 应急响应人员在处理危险化学品事故时需要快速准确地获取关键信息，但目前主要依赖手动查阅化学指南，效率较低。研究旨在探索语言模型是否能辅助应急响应人员快速理解关键信息、识别危险并提供建议。

Method: 构建ChEmREF基准，包含来自应急响应指南和PubChem数据库的1,035种危险化学品的相关问题，分为三个任务：化学表示转换、应急响应生成和领域知识问答。

Result: 最佳模型在非结构化危险化学品表示转换任务中获得68.0%的精确匹配率，在事故响应建议任务中获得52.7%的LLM评估分数，在危险化学品考试中获得63.9%的多项选择准确率。

Conclusion: 语言模型在辅助应急响应人员处理危险化学品事故方面显示出潜力，但由于当前局限性，需要谨慎的人工监督。

Abstract: Emergency responders managing hazardous material HAZMAT incidents face critical, time-sensitive decisions, manually navigating extensive chemical guidelines. We investigate whether today's language models can assist responders by rapidly and reliably understanding critical information, identifying hazards, and providing recommendations.We introduce the Chemical Emergency Response Evaluation Framework (ChEmREF), a new benchmark comprising questions on 1,035 HAZMAT chemicals from the Emergency Response Guidebook and the PubChem Database. ChEmREF is organized into three tasks: (1) translation of chemical representation between structured and unstructured forms (e.g., converting C2H6O to ethanol), (2) emergency response generation (e.g., recommending appropriate evacuation distances) and (3) domain knowledge question answering from chemical safety and certification exams. Our best evaluated models received an exact match of 68.0% on unstructured HAZMAT chemical representation translation, a LLM Judge score of 52.7% on incident response recommendations, and a multiple-choice accuracy of 63.9% on HAMZAT examinations.These findings suggest that while language models show potential to assist emergency responders in various tasks, they require careful human oversight due to their current limitations.

</details>


### [17] [Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning](https://arxiv.org/abs/2511.10037)
*Xiaolong Wei,Yuehu Dong,Xingliang Wang,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.AI

TL;DR: 提出了一种新的Planner-centric Plan-Execute范式，通过全局DAG规划解决现有工具增强LLM在复杂查询处理中的局部优化陷阱问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强大语言模型在处理复杂查询时面临显著挑战，如ReAct等框架因依赖增量决策过程而容易陷入局部优化陷阱。

Method: 提出Planner-centric Plan-Execute范式，核心是新型Planner模型执行全局DAG规划；引入ComplexTool-Plan基准数据集；开发两阶段训练方法（SFT+GRPO）提升规划能力。

Result: 在StableToolBench基准测试中实现了最先进的性能，展示了优越的端到端执行能力和对复杂多工具工作流的鲁棒处理。

Conclusion: 该框架通过架构创新从根本上解决了局部优化瓶颈，为复杂查询处理提供了有效的解决方案。

Abstract: Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Planner's tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.

</details>


### [18] [Efficient Thought Space Exploration through Strategic Intervention](https://arxiv.org/abs/2511.10038)
*Ziheng Li,Hengyi Cai,Xiaochi Wei,Yuchen Li,Shuaiqiang Wang,Zhi-Hong Deng,Dawei Yin*

Main category: cs.AI

TL;DR: HPR框架通过分布不一致性减少（DIR）指标动态识别关键决策点，让强大的提示者模型提供概率指导，高效的小型实践者模型执行主要推理步骤，在算术和常识推理任务中实现最先进的效率-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 当前推理时扩展方法通过穷举采样产生过高计算成本，研究发现大多数下一个token预测与正确输出一致，只有少数关键token会导致偏差，需要针对性干预。

Method: 提出Hint-Practice Reasoning（HPR）框架，包含提示者（强大LLM）和实践者（高效小模型）两个协同组件，使用分布不一致性减少（DIR）指标在树状概率空间中动态识别干预点。

Result: 在算术和常识推理基准测试中，HPR达到与自一致性和MCTS基线相当的性能，同时仅解码1/5的token，在保持相似或更低FLOPs的情况下，比现有方法最多提高5.1%绝对准确率。

Conclusion: HPR框架通过选择性干预关键决策点，实现了高效的推理过程，在保持高性能的同时显著降低了计算成本。

Abstract: While large language models (LLMs) demonstrate emerging reasoning capabilities, current inference-time expansion methods incur prohibitive computational costs by exhaustive sampling. Through analyzing decoding trajectories, we observe that most next-token predictions align well with the golden output, except for a few critical tokens that lead to deviations. Inspired by this phenomenon, we propose a novel Hint-Practice Reasoning (HPR) framework that operationalizes this insight through two synergistic components: 1) a hinter (powerful LLM) that provides probabilistic guidance at critical decision points, and 2) a practitioner (efficient smaller model) that executes major reasoning steps. The framework's core innovation lies in Distributional Inconsistency Reduction (DIR), a theoretically-grounded metric that dynamically identifies intervention points by quantifying the divergence between practitioner's reasoning trajectory and hinter's expected distribution in a tree-structured probabilistic space. Through iterative tree updates guided by DIR, HPR reweights promising reasoning paths while deprioritizing low-probability branches. Experiments across arithmetic and commonsense reasoning benchmarks demonstrate HPR's state-of-the-art efficiency-accuracy tradeoffs: it achieves comparable performance to self-consistency and MCTS baselines while decoding only 1/5 tokens, and outperforms existing methods by at most 5.1% absolute accuracy while maintaining similar or lower FLOPs.

</details>


### [19] [Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical Report Generation](https://arxiv.org/abs/2511.10065)
*Bodong Du,Honglong Yang,Xiaomeng Li*

Main category: cs.AI

TL;DR: RadFlow是一个分层工作流引导的强化优化框架，用于医学报告生成，通过建模临床报告的结构化特性来解决现有系统忽视报告层次组织的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学报告生成系统将报告视为扁平序列，忽视了放射科医生结构化的工作流程（描述发现、总结印象、精炼关键病例），导致描述性和诊断性内容不一致。

Method: 提出RadFlow框架，包含临床基础奖励层次：全局奖励整合语言流畅性、医学领域正确性和发现与印象之间的跨部分一致性；局部奖励强调印象质量；关键感知策略优化机制自适应规范高风险病例的学习。

Result: 在胸部X光和颈动脉超声数据集上的实验表明，RadFlow相比最先进的基线方法持续提高了诊断一致性和整体报告质量。

Conclusion: RadFlow通过将结构化报告范式转化为强化微调过程，能够生成既语言一致又临床对齐的报告，更好地模拟了放射科医生的实际工作流程。

Abstract: Radiologists compose diagnostic reports through a structured workflow: they describe visual findings, summarize them into impressions, and carefully refine statements in clinically critical cases. However, most existing medical report generation (MRG) systems treat reports as flat sequences, overlooking this hierarchical organization and leading to inconsistencies between descriptive and diagnostic content. To align model behavior with real-world reporting practices, we propose RadFlow, a hierarchical workflow-guided reinforcement optimization framework that explicitly models the structured nature of clinical reporting. RadFlow introduces a clinically grounded reward hierarchy that mirrors the organization of radiological reports. At the global level, the reward integrates linguistic fluency, medical-domain correctness, and cross-sectional consistency between Finding and Impression, promoting coherent and clinically faithful narratives. At the local level, a section-specific reward emphasizes Impression quality, reflecting its central role in diagnostic accuracy. Furthermore, a critical-aware policy optimization mechanism adaptively regularizes learning for high-risk or clinically sensitive cases, emulating the cautious refinement behavior of radiologists when documenting critical findings. Together, these components translate the structured reporting paradigm into the reinforcement fine-tuning process, enabling the model to generate reports that are both linguistically consistent and clinically aligned. Experiments on chest X-ray and carotid ultrasound datasets demonstrate that RadFlow consistently improves diagnostic coherence and overall report quality compared with state-of-the-art baselines.

</details>


### [20] [Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning](https://arxiv.org/abs/2511.10067)
*Yuxuan Zhou,Yubin Wang,Bin Wang,Chen Ning,Xien Liu,Ji Wu,Jianye Hao*

Main category: cs.AI

TL;DR: MuSeR是一种数据驱动方法，通过自我评估和精炼来增强LLM在医疗领域的上下文感知能力，涵盖决策制定、沟通和安全三个关键方面。该方法通过属性条件查询生成器模拟真实用户场景，让LLM自我评估并精炼回答，然后用于监督微调。在HealthBench数据集上显著提升性能，特别是上下文感知方面。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医疗基准测试中表现良好，但在真实医疗场景中仍表现不佳，需要更强的上下文感知能力来识别缺失或关键细节，并提供安全、有帮助且上下文适当的回答。

Method: 提出多层面自我精炼(MuSeR)方法：1) 使用属性条件查询生成器模拟多样化真实用户上下文；2) LLM回答查询并沿三个关键层面自我评估；3) 精炼回答以更好符合各层面要求；4) 使用查询和精炼回答进行监督微调。

Result: 在最新HealthBench数据集上，该方法显著提升LLM性能，特别是在上下文感知维度。通过结合知识蒸馏，较小骨干LLM(Qwen3-32B)性能超过其教师模型，在HealthBench上达到63.8%的新SOTA，其困难子集达到43.1%。

Conclusion: MuSeR方法有效增强了LLM在医疗领域的上下文感知能力，通过自我评估和精炼机制显著提升模型在真实医疗场景中的表现，为医疗AI应用提供了有前景的解决方案。

Abstract: Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at https://muser-llm.github.io.

</details>


### [21] [RAGFort: Dual-Path Defense Against Proprietary Knowledge Base Extraction in Retrieval-Augmented Generation](https://arxiv.org/abs/2511.10128)
*Qinfeng Li,Miao Pan,Ke Xiong,Ge Su,Zhiqiang Shen,Yan Liu,Bing Sun,Hao Peng,Xuhong Zhang*

Main category: cs.AI

TL;DR: 本文提出RAGFort防御系统，通过对比重索引和约束级联生成的双模块设计，有效抵御针对检索增强生成系统的知识库重构攻击，同时保持回答质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统防御方法仅针对知识重构攻击的单一路径（类内或类间），无法提供全面保护，导致知识库面临被完整提取的风险。

Method: 提出RAGFort防御框架，包含两个核心模块：对比重索引实现类间隔离，约束级联生成实现类内保护，形成结构感知的双重防御机制。

Result: 实验验证RAGFort在安全性、性能和鲁棒性方面均表现优异，显著降低重构攻击成功率，同时保持回答质量不受影响。

Conclusion: RAGFort通过联合保护类内和类间路径，为RAG系统提供了全面有效的知识库提取防御方案，解决了现有防御方法的局限性。

Abstract: Retrieval-Augmented Generation (RAG) systems deployed over proprietary knowledge bases face growing threats from reconstruction attacks that aggregate model responses to replicate knowledge bases. Such attacks exploit both intra-class and inter-class paths, progressively extracting fine-grained knowledge within topics and diffusing it across semantically related ones, thereby enabling comprehensive extraction of the original knowledge base. However, existing defenses target only one path, leaving the other unprotected. We conduct a systematic exploration to assess the impact of protecting each path independently and find that joint protection is essential for effective defense. Based on this, we propose RAGFort, a structure-aware dual-module defense combining "contrastive reindexing" for inter-class isolation and "constrained cascade generation" for intra-class protection. Experiments across security, performance, and robustness confirm that RAGFort significantly reduces reconstruction success while preserving answer quality, offering comprehensive defense against knowledge base extraction attacks.

</details>


### [22] [DenoGrad: Deep Gradient Denoising Framework for Enhancing the Performance of Interpretable AI Models](https://arxiv.org/abs/2511.10161)
*J. Javier Alonso-Ramos,Ignacio Aguilera-Martos,Andrés Herrera-Poyatos,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文提出DenoGrad，一种基于梯度的实例去噪框架，利用在目标数据上训练的深度学习模型的梯度来检测和调整噪声样本，相比传统方法能更好地保持数据分布并提升AI模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法会改变原始数据分布，导致不现实的场景和偏置模型，这在可解释AI中尤其成问题，因为其可解释性依赖于底层数据模式的保真度。

Method: 提出DenoGrad框架，使用任务特定的高质量解决方案作为参考，利用准确深度学习模型的梯度来动态检测和校正噪声实例，保持问题数据分布。

Result: 在表格和时间序列数据集的各种噪声设置下，DenoGrad优于现有最先进的去噪策略，是唯一能保持原始数据分布的高质量方法。

Conclusion: DenoGrad通过基于梯度的动态校正方法，有效提升了可解释AI模型的性能，同时保持了数据分布的真实性。

Abstract: The performance of Machine Learning (ML) models, particularly those operating within the Interpretable Artificial Intelligence (Interpretable AI) framework, is significantly affected by the presence of noise in both training and production data. Denoising has therefore become a critical preprocessing step, typically categorized into instance removal and instance correction techniques. However, existing correction approaches often degrade performance or oversimplify the problem by altering the original data distribution. This leads to unrealistic scenarios and biased models, which is particularly problematic in contexts where interpretable AI models are employed, as their interpretability depends on the fidelity of the underlying data patterns. In this paper, we argue that defining noise independently of the solution may be ineffective, as its nature can vary significantly across tasks and datasets. Using a task-specific high quality solution as a reference can provide a more precise and adaptable noise definition. To this end, we propose DenoGrad, a novel Gradient-based instance Denoiser framework that leverages gradients from an accurate Deep Learning (DL) model trained on the target data -- regardless of the specific task -- to detect and adjust noisy samples. Unlike conventional approaches, DenoGrad dynamically corrects noisy instances, preserving problem's data distribution, and improving AI models robustness. DenoGrad is validated on both tabular and time series datasets under various noise settings against the state-of-the-art. DenoGrad outperforms existing denoising strategies, enhancing the performance of interpretable IA models while standing out as the only high quality approach that preserves the original data distribution.

</details>


### [23] [Two Constraint Compilation Methods for Lifted Planning](https://arxiv.org/abs/2511.10164)
*Periklis Mantenoglou,Luigi Bonassi,Enrico Scala,Pedro Zuidberg Dos Martires*

Main category: cs.AI

TL;DR: 该论文提出了两种无需基础化（grounding）就能编译掉PDDL中定性状态轨迹约束的方法，解决了现有编译器在大规模规划问题中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界规划问题通常包含安全性要求、任务排序条件和中间子目标等定性状态轨迹约束。现有方法需要先基础化问题再进行编译，这在对象数量多、动作元数高的情况下无法扩展。

Method: 提出了两种无需基础化的约束编译方法：一种基于约束的编码，另一种基于动作的编码。两种方法都能避免问题的基础化过程。

Result: 在国际规划竞赛领域上的实验表明，新方法高效且产生的规划规范比基础化编译器产生的规范简洁数个数量级，同时在使用最先进规划器时仍保持竞争力。

Conclusion: 无需基础化的约束编译方法能够有效处理大规模规划问题中的定性状态轨迹约束，显著提高了编译效率并减少了规划规范的规模。

Abstract: We study planning in a fragment of PDDL with qualitative state-trajectory constraints, capturing safety requirements, task ordering conditions, and intermediate sub-goals commonly found in real-world problems. A prominent approach to tackle such problems is to compile their constraints away, leading to a problem that is supported by state-of-the-art planners. Unfortunately, existing compilers do not scale on problems with a large number of objects and high-arity actions, as they necessitate grounding the problem before compilation. To address this issue, we propose two methods for compiling away constraints without grounding, making them suitable for large-scale planning problems. We prove the correctness of our compilers and outline their worst-case time complexity. Moreover, we present a reproducible empirical evaluation on the domains used in the latest International Planning Competition. Our results demonstrate that our methods are efficient and produce planning specifications that are orders of magnitude more succinct than the ones produced by compilers that ground the domain, while remaining competitive when used for planning with a state-of-the-art planner.

</details>


### [24] [MTP: Exploring Multimodal Urban Traffic Profiling with Modality Augmentation and Spectrum Fusion](https://arxiv.org/abs/2511.10218)
*Haolong Xiang,Peisi Wang,Xiaolong Xu,Kun Yi,Xuyun Zhang,Quanzheng Sheng,Amin Beheshti,Wei Fan*

Main category: cs.AI

TL;DR: MTP是一个用于城市交通画像的多模态框架，通过数值、视觉和文本三个视角学习多模态特征，在频域中进行交通信号学习，并在六个真实数据集上表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有交通信号建模方法主要依赖传感器原始数值数据，忽略了多模态异构城市数据中的语义信息，这阻碍了对交通信号的全面理解并限制了复杂交通动态的准确预测。

Method: 提出MTP多模态框架，通过三个分支学习：1）将交通信号转换为频域图像和周期性图像进行视觉学习；2）基于特定主题、背景信息和项目描述生成描述性文本进行文本学习；3）使用频域多层感知器对原始数值数据进行学习；采用分层对比学习融合三个模态的信息。

Result: 在六个真实世界数据集上的广泛实验表明，该方法相比最先进方法具有优越性能。

Conclusion: MTP框架通过整合数值、视觉和文本多模态信息，能够更全面地理解交通信号，有效提升交通动态预测的准确性。

Abstract: With rapid urbanization in the modern era, traffic signals from various sensors have been playing a significant role in monitoring the states of cities, which provides a strong foundation in ensuring safe travel, reducing traffic congestion and optimizing urban mobility. Most existing methods for traffic signal modeling often rely on the original data modality, i.e., numerical direct readings from the sensors in cities. However, this unimodal approach overlooks the semantic information existing in multimodal heterogeneous urban data in different perspectives, which hinders a comprehensive understanding of traffic signals and limits the accurate prediction of complex traffic dynamics. To address this problem, we propose a novel \textit{M}ultimodal framework, \textit{MTP}, for urban \textit{T}raffic \textit{P}rofiling, which learns multimodal features through numeric, visual, and textual perspectives. The three branches drive for a multimodal perspective of urban traffic signal learning in the frequency domain, while the frequency learning strategies delicately refine the information for extraction. Specifically, we first conduct the visual augmentation for the traffic signals, which transforms the original modality into frequency images and periodicity images for visual learning. Also, we augment descriptive texts for the traffic signals based on the specific topic, background information and item description for textual learning. To complement the numeric information, we utilize frequency multilayer perceptrons for learning on the original modality. We design a hierarchical contrastive learning on the three branches to fuse the spectrum of three modalities. Finally, extensive experiments on six real-world datasets demonstrate superior performance compared with the state-of-the-art approaches.

</details>


### [25] [Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation](https://arxiv.org/abs/2511.10233)
*Jianghan Zhu,Yaoxin Wu,Zhuoyi Lin,Zhengyuan Zhang,Haiyan Yin,Zhiguang Cao,Senthilnath Jayavelu,Xiaoli Li*

Main category: cs.AI

TL;DR: EvoReal方法通过进化模块和大型语言模型生成具有真实世界结构特征的合成实例，显著提升了神经组合优化方法在真实基准实例上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器在从均匀分布的合成训练数据泛化到真实世界VRP场景时存在困难，需要弥合这一泛化差距。

Method: 使用进化模块和LLM生成结构特征统计上模拟真实实例的合成实例，然后对预训练NCO模型进行渐进式精炼，先对齐结构丰富的合成分布，再在真实基准实例上微调。

Result: EvoReal显著提升了最先进神经求解器的泛化能力，在TSPLib和CVRPLib基准上的性能差距分别降至1.05%和2.71%。

Conclusion: EvoReal通过生成结构真实的合成实例和渐进式精炼策略，有效缩小了神经求解器在真实世界VRP问题上的性能差距。

Abstract: Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales.

</details>


### [26] [ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs](https://arxiv.org/abs/2511.10240)
*Minbae Park,Hyemin Yang,Jeonghyun Kim,Kunsoo Park,Hyunjoon Kim*

Main category: cs.AI

TL;DR: ProgRAG是一个多跳知识图谱问答框架，通过将复杂问题分解为子问题，逐步扩展推理路径来解决LLM在知识图谱问答中的检索和推理失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有KG增强的LLM方法在复杂知识密集型任务中仍面临检索不准确和推理失败的问题，特别是长输入上下文掩盖相关信息，以及上下文构建难以捕捉不同问题类型所需的丰富逻辑方向。

Method: ProgRAG将复杂问题分解为子问题，逐步扩展部分推理路径。在每一步中，外部检索器收集候选证据，然后通过LLM进行不确定性感知剪枝。最后，通过组织和重新排列从子问题答案获得的部分推理路径来优化LLM推理的上下文。

Result: 在三个知名数据集上的实验表明，ProgRAG在多跳知识图谱问答中优于现有基线方法，提供了更高的可靠性和推理质量。

Conclusion: ProgRAG通过逐步分解和扩展推理路径的方法，有效解决了KG增强LLM中的检索和推理挑战，在多跳知识图谱问答任务中表现出色。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.

</details>


### [27] [Beyond Single-Step Updates: Reinforcement Learning of Heuristics with Limited-Horizon Search](https://arxiv.org/abs/2511.10264)
*Gal Hadar,Forest Agostinelli,Shahaf S. Shperberg*

Main category: cs.AI

TL;DR: 本文提出了一种增强启发式搜索的通用方法，通过有限范围搜索和基于到搜索边界最短路径的启发式更新来改进状态采样和启发式函数学习。


<details>
  <summary>Details</summary>
Motivation: 传统启发式搜索方法依赖单步贝尔曼更新，仅基于最佳邻居状态更新启发式函数，这种方法可能无法充分利用状态间的多步关系。

Method: 提出广义方法，执行有限范围搜索，基于到搜索边界的最短路径（结合边成本和边界状态启发式值）来更新每个状态的启发式函数。

Result: 该方法能够更有效地学习启发式函数，通过考虑多步路径关系提升搜索性能。

Conclusion: 通过有限范围搜索和基于最短路径的启发式更新，可以显著改进启发式搜索的效果，为顺序决策问题提供更优的解决方案。

Abstract: Many sequential decision-making problems can be formulated as shortest-path problems, where the objective is to reach a goal state from a given starting state. Heuristic search is a standard approach for solving such problems, relying on a heuristic function to estimate the cost to the goal from any given state. Recent approaches leverage reinforcement learning to learn heuristics by applying deep approximate value iteration. These methods typically rely on single-step Bellman updates, where the heuristic of a state is updated based on its best neighbor and the corresponding edge cost. This work proposes a generalized approach that enhances both state sampling and heuristic updates by performing limited-horizon searches and updating each state's heuristic based on the shortest path to the search frontier, incorporating both edge costs and the heuristic values of frontier states.

</details>


### [28] [Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention](https://arxiv.org/abs/2511.10268)
*Zhe Xu,Zhicai Wang,Junkang Wu,Jinda Lu,Xiang Wang*

Main category: cs.AI

TL;DR: 该论文提出大型视觉语言模型存在物体幻觉问题，主要源于训练中高度共现物体之间的伪相关性。作者引入因果分析建立结构因果模型，开发了Causal-HalBench基准来量化伪相关性影响，并提出了可扩展的反事实样本生成流程。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型经常出现物体幻觉问题，错误判断图像中物体的存在。现有基准主要关注幻觉检测，但缺乏对伪相关性的形式化描述和定量评估。

Method: 将因果分析引入大型视觉语言模型的物体识别场景，建立结构因果模型。开发Causal-HalBench基准，包含反事实样本和综合因果指标，并提出了利用专有LVLMs和文本到图像模型生成反事实样本的可扩展流程。

Result: 对主流大型视觉语言模型的评估表明，这些模型在不同程度上都容易受到伪相关性的影响。

Conclusion: 大型视觉语言模型确实存在由物体共现偏差引起的伪相关性问题，需要通过因果分析方法和专门的基准来进行量化和评估。

Abstract: Large Vision-Language Models (LVLMs) often suffer from object hallucination, making erroneous judgments about the presence of objects in images. We propose this primar- ily stems from spurious correlations arising when models strongly associate highly co-occurring objects during train- ing, leading to hallucinated objects influenced by visual con- text. Current benchmarks mainly focus on hallucination de- tection but lack a formal characterization and quantitative evaluation of spurious correlations in LVLMs. To address this, we introduce causal analysis into the object recognition scenario of LVLMs, establishing a Structural Causal Model (SCM). Utilizing the language of causality, we formally de- fine spurious correlations arising from co-occurrence bias. To quantify the influence induced by these spurious correla- tions, we develop Causal-HalBench, a benchmark specifically constructed with counterfactual samples and integrated with comprehensive causal metrics designed to assess model ro- bustness against spurious correlations. Concurrently, we pro- pose an extensible pipeline for the construction of these coun- terfactual samples, leveraging the capabilities of proprietary LVLMs and Text-to-Image (T2I) models for their genera- tion. Our evaluations on mainstream LVLMs using Causal- HalBench demonstrate these models exhibit susceptibility to spurious correlations, albeit to varying extents.

</details>


### [29] [Bidirectional Bounded-Suboptimal Heuristic Search with Consistent Heuristics](https://arxiv.org/abs/2511.10272)
*Shahaf S. Shperberg,Natalie Morad,Lior Siag,Ariel Felner,Dor Atzmon*

Main category: cs.AI

TL;DR: 本文基于最优双向搜索算法BAE*，针对有界次优双向搜索问题提出了多个变体算法，并通过实验比较了这些新算法与其他有界次优双向算法以及标准加权A*算法的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然双向启发式搜索在理论上取得了重要进展，但以往研究主要集中在最优搜索方法上。本文关注有界次优双向搜索问题，即指定解成本的次优性界限的情况。

Method: 基于最先进的最优双向搜索算法BAE*（适用于一致启发式），提出了多个专门针对有界次优上下文定制的BAE*变体算法。

Result: 实验评估表明，每种算法在不同条件下表现优异，突显了各种方法的优势和局限性。

Conclusion: 不同算法在不同场景下各有优势，需要根据具体条件选择合适的算法。

Abstract: Recent advancements in bidirectional heuristic search have yielded significant theoretical insights and novel algorithms. While most previous work has concentrated on optimal search methods, this paper focuses on bounded-suboptimal bidirectional search, where a bound on the suboptimality of the solution cost is specified. We build upon the state-of-the-art optimal bidirectional search algorithm, BAE*, designed for consistent heuristics, and introduce several variants of BAE* specifically tailored for the bounded-suboptimal context. Through experimental evaluation, we compare the performance of these new variants against other bounded-suboptimal bidirectional algorithms as well as the standard weighted A* algorithm. Our results demonstrate that each algorithm excels under distinct conditions, highlighting the strengths and weaknesses of each approach.

</details>


### [30] [FactGuard: Event-Centric and Commonsense-Guided Fake News Detection](https://arxiv.org/abs/2511.10281)
*Jing He,Han Zhang,Yuanhui Xiao,Wei Guo,Shaowen Yao,Renyang Liu*

Main category: cs.AI

TL;DR: FactGuard是一个利用大语言模型提取事件中心内容来减少写作风格影响的假新闻检测框架，通过动态可用性机制和知识蒸馏实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 随着攻击者模仿真实新闻风格，基于写作风格的假新闻检测方法效果逐渐减弱，而现有LLM方法存在功能探索浅、可用性模糊和推理成本高等问题。

Method: 提出FactGuard框架：1）利用LLM提取事件中心内容减少风格影响；2）引入动态可用性机制识别事实推理中的矛盾和模糊案例；3）通过知识蒸馏得到FactGuard-D用于冷启动和资源受限场景。

Result: 在两个基准数据集上的综合实验表明，该方法在鲁棒性和准确性方面持续优于现有方法，有效解决了假新闻检测中的风格敏感性和LLM可用性问题。

Conclusion: FactGuard框架通过事件内容提取和动态可用性机制，显著提升了假新闻检测的性能和实用性，为LLM在假新闻检测中的实际应用提供了可行方案。

Abstract: Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.

</details>


### [31] [Beyond Verification: Abductive Explanations for Post-AI Assessment of Privacy Leakage](https://arxiv.org/abs/2511.10284)
*Belona Sonna,Alban Grastien,Claire Benn*

Main category: cs.AI

TL;DR: 提出基于溯因解释的隐私泄露审计框架，通过识别最小充分证据来检测敏感信息泄露，在德国信用数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: AI决策过程中的隐私泄露风险日益严重，需要既能提供严格隐私保证又能生成人类可理解解释的审计工具。

Method: 使用溯因解释框架，识别最小充分证据来证明模型决策，引入潜在适用解释(PAE)概念来识别能保护敏感特征个体的结果。

Result: 在德国信用数据集上的实验表明，敏感字面量在模型决策过程中的重要性会影响隐私泄露程度。

Conclusion: 尽管存在计算挑战和简化假设，溯因推理能够实现可解释的隐私审计，为协调透明度、模型可解释性和隐私保护提供了实用途径。

Abstract: Privacy leakage in AI-based decision processes poses significant risks, particularly when sensitive information can be inferred. We propose a formal framework to audit privacy leakage using abductive explanations, which identifies minimal sufficient evidence justifying model decisions and determines whether sensitive information disclosed. Our framework formalizes both individual and system-level leakage, introducing the notion of Potentially Applicable Explanations (PAE) to identify individuals whose outcomes can shield those with sensitive features. This approach provides rigorous privacy guarantees while producing human understandable explanations, a key requirement for auditing tools. Experimental evaluation on the German Credit Dataset illustrates how the importance of sensitive literal in the model decision process affects privacy leakage. Despite computational challenges and simplifying assumptions, our results demonstrate that abductive reasoning enables interpretable privacy auditing, offering a practical pathway to reconcile transparency, model interpretability, and privacy preserving in AI decision-making.

</details>


### [32] [SITA: A Framework for Structure-to-Instance Theorem Autoformalization](https://arxiv.org/abs/2511.10356)
*Chenyi Li,Wanli Ma,Zichen Wang,Zaiwen Wen*

Main category: cs.AI

TL;DR: 本文提出了一个结构到实例定理自动形式化（SITA）框架，用于自动形式化抽象数学结构在具体实例中的应用，特别针对Lean证明助手环境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理方面虽有进展，但在形式化抽象结构具体实例化产生的定理时仍面临挑战。目标是实现研究级数学结果的自动形式化。

Method: 将形式化的抽象结构作为模块化模板，包含定义、假设、操作和定理。利用Lean的类型类机制，通过LLM生成和反馈引导的细化来确保自动化和形式正确性。

Result: 在优化问题数据集上的实验表明，SITA能够有效形式化基于抽象结构的多样化实例。

Conclusion: SITA框架成功弥合了抽象数学理论与具体应用之间的差距，为研究级数学结果的自动形式化提供了有效解决方案。

Abstract: While large language models (LLMs) have shown progress in mathematical reasoning, they still face challenges in formalizing theorems that arise from instantiating abstract structures in concrete settings. With the goal of auto-formalizing mathematical results at the research level, we develop a framework for structure-to-instance theorem autoformalization (SITA), which systematically bridges the gap between abstract mathematical theories and their concrete applications in Lean proof assistant. Formalized abstract structures are treated as modular templates that contain definitions, assumptions, operations, and theorems. These templates serve as reusable guides for the formalization of concrete instances. Given a specific instantiation, we generate corresponding Lean definitions and instance declarations, integrate them using Lean's typeclass mechanism, and construct verified theorems by checking structural assumptions. We incorporate LLM-based generation with feedback-guided refinement to ensure both automation and formal correctness. Experiments on a dataset of optimization problems demonstrate that SITA effectively formalizes diverse instances grounded in abstract structures.

</details>


### [33] [Explaining Decentralized Multi-Agent Reinforcement Learning Policies](https://arxiv.org/abs/2511.10409)
*Kayla Boggess,Sarit Kraus,Lu Feng*

Main category: cs.AI

TL;DR: 提出了一种为去中心化多智能体强化学习生成策略摘要和基于查询解释的方法，能够捕捉任务排序和智能体合作，并回答关于特定智能体行为的用户查询。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法主要关注中心化多智能体强化学习，无法处理去中心化设置中固有的不确定性和非确定性。

Method: 生成策略摘要以捕捉任务排序和智能体合作，并提供基于查询的解释（When、Why Not、What类型）来回答特定智能体行为的用户查询。

Result: 在四个多智能体强化学习领域和两种去中心化算法上评估，展示了方法的通用性和计算效率。用户研究表明，策略摘要和解释显著提高了用户问答性能，并增强了理解和满意度等主观评分。

Conclusion: 该方法有效解决了去中心化多智能体强化学习中的解释性问题，提高了用户对智能体行为的理解和满意度。

Abstract: Multi-Agent Reinforcement Learning (MARL) has gained significant interest in recent years, enabling sequential decision-making across multiple agents in various domains. However, most existing explanation methods focus on centralized MARL, failing to address the uncertainty and nondeterminism inherent in decentralized settings. We propose methods to generate policy summarizations that capture task ordering and agent cooperation in decentralized MARL policies, along with query-based explanations for When, Why Not, and What types of user queries about specific agent behaviors. We evaluate our approach across four MARL domains and two decentralized MARL algorithms, demonstrating its generalizability and computational efficiency. User studies show that our summarizations and explanations significantly improve user question-answering performance and enhance subjective ratings on metrics such as understanding and satisfaction.

</details>


### [34] [Generalizing Analogical Inference from Boolean to Continuous Domains](https://arxiv.org/abs/2511.10416)
*Francisco Cunha,Yves Lepage,Zied Bouraoui,Miguel Couceiro*

Main category: cs.AI

TL;DR: 本文提出了一个统一的类比推理框架，适用于实值域，基于广义均值定义的参数化类比，涵盖了布尔分类和回归任务，并推导了误差界限。


<details>
  <summary>Details</summary>
Motivation: 现有的类比推理形式框架主要针对布尔域，无法扩展到回归任务或连续域，且现有泛化界限在布尔设置中也可能失效。

Method: 引入基于广义均值的参数化类比统一框架，支持连续函数的类比推理，并分析了类比保持函数的类别。

Result: 在平滑性假设下推导了最坏情况和平均情况误差界限，提供了一个跨离散和连续域的通用类比推理理论。

Conclusion: 该框架为类比推理提供了统一的理论基础，能够同时处理布尔分类和连续回归任务，扩展了类比推理的应用范围。

Abstract: Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.

</details>


### [35] [Using Certifying Constraint Solvers for Generating Step-wise Explanations](https://arxiv.org/abs/2511.10428)
*Ignace Bleukx,Maarten Flippo,Bart Bogaerts,Emir Demirović,Tias Guns*

Main category: cs.AI

TL;DR: 本文提出了一种基于约束求解器生成的证明来快速计算逐步解释序列的方法，通过抽象证明框架和修剪简化技术，显著提高了解释生成效率。


<details>
  <summary>Details</summary>
Motivation: 当前计算逐步解释的方法计算成本高昂，限制了其应用范围。需要找到更高效的方法来生成解释不可满足性问题的逐步推理过程。

Method: 定义抽象证明框架，将证明和逐步解释统一表示；提出多种从证明转换为逐步解释序列的方法，特别关注修剪和简化技术以保持序列简洁。

Result: 该方法显著加快了逐步解释序列的生成速度，同时生成的质量与当前最先进方法相当。

Conclusion: 利用约束求解器生成的证明作为起点，结合修剪简化技术，可以高效生成高质量的逐步解释序列，解决了原有方法计算成本高的问题。

Abstract: In the field of Explainable Constraint Solving, it is common to explain to a user why a problem is unsatisfiable. A recently proposed method for this is to compute a sequence of explanation steps. Such a step-wise explanation shows individual reasoning steps involving constraints from the original specification, that in the end explain a conflict. However, computing a step-wise explanation is computationally expensive, limiting the scope of problems for which it can be used. We investigate how we can use proofs generated by a constraint solver as a starting point for computing step-wise explanations, instead of computing them step-by-step. More specifically, we define a framework of abstract proofs, in which both proofs and step-wise explanations can be represented. We then propose several methods for converting a proof to a step-wise explanation sequence, with special attention to trimming and simplification techniques to keep the sequence and its individual steps small. Our results show our method significantly speeds up the generation of step-wise explanation sequences, while the resulting step-wise explanation has a quality similar to the current state-of-the-art.

</details>


### [36] [Preference Elicitation for Step-Wise Explanations in Logic Puzzles](https://arxiv.org/abs/2511.10436)
*Marco Foschini,Marianne Defresne,Emilio Gamba,Bart Bogaerts,Tias Guns*

Main category: cs.AI

TL;DR: 本文研究了如何通过交互式偏好学习方法来优化逻辑谜题的分步解释质量，提出了动态归一化技术和MACHOP查询生成策略，在数独和逻辑网格谜题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 分步解释虽然能展示逻辑谜题的推导过程，但存在多种候选解释步骤，需要用户定义目标函数来量化解释质量，而定义好的目标函数具有挑战性。

Method: 采用交互式偏好学习方法，提出动态归一化技术来稳定学习过程，并引入MACHOP查询生成策略结合非支配约束和上置信界多样化。

Result: 在数独和逻辑网格谜题上，MACHOP方法相比标准方法能持续产生更高质量的解释，在人工用户和真实用户评估中都表现更好。

Conclusion: 交互式偏好学习可以有效提升分步解释的质量，MACHOP策略在解释质量优化方面优于标准方法，为解决解释质量量化问题提供了可行方案。

Abstract: Step-wise explanations can explain logic puzzles and other satisfaction problems by showing how to derive decisions step by step. Each step consists of a set of constraints that derive an assignment to one or more decision variables. However, many candidate explanation steps exist, with different sets of constraints and different decisions they derive. To identify the most comprehensible one, a user-defined objective function is required to quantify the quality of each step. However, defining a good objective function is challenging. Here, interactive preference elicitation methods from the wider machine learning community can offer a way to learn user preferences from pairwise comparisons. We investigate the feasibility of this approach for step-wise explanations and address several limitations that distinguish it from elicitation for standard combinatorial problems. First, because the explanation quality is measured using multiple sub-objectives that can vary a lot in scale, we propose two dynamic normalization techniques to rescale these features and stabilize the learning process. We also observed that many generated comparisons involve similar explanations. For this reason, we introduce MACHOP (Multi-Armed CHOice Perceptron), a novel query generation strategy that integrates non-domination constraints with upper confidence bound-based diversification. We evaluate the elicitation techniques on Sudokus and Logic-Grid puzzles using artificial users, and validate them with a real-user evaluation. In both settings, MACHOP consistently produces higher-quality explanations than the standard approach.

</details>


### [37] [Non-Monotonic S4F Standpoint Logic](https://arxiv.org/abs/2511.10449)
*Piotr Gorczyca,Hannes Strass*

Main category: cs.AI

TL;DR: 本文提出了一种新的S4F立场逻辑形式化方法，它结合了S4F模态逻辑和立场命题逻辑，能够表达多视角、非单调的语义承诺。


<details>
  <summary>Details</summary>
Motivation: 为了统一表示多个异质视角，并将非单调推理框架自然地通过模态逻辑（特别是S4F）进行捕捉，需要一种能够同时处理多视角和非单调性的逻辑形式化方法。

Method: 提出了S4F立场逻辑，定义了其语法和语义，分析了计算复杂性，并概述了轻信和怀疑接受机制。

Result: S4F立场逻辑在计算复杂性上并不比其构成逻辑更复杂，无论是单调还是非单调形式。

Conclusion: S4F立场逻辑成功地将S4F和立场命题逻辑进行了泛化，能够表达多视角的非单调语义承诺，为多视角推理提供了有效的形式化工具。

Abstract: Standpoint logics offer unified modal logic-based formalisms for representing multiple heterogeneous viewpoints. At the same time, many non-monotonic reasoning frameworks can be naturally captured using modal logics, in particular using the modal logic S4F. In this work, we propose a novel formalism called S4F Standpoint Logic, which generalises both S4F and standpoint propositional logic and is therefore capable of expressing multi-viewpoint, non-monotonic semantic commitments. We define its syntax and semantics and analyze its computational complexity, obtaining the result that S4F Standpoint Logic is not computationally harder than its constituent logics, whether in monotonic or non-monotonic form. We also outline mechanisms for credulous and sceptical acceptance and illustrate the framework with an example.

</details>


### [38] [Proceedings of The third international workshop on eXplainable AI for the Arts (XAIxArts)](https://arxiv.org/abs/2511.10482)
*Corey Ford,Elizabeth Wilson,Shuoyang Zheng,Gabriel Vigliensoni,Jeba Rezwana,Lanxi Xiao,Michael Clemens,Makayla Lewis,Drew Hemment,Alan Chamberlain,Helen Kennedy,Nick Bryan-Kinns*

Main category: cs.AI

TL;DR: 第三届国际艺术可解释AI研讨会汇集了HCI、交互设计、AI、可解释AI和数字艺术领域的研究者，探讨XAI在艺术中的作用。


<details>
  <summary>Details</summary>
Motivation: 将可解释AI技术应用于艺术领域，促进不同学科研究者之间的交流与合作。

Method: 通过在线研讨会形式，在ACM创造力与认知会议上组织专题讨论。

Result: 成功汇集了来自HCI、交互设计、AI、XAI和数字艺术等多个领域的研究者。

Conclusion: 该研讨会为探索可解释AI在艺术中的应用提供了重要的交流平台。

Abstract: This third international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. Workshop held at the 17th ACM Conference on Creativity and Cognition (C&C 2025), online.

</details>


### [39] [Rethinking Science in the Age of Artificial Intelligence](https://arxiv.org/abs/2511.10524)
*Maksim E. Eren,Dorianis M. Perez*

Main category: cs.AI

TL;DR: AI正在从计算工具转变为科学研究的积极合作者，重塑研究流程，但需要谨慎整合和治理，确保AI增强而非取代人类判断。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何改变研究流程，从信息管理到实验设计，以及这种转变带来的整合和治理需求。

Method: 通过评论性分析，考察AI在研究工作流中的角色转变和应用现状。

Result: AI系统已能帮助研究人员管理信息、筛选文献、发现跨学科联系、生成假设并设计执行实验。

Conclusion: 需要在科学实践中审慎采用AI，通过促进透明度、可重复性和问责制的政策来确保AI作为人类判断的补充而非替代。

Abstract: Artificial intelligence (AI) is reshaping how research is conceived, conducted, and communicated across fields from chemistry to biomedicine. This commentary examines how AI is transforming the research workflow. AI systems now help researchers manage the information deluge, filtering the literature, surfacing cross-disciplinary links for ideas and collaborations, generating hypotheses, and designing and executing experiments. These developments mark a shift from AI as a mere computational tool to AI as an active collaborator in science. Yet this transformation demands thoughtful integration and governance. We argue that at this time AI must augment but not replace human judgment in academic workflows such as peer review, ethical evaluation, and validation of results. This paper calls for the deliberate adoption of AI within the scientific practice through policies that promote transparency, reproducibility, and accountability.

</details>


### [40] [Regular Games -- an Automata-Based General Game Playing Language](https://arxiv.org/abs/2511.10593)
*Radosław Miernik,Marek Szykuła,Jakub Kowalski,Jakub Cieśluk,Łukasz Galas,Wojciech Pawlik*

Main category: cs.AI

TL;DR: 提出了一个名为Regular Games (RG)的新通用游戏系统，旨在实现计算效率和游戏设计便利性。系统包含多个语言层级，核心是使用有限自动机定义规则的低级语言，支持所有有限回合制不完全信息游戏。RG在生成前向模型方面比现有系统更快，并提供了完整的开发工具生态系统。


<details>
  <summary>Details</summary>
Motivation: 开发一个既计算高效又便于游戏设计的通用游戏系统，解决现有系统在效率和易用性方面的不足。

Method: 采用分层语言设计：核心是使用有限自动机定义规则的低级语言，具有最小化机制便于自动处理；高级语言用于游戏设计，最终翻译为低级语言。

Result: RG生成的前向模型比当前最先进系统（Regular Boardgames, Ludii）更快，在效率方面表现更优。

Conclusion: RG系统成功实现了计算效率和游戏设计便利性的双重目标，为通用游戏开发提供了高效的工具生态系统。

Abstract: We propose a new General Game Playing (GGP) system called Regular Games (RG). The main goal of RG is to be both computationally efficient and convenient for game design. The system consists of several languages. The core component is a low-level language that defines the rules by a finite automaton. It is minimal with only a few mechanisms, which makes it easy for automatic processing (by agents, analysis, optimization, etc.). The language is universal for the class of all finite turn-based games with imperfect information. Higher-level languages are introduced for game design (by humans or Procedural Content Generation), which are eventually translated to a low-level language. RG generates faster forward models than the current state of the art, beating other GGP systems (Regular Boardgames, Ludii) in terms of efficiency. Additionally, RG's ecosystem includes an editor with LSP, automaton visualization, benchmarking tools, and a debugger of game description transformations.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [41] [Behavior Modeling for Training-free Building of Private Domain Multi Agent System](https://arxiv.org/abs/2511.10283)
*Won Ik Cho,Woonghee Han,Kyung Seo Ki,Young Min Kim*

Main category: cs.MA

TL;DR: 提出了一个无需训练和数据生成的私有领域多智能体对话系统框架，通过行为建模和文档化实现工具集成和领域适应。


<details>
  <summary>Details</summary>
Motivation: 现有开放领域框架难以应用于私有领域，存在工具格式异构、领域术语、API访问限制和复杂治理等问题，传统微调方法负担重且脆弱。

Method: 采用编排器、工具调用智能体和通用聊天智能体的三组件架构，通过结构化规范和领域指令定义工具集成，避免训练过程。

Result: 框架支持轻量级多智能体系统部署、API规范作为检索资源、生成合成对话进行评估，实现可持续的领域专业知识对齐。

Conclusion: 该框架为私有对话生态系统提供了一种无需持续重训练的可扩展适应方法，能够有效处理私有工具和动态上下文。

Abstract: The rise of agentic systems that combine orchestration, tool use, and conversational capabilities, has been more visible by the recent advent of large language models (LLMs). While open-domain frameworks exist, applying them in private domains remains difficult due to heterogeneous tool formats, domain-specific jargon, restricted accessibility of APIs, and complex governance. Conventional solutions, such as fine-tuning on synthetic dialogue data, are burdensome and brittle under domain shifts, and risk degrading general performance. In this light, we introduce a framework for private-domain multi-agent conversational systems that avoids training and data generation by adopting behavior modeling and documentation. Our design simply assumes an orchestrator, a tool-calling agent, and a general chat agent, with tool integration defined through structured specifications and domain-informed instructions. This approach enables scalable adaptation to private tools and evolving contexts without continual retraining. The framework supports practical use cases, including lightweight deployment of multi-agent systems, leveraging API specifications as retrieval resources, and generating synthetic dialogue for evaluation -- providing a sustainable method for aligning agent behavior with domain expertise in private conversational ecosystems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 本文评估了Ksurf方法在基于估计的资源编排任务中的表现，特别是在高可变性云场景下作为上下文多臂老虎机目标函数模型时的效果。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中资源编排和配置参数搜索是关键问题，云环境的不确定性和高可变性使得编排决策准确性降低。

Method: 使用Ksurf作为上下文多臂老虎机目标函数模型，结合Drone编排器，在VarBench Kubernetes基准测试上进行评估。

Result: Ksurf实现了显著更低的延迟方差（p95降低41%，p99降低47%），CPU使用率降低4%，主节点内存使用减少7MB，平均工作pod数量节省7%成本。

Conclusion: Ksurf在高可变性云环境中能够有效优化资源估计和编排，显著提升性能并降低成本。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [43] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa是一个新型的预训练性能建模框架，统一了多维优化特征和容错机制，通过增强的成本模型准确捕捉关键优化效果，并基于历史集群可靠性数据集成容错模型，为LLM预训练系统提供先验指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模从数十亿参数增长到数万亿参数，分布式预训练在大规模集群上变得必要。混合并行化策略虽然支持这种预训练，但巨大的组合策略空间带来了显著的优化挑战。传统手动调优方法成本高昂，现有性能建模方法存在关键局限：未能全面考虑常见优化特征，并忽略了容错机制（如检查点恢复）在长期预训练中的显著开销。

Method: 提出MoFa预训练性能建模框架，包含增强的成本模型来准确捕捉关键优化效果，并基于历史集群可靠性数据集成容错模型。同时开发了基于MoFa的调优系统，用于探索各种场景下的最优预训练性能和潜在瓶颈。

Result: 广泛的建模评估表明，MoFa在各种场景下都能实现高预测精度。通过全面的调优实验，该框架系统性地揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa框架为LLM预训练系统设计和部署提供了坚实的先验指导，能够有效解决大规模分布式预训练中的性能优化挑战。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [44] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: 本文分析了多GPU系统中由热不平衡引起的性能变化问题（称为Lit Silicon效应），提出了检测和缓解技术，并在AMD MI300X GPU系统上实现了最高6%的性能提升和4%的功耗改进。


<details>
  <summary>Details</summary>
Motivation: GPU系统在现代数据中心中广泛应用，但存在节点和集群级别的性能变化问题，严重影响高性能计算和AI工作负载（如大语言模型训练）。研究发现热不平衡与并发计算通信技术相互作用导致性能变化。

Method: 分析了单节点多GPU系统运行LLM训练的性能，提出Lit Silicon效应理论，建立了分析和功耗模型，设计了检测和缓解技术，评估了三种不同的功耗管理解决方案。

Result: 在两个AMD MI300X GPU系统和两个LLM训练框架上进行实验，观察到最高6%的性能提升和4%的功耗改进，可能为数据中心节省数亿美元成本。

Conclusion: 提出的解决方案几乎零成本，可以作为新的节点级功耗管理层轻松在数据中心中采用，有效解决了Lit Silicon问题。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [45] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: 论文提出CacheX解决方案，通过驱逐集在虚拟机内探测精确细粒度的缓存抽象，无需硬件或虚拟机监控程序支持，并展示了两种新技术：LLC竞争感知任务调度和虚拟颜色感知页面缓存管理。


<details>
  <summary>Details</summary>
Motivation: 在公共云中，CPU缓存可以在虚拟机之间分区或共享，但虚拟机无法了解缓存配置细节，也无法通过页面放置策略影响缓存使用，导致基于缓存的优化往往无效。

Method: 使用驱逐集在虚拟机内探测准确细粒度的缓存抽象，无需硬件或虚拟机监控程序支持，并开发了LLC竞争感知任务调度和虚拟颜色感知页面缓存管理两种技术。

Result: 在x86 Linux内核中实现的CacheX评估表明，它可以有效提高公共云虚拟机中各种工作负载的缓存利用率。

Conclusion: CacheX能够有效解决云虚拟机中缓存可见性和控制不足的问题，显著提升缓存利用效率。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [46] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 提出基于监督学习的稀疏矩阵重排序算法选择模型，通过理解矩阵特征与常用重排序算法之间的关系，实现自动化智能选择，在佛罗里达稀疏矩阵数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵排序算法选择依赖暴力搜索或经验知识，缺乏对不同稀疏矩阵结构的适应性，需要更智能的自动化选择方法。

Method: 采用监督学习模型，学习矩阵特征与常用重排序算法之间的关联关系，实现算法自动选择。

Result: 在佛罗里达稀疏矩阵数据集上，模型能准确预测最优重排序算法，相比仅使用AMD算法，求解时间减少55.37%，平均加速比为1.45。

Conclusion: 基于监督学习的稀疏矩阵重排序算法选择模型能有效提升稀疏矩阵求解效率，实现智能化的算法选择。

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [47] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种现代工作负载调度器分类的新方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类调度器，分析了它们从早期采用到现代实现的发展历程及算法使用和特性。


<details>
  <summary>Details</summary>
Motivation: 为了系统化地理解和分类现代工作负载调度器的发展历程，揭示不同类型调度器在算法设计和功能特性上的演变规律。

Method: 通过描述三类主要调度器（操作系统进程调度器、集群系统作业调度器、大数据调度器）的演进过程，分析其算法使用和功能特性，并进行比较分析。

Result: 建立了现代工作负载调度器的分类框架，揭示了各类调度器从早期到现代的发展轨迹，识别了它们在算法设计和功能实现上的差异。

Conclusion: 尽管调度器应用于不同规模的系统（本地和分布式），但在调度策略设计上存在显著的相似性，这为跨系统调度设计提供了重要启示。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [48] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE是一个用于合成高保真执行轨迹的框架，能够准确建模大语言模型工作负载，支持全面的并行化策略，可扩展到32K GPU规模。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在大型AI训练和推理系统上的性能优化需要可扩展的分布式工作负载执行建模机制，但大型基础设施访问受限且现有平台轨迹难以适应未来更大规模系统配置。

Method: 引入符号张量图生成器(STAGE)框架，合成高保真执行轨迹，支持全面的并行化策略，系统探索各种LLM架构和系统配置。

Result: STAGE展示了其可扩展性，能够合成跨越32K GPU的高保真LLM轨迹，同时在计算、内存和通信方面保持张量级精度。

Conclusion: STAGE将公开可用，以促进分布式机器学习系统的进一步研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>
