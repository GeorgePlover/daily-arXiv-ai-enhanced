<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.DC](#cs.DC) [Total: 13]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: DataSage是一个新颖的多智能体框架，通过外部知识检索、多角色辩论机制和多路径推理来解决现有数据洞察智能体在领域知识利用不足、分析深度浅和代码生成易出错的问题，在InsightBench上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动时代，全自动端到端数据分析特别是洞察发现对于组织决策至关重要。现有数据洞察智能体存在领域知识利用不足、分析深度浅和代码生成易出错等限制，无法提供满意结果。

Method: 提出DataSage多智能体框架，包含三个创新特性：外部知识检索以丰富分析上下文，多角色辩论机制模拟多样化分析视角并加深分析深度，多路径推理提高生成代码和洞察的准确性。

Result: 在InsightBench上的广泛实验表明，DataSage在所有难度级别上始终优于现有数据洞察智能体。

Conclusion: DataSage为自动化数据洞察发现提供了一个有效的解决方案，解决了现有方法的局限性。

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [2] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 该论文介绍了SpatiaLite基准测试，用于评估视觉语言模型的空间推理能力，发现当前先进VLMs主要依赖语言表示进行空间推理，在视觉中心任务上表现不足且效率低下，并提出了Imagery Driven Framework来改进。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型和视觉语言模型在逻辑推理、问题解决等方面表现出色，但空间推理这一人类认知的基本组成部分仍然是重大挑战。研究者假设想象力是空间世界模型中的主导推理机制。

Method: 引入SpatiaLite这一完全合成的基准测试，联合测量空间推理准确性和效率。通过综合实验分析VLMs的空间推理机制，并提出Imagery Driven Framework用于数据合成和训练。

Result: 发现三个关键结果：1）先进VLMs主要依赖语言表示进行推理和想象，在需要感知空间关系和3D几何变换的视觉中心任务上存在显著缺陷；2）当前空间推理机制效率严重低下，随着变换复杂度增加，token使用量快速增长；3）提出的IDF框架可以隐式构建内部世界模型，这对VLMs的空间推理至关重要。

Conclusion: 基于SpatiaLite，这项工作描绘了先进VLMs的空间推理限制和模式，识别了关键缺陷，并为未来进展提供了信息。Imagery Driven Framework有望改进VLMs的空间推理能力。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [3] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: 本文研究将Bjøru等人提出的反事实概率边界计算算法从马尔可夫结构因果模型扩展到半马尔可夫模型，以处理更复杂的混杂关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅适用于马尔可夫模型，其中外生变量仅影响单个内生变量。半马尔可夫模型允许外生变量影响多个内生变量，能表示更复杂的混杂关系，因此需要扩展方法。

Method: 通过最小示例说明扩展挑战，提出替代解决方案策略，并进行理论和计算评估。

Result: 识别了将分治算法扩展到半马尔可夫模型的挑战，并开发了多种替代策略来解决这些问题。

Conclusion: 成功将反事实概率边界计算方法扩展到半马尔可夫结构因果模型，为处理更复杂的混杂关系提供了可行方案。

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [4] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: 本文系统分析了智能交通系统中大型视觉语言模型在精心设计的越狱攻击下的脆弱性，提出了基于图像排版操纵和多轮提示的新型越狱攻击方法，并开发了多层响应过滤防御技术。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态推理和实际应用中表现出强大能力，但极易受到越狱攻击。本研究旨在揭示智能交通系统中集成LVLMs的安全漏洞，并开发相应的防御机制。

Method: 首先构建与交通相关的有害查询数据集；其次提出利用图像排版操纵和多轮提示的越狱攻击方法；最后设计多层响应过滤防御技术防止模型生成不当响应。

Result: 在开源和闭源的最先进LVLMs上进行了广泛实验，使用GPT-4判断和人工验证评估攻击和防御效果。与现有越狱技术相比，所提方法在智能交通系统中存在严重安全风险。

Conclusion: 图像排版操纵和多轮提示的越狱攻击对智能交通系统中的LVLMs构成严重威胁，需要多层防御机制来保护系统安全。

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [5] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: CORGI是一种新的模式匹配算法，针对规则系统中指数级复杂度的匹配问题，提供二次时间空间保证，能够迭代流式输出匹配结果，避免内存溢出。


<details>
  <summary>Details</summary>
Motivation: 解决实时AI系统和数据库查询中规则匹配的指数复杂度问题，特别是当AI系统自动生成规则时容易产生最坏情况匹配模式，导致程序执行缓慢或内存耗尽。

Method: 采用两步法：前向传递构建/维护基础关系图，后向迭代器按需生成匹配，不同于RETE的β内存收集部分匹配的方法。

Result: 在性能评估中，CORGI在简单组合匹配任务上显著优于SOAR和OPS5的RETE实现。

Conclusion: CORGI算法通过消除传统冲突集的内存负担，为实时应用提供了可预测的性能保证，使自动生成的规则系统更加实用。

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [6] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 提出了一种基于场景图引导的生成AI框架，通过分析OSHA事故报告生成逼真的工作场所危险场景图像，并引入VQA框架评估生成数据的真实性和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 获取真实的工作场所危险场景图像数据集非常困难，因为捕捉事故触发场景几乎不可能。需要一种方法来生成逼真的危险场景图像以训练视觉模型。

Method: 使用GPT-4o分析OSHA事故报告提取结构化危险推理，转换为对象级场景图，然后用文本到图像扩散模型生成构图准确的危险场景，并引入VQA框架进行评估。

Result: 提出的VQA图分数在四个最先进的生成模型中优于CLIP和BLIP指标，基于熵验证确认其具有更高的判别敏感性。

Conclusion: 该框架能够有效生成逼真的工作场所危险场景图像，为训练安全检测模型提供了可行的数据生成解决方案。

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [7] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: ALEX是一个轻量级知识编辑框架，通过分层内存架构将知识更新组织成语义簇，将检索复杂度从O(N)降低到O(K+N/C)，显著提高了多跳问答的准确性和推理路径的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的知识是静态的，难以适应不断变化的信息，现有方法在处理需要多步推理的复杂多跳问题时面临可扩展性和检索效率的挑战。

Method: ALEX采用分层内存架构组织知识更新，包含推理查询合成模块弥合查询与事实之间的语义差距，以及动态证据裁决引擎执行高效的两阶段检索过程。

Result: 在MQUAKE基准测试中，ALEX显著提高了多跳答案准确性和推理路径可靠性，同时将所需搜索空间减少了80%以上。

Conclusion: ALEX为构建可扩展、高效和准确的知识编辑系统提供了一条有前景的路径。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [8] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: Syn-STARTS是一个使用LLM生成大规模伤员事故分类案例的框架，验证了合成数据在开发高性能医疗AI模型中的潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模伤员事故中分类决策至关重要，但真实数据难以收集，需要开发合成数据生成方法来支持AI模型的训练和评估。

Method: 开发Syn-STARTS框架，利用LLM生成分类案例，并与人工整理的TRIAGE开放数据集进行定性比较，评估LLM在标准START分类方法下的准确性。

Result: Syn-STARTS生成的分类案例在质量上与人工整理的数据集难以区分，LLM在绿、黄、红、黑四个分类类别中的准确性表现高度稳定。

Conclusion: 合成数据在开发严重和关键医疗场景的高性能AI模型中具有重要潜力，为解决真实数据稀缺问题提供了可行方案。

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [9] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 本文提出了一种教师主导的自适应学习反馈循环，将概念级评估证据转化为经过验证的微干预。算法包含充分性、注意力和多样性三个保障机制，通过整数规划方法实现干预分配，在模拟和实际物理课程部署中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自适应学习系统诊断精确但干预薄弱，导致帮助时机不当或内容不匹配。需要开发能够将诊断证据有效转化为针对性教学干预的系统。

Method: 将干预分配建模为带约束的二进制整数规划问题，包含覆盖度、时间、难度窗口、先决条件和反冗余等约束。采用贪心选择、基于梯度的松弛方法和混合方法三种求解策略。

Result: 在1204名学生的物理课程部署中，两种求解器都能在有限观看时间内为几乎所有学习者实现完整的技能覆盖。基于梯度的方法比贪心方法减少约12%的冗余覆盖，同时在不同难度水平上实现更好的协调。

Conclusion: 该方法构建了一个可追踪和可审计的控制器，闭合了诊断-教学循环，在课堂规模上实现了公平且负载感知的个性化教学。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [10] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: APD-agents是一个基于大语言模型的多智能体框架，用于自动化移动应用页面设计，通过多个智能体的协作将用户描述转化为页面布局设计。


<details>
  <summary>Details</summary>
Motivation: 移动应用页面布局设计耗时且需要专业技能，现有设计软件需要大量培训，跨页面协作设计还需要额外时间统一标准。

Method: 采用多智能体框架，包含编排智能体、语义解析智能体、主布局智能体、模板检索智能体和递归组件智能体，通过智能体协作将用户描述转化为结构化数据并生成页面布局。

Result: 在RICO数据集上的实验结果表明，APD-agents达到了最先进的性能水平。

Conclusion: 该工作充分利用了大模型驱动的多智能体系统的自动协作能力，实现了高效的自动化页面设计。

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [11] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: 提出了R3框架，通过双过程思维将LLMs的泛化能力与VLN专家知识结合，包含Runner、Ruminator和Regulator三个核心模块，在REVERIE基准上显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLN任务中LLMs在理解真实世界空间关系方面的不足，以及计算成本和推理延迟问题，同时结合LLMs的常识知识和推理能力。

Method: 采用双过程思维框架，Runner是轻量级专家模型负责常规导航，Ruminator使用多模态LLM进行结构化推理，Regulator根据三个标准监控导航进度并控制思维模式。

Result: 在REVERIE基准上SPL和RGSPL分别超过现有最佳方法3.28%和3.30%，显著提升了VLN任务的完成性能。

Conclusion: R3框架有效解决了VLN任务中的挑战，通过整合LLMs泛化能力和领域专业知识，实现了性能的显著提升。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [12] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 本文测试了大型语言模型在金融经济学应用中对时间顺序的理解能力，通过时序排序、条件排序和时代错误检测等任务，发现模型在保持局部顺序的同时难以维持全局一致的时间线，而增加推理预算能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在金融和经济学中的广泛应用，需要验证这些模型是否真正理解时间顺序，以避免前瞻性偏差的影响。

Method: 设计了一系列复杂度递增的时间顺序任务，包括时序排序、条件排序（先过滤后排序）和时代错误检测，评估了GPT-4.1、Claude-3.7 Sonnet（有/无扩展思维）和GPT-5在不同推理强度设置下的表现。

Result: 随着序列长度增加，精确匹配率急剧下降，但等级相关性保持较高；条件排序中大多数失败源于过滤步骤而非排序步骤；时代错误检测是最简单的任务，但性能仍随时间线或实体重叠增加而下降。

Conclusion: 分配明确的推理预算有助于时间顺序任务，GPT-5在中等/高推理强度下能在所有长度上实现完美排序和条件排序，而低推理强度会随列表变长而性能下降。这些发现为LLMs在金融领域的实时应用提供了重要参考。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [13] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 本文提出了一种两阶段架构来减少Whisper模型在噪声环境下的幻觉错误，包括自适应层注意力增强编码器鲁棒性和多目标知识蒸馏抑制幻觉。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在噪声声学条件下经常出现幻觉错误，而现有方法主要关注音频预处理或转录后处理，对模型本身的修改探索不足。

Method: 两阶段方法：第一阶段通过自适应层注意力将编码器层分组为语义连贯的块，使用多头注意力融合块表示；第二阶段使用多目标知识蒸馏框架，在噪声音频上训练学生模型以对齐其语义和注意力分布。

Result: 在噪声语音基准测试中显著减少了幻觉和词错误率，同时在干净语音上保持了性能。

Conclusion: 自适应层注意力和知识蒸馏为在真实世界噪声条件下提高Whisper可靠性提供了原则性策略。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [14] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt是一个基于大语言模型的物联网设备操作推荐系统，通过持续预训练、多任务微调、直接偏好优化和置信度控制机制，显著提升了推荐性能，并在小米家庭应用中成功部署。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型在处理物联网设备操作时面临复杂操作逻辑、多样化用户偏好和对次优建议敏感等问题，限制了其适用性。

Method: 1. 通过持续预训练和多任务微调为LLM装备物联网操作领域知识；2. 使用直接偏好优化使微调后的LLM与特定用户偏好对齐；3. 设计基于置信度的曝光控制机制避免低质量推荐带来的负面用户体验。

Result: 在实验中，DevPiolt在所有数据集上显著优于基线方法，所有指标平均提升69.5%。在线部署结果显示，独立访客设备覆盖率增加21.6%，页面浏览接受率增加29.1%。

Conclusion: DevPiolt成功解决了物联网设备操作推荐的关键挑战，在实际应用中取得了显著效果，证明了其可行性和有效性。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [15] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的时间序列预测框架，用于预测区域层面的Airbnb关键指标（收入、预订天数、预订数量），通过结合房源特征和外部上下文因素构建区域表示，使用LLM生成区域嵌入，并采用先进的时间序列模型进行1-3个月的预测。


<details>
  <summary>Details</summary>
Motivation: 短期租赁平台（如Airbnb）的扩张严重扰乱了当地住房市场，导致租金上涨和住房负担能力问题。准确预测区域Airbnb市场趋势可为政策制定者和城市规划者提供关键见解以减轻这些影响。

Method: 使用滑动窗口方法，将结构化表格数据转换为基于提示的LLM输入，生成全面的区域嵌入，然后将这些嵌入输入到先进的时间序列模型（RNN、LSTM、Transformer）中，以更好地捕捉复杂的时空动态。

Result: 在首尔Airbnb数据集上的实验表明，与传统基线（包括传统统计和机器学习模型）相比，该方法将平均RMSE和MAE降低了约48%。

Conclusion: 该框架不仅提高了预测准确性，还为检测供应过剩区域和支持数据驱动的城市政策决策提供了实用见解。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [16] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind是一个新颖的知识图谱推理框架，通过选择性引导LLM使用重要推理路径来增强忠实和可解释的推理。它采用"检索-优先化-推理"范式，包含路径优先化机制和双阶段训练策略，在复杂推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM-based知识图谱推理方法的两个关键限制：1) 无差别提取推理路径可能引入无关噪声误导LLM；2) 动态探索推理路径需要高检索需求和频繁LLM调用。

Method: PathMind框架遵循"检索-优先化-推理"范式：1) 检索模块从KG中提取查询子图；2) 路径优先化机制使用语义感知路径优先级函数识别重要推理路径；3) 通过任务特定指令调优和路径偏好对齐的双阶段训练策略生成准确响应。

Result: 在基准数据集上的广泛实验表明，PathMind始终优于竞争基线方法，特别是在复杂推理任务上，通过识别关键推理路径，使用更少的输入token实现更好性能。

Conclusion: PathMind通过选择性引导LLM使用重要推理路径，有效解决了现有方法的局限性，在知识图谱推理任务中实现了更忠实、可解释和高效的推理性能。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [17] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: 本文通过重新表述和扰动经典CSPLib问题来测试LLMs的模型生成能力，发现虽然LLMs能生成语法有效且语义合理的模型，但在上下文和语言变化下性能急剧下降，表明其理解浅层且对措辞敏感。


<details>
  <summary>Details</summary>
Motivation: 检验LLMs自动生成优化和约束编程模型的能力是否源于数据污染而非真正推理，因为许多标准CP问题可能已包含在训练数据中。

Method: 系统性地重新表述和扰动一组知名CSPLib问题，保持结构不变但修改上下文并引入误导元素，然后比较三个代表性LLMs在原始和修改描述下生成的模型。

Result: LLMs能生成语法有效且语义合理的模型，但在上下文和语言变化下性能急剧下降，显示出浅层理解和对措辞的敏感性。

Conclusion: LLMs在自动生成优化模型方面的成功可能更多源于数据污染而非真正的推理能力，其性能对问题表述的变化非常敏感。

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [18] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 该研究探讨了在LLM对齐过程中考虑多元社会价值观的影响，发现人口统计因素和技术设计选择都会显著影响模型行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的对齐决策往往忽视人类社会的多样性，需要研究如何将多元价值观纳入对齐流程。

Method: 收集美国和德国参与者的对齐数据，在不同维度上评估LLM响应，并使用不同社会群体的偏好微调多个大语言模型和大推理模型，同时改变评分尺度、分歧处理方法和优化技术。

Result: 发现系统性人口统计效应：男性参与者对毒性的评分比女性低18%；保守派和黑人参与者对情感意识的评分分别比自由派和白人参与者高27.9%和44%。技术设计选择也显示强烈影响：保留评分者分歧比多数投票实现约53%更大的毒性减少；5点量表比二元格式产生约22%更多减少；DPO在多值优化中持续优于GRPO。

Conclusion: 这些发现为回答关键问题提供了初步步骤：对齐应如何平衡专家驱动和用户驱动的信号，以确保安全性和公平代表性。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [19] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 提出基于率失真理论和最优传输几何的知识图谱构建与优化框架，通过FGW耦合量化语义失真，使用精炼操作最小化率失真拉格朗日量，生成紧凑且保留信息的知识图谱。


<details>
  <summary>Details</summary>
Motivation: 解决将非结构化教育材料转换为捕捉关键教学内容的知识图谱的困难，为AI辅助教育中的个性化学习提供理论基础。

Method: 将讲座内容建模为度量-测度空间，使用Fused Gromov-Wasserstein耦合对齐候选知识图谱以量化语义失真，通过添加、合并、拆分、移除和重连等精炼操作最小化率失真拉格朗日量。

Result: 在数据科学讲座上的原型应用显示，从优化后的知识图谱生成的多项选择题在15个质量标准上持续优于原始笔记生成的问题。

Conclusion: 本研究为个性化AI辅助教育中的信息论知识图谱优化建立了理论基础。

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 本论文将GCS算法的形式模型扩展到实现近似的假设下，通过将单向测量范式改为双向测量范式，移除了许多限制，同时保持了GCS的核心行为。


<details>
  <summary>Details</summary>
Motivation: 扩展GCS算法的形式模型，使其在更接近实际实现的假设下运行，移除先前工作中为证明性能而施加的限制，提高算法的实用性和准确性。

Method: 将单向测量范式替换为双向测量范式，取消统一链路长度的要求，形式化建模频率源，对算法估计误差的不同组成部分进行细粒度区分。

Result: 显著降低了不确定性对算法估计误差的贡献，从每链路延迟的数量级降低到每链路延迟的10%到0.1%范围内，并给出了GCS局部和全局偏差的匹配上界。

Conclusion: 通过改变测量范式和移除限制，成功扩展了GCS算法的形式模型，使其在更现实的假设下运行，同时显著提高了估计精度并保持了算法的核心行为。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [21] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia是一个GPU即服务模型和架构，通过动态执行模式识别和运行时评估，为异构环境中的无服务器AI工作负载提供SLO感知、成本高效的硬件加速。


<details>
  <summary>Details</summary>
Motivation: 当前平台在管理硬件加速方面存在困难，静态用户-设备分配无法在变化负载或放置下确保SLO合规性，一次性动态选择往往导致次优或成本低效的配置。

Method: Gaia结合了轻量级执行模式标识器（在部署时检查函数代码并发出四种执行模式之一）和动态函数运行时（持续重新评估用户定义的SLO以在CPU和GPU后端之间进行升级或降级）。

Result: 评估显示，Gaia无缝选择最适合工作负载的硬件加速，将端到端延迟降低高达95%。

Conclusion: Gaia能够为异构环境中的无服务器AI实现SLO感知、成本高效的加速。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [22] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: TT-Edge是一个软硬件协同设计框架，通过在边缘AI处理器上专门优化张量训练分解(TTD)的计算流程，解决了TTD在资源受限设备上的延迟和能耗问题。


<details>
  <summary>Details</summary>
Motivation: 分布式学习在资源受限边缘设备上的需求日益增长，需要高效的设备端模型压缩。TTD虽然提供高压缩比和低精度损失，但其重复的SVD和矩阵乘法操作在低功耗处理器上会产生显著的延迟和能耗开销。

Method: TT-Edge将SVD分解为双对角化和对角化两个阶段，将计算密集型任务卸载到专门的TTD引擎。该引擎与现有的GEMM加速器紧密集成，减少频繁的矩阵向量传输。采用轻量级设计，重用GEMM资源并使用共享浮点单元。

Result: 在基于RISC-V的边缘AI处理器上实现，压缩ResNet-32模型时相比仅使用GEMM的基线获得1.7倍加速，总体能耗降低40.2%。总功耗仅增加4%，硬件开销最小。

Conclusion: TT-Edge通过软硬件协同设计有效解决了TTD在边缘环境中基于压缩的延迟和能耗瓶颈问题。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [23] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: VOLT是一个针对开源GPU的轻量级编译器工具链，旨在解决现有GPU程序在开源GPU架构上执行和优化的技术挑战。


<details>
  <summary>Details</summary>
Motivation: 开源GPU架构需要复杂的编译器框架来支持SIMT执行，但现有工具链在技术复杂度和开发成本方面存在不足。

Method: 采用分层设计，在中间端集中处理SIMT相关分析和优化，支持多种前端语言和开源GPU硬件，确保可扩展性。

Result: VOLT能够跨多个抽象层次生成和优化SIMT代码，并通过ISA扩展和主机运行时API的案例研究验证了其扩展能力。

Conclusion: VOLT为开源GPU开发提供了可扩展的编译器框架，能够适应不断发展的GPU架构需求。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [24] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: ADP是一个完全在GPU上运行的框架，通过自动动态精度技术，使用低精度单元（如FP4）模拟双精度矩阵乘法，在保证FP64精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代GPU硬件转向低精度格式（FP16、FP8、FP4），双精度计算吞吐量相对较低。需要利用低精度单元模拟双精度精度，同时保持高性能。

Method: 提出指数跨度容量（ESC）硬件无关估计器确定分解参数，集成异常处理、运行时启发式和原生FP64回退机制，并使用无符号整数切片方案提高表示效率。

Result: 在55位尾数设置下，相比原生FP64 GEMM，在NVIDIA Blackwell GB200和RTX Pro 6000 Blackwell Server Edition上分别实现2.3倍和13.2倍加速，运行时间开销低于10%。

Conclusion: 低精度加速器可以作为高保真、高性能科学计算工作负载的实用生产就绪基础，证明了在保持FP64精度的同时实现显著性能提升的可行性。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [25] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出语义多路复用新概念，将多路复用从比特级提升到任务级，通过合并多个任务相关的压缩表示来创建单一语义表示，从而在语义层扩展有效自由度，实现在不增加天线或带宽的情况下同时处理更多任务。


<details>
  <summary>Details</summary>
Motivation: 移动设备需要在无线边缘并行执行多个计算任务，现有通信系统仅支持比特级并行传输，这从根本上限制了可并发处理的任务数量。

Method: 提出语义多路复用方法，将多个任务相关的压缩表示合并为单一语义表示，在Jetson Orin Nano和毫米波软件定义无线电实验平台上进行原型实现，并在图像分类和情感分析任务上测试性能。

Result: 实验表明语义多路复用可在语义层联合处理多个任务同时保持足够的任务准确性。例如，在4×4信道上将多路复用任务数从2增加到8时，图像分类准确率下降不到4%。相比基线方法，延迟降低8倍，能耗降低25倍，通信负载降低54倍。

Conclusion: 语义多路复用通过将多路复用从比特级转移到任务级，有效扩展了语义层的自由度，显著提升了并行任务处理能力，同时保持了性能表现。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [26] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: MPI派生数据类型(DDTs)在实际性能表现上存在争议，本文通过三个2D应用对四种MPI实现进行跨实现评估，发现DDTs性能表现不一，没有一种策略在所有程序、语义和MPI栈中占主导地位。


<details>
  <summary>Details</summary>
Motivation: MPI派生数据类型承诺简化非连续数据的无拷贝通信，但其实际性能仍存在争议，且通常只针对单一MPI栈进行报告，需要进行跨实现评估。

Method: 使用三个2D应用：Jacobi CFD求解器、康威生命游戏和基于格点的图像重建，每个应用分别实现基础版本（手动打包）和DDT版本。在1-4个进程上运行强扩展和弱扩展测试，评估四种MPI实现：MPICH、Open MPI、Intel MPI和MVAPICH2。

Result: 结果混合：DDTs在某些情况下最快（如图像重建在Intel MPI和MPICH上），但在其他栈上可能最慢（如Open MPI和MVAPICH2）。CFD求解器中基础版本通常优于DDTs，而生命游戏中排名因MPI库而异。还观察到栈特定异常，如MPICH在DDT邻域和持久模式下的减速。

Conclusion: 没有一种策略在所有程序、语义和MPI栈中占主导地位，DDTs的性能可移植性无法保证。建议在目标MPI实现和通信模式下同时分析DDT和手动打包设计的性能。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [27] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个简化的 CUDA 框架，通过八个核心原语和统一编程模板，系统性地指导最优多 GPU 内核设计，在 Hopper 和 Blackwell 架构上显著提升各种并行工作负载的性能。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型规模扩大和硬件计算吞吐量提升超过互连带宽改进，GPU 间通信已成为现代 AI 工作负载的主要瓶颈。现有系统通过计算-通信重叠来缓解，但往往无法在异构工作负载和新加速器上达到理论峰值性能。

Method: PK 扩展 ThunderKittens 框架，通过八个核心原语和统一编程模板体现多 GPU 内核设计原则，这些原则基于对数据传输机制、资源调度和设计开销等影响多 GPU 性能因素的综合分析。

Result: 在 Hopper 和 Blackwell 架构上验证，仅用不到 50 行设备代码，PK 实现了：数据并行和 tensor 并行工作负载最高 2.33 倍加速，序列并行工作负载 4.08 倍加速，专家并行工作负载 1.22 倍加速。

Conclusion: PK 框架证明了一小组简单、可重用的原则可以系统性地指导最优多 GPU 内核设计，显著简化了重叠多 GPU 内核的开发过程，并在多种并行工作负载上实现了显著的性能提升。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [28] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe是一个容错的张量并行服务系统，通过循环KVCache放置、混合注意力和细粒度负载感知路由等技术，在GPU故障时维持高性能LLM推理。


<details>
  <summary>Details</summary>
Motivation: 传统张量并行(TP)在GPU故障时会导致执行中断、昂贵的KVCache重新计算以及长期的计算和内存不平衡，系统脆弱性高。

Method: 采用循环KVCache放置实现均匀内存利用，混合注意力结合张量和数据并行消除滞后，细粒度负载感知路由动态平衡请求，并采用主动KVCache备份和按需权重恢复。

Result: 在8xH100 DGX系统上，相比标准故障处理方法，FailSafe实现了高达2倍的吞吐量和两个数量级更低的恢复延迟，即使最多三个GPU故障也能维持高吞吐量和平衡利用率。

Conclusion: FailSafe展示了在动态不可靠硬件条件下实现稳健高效LLM服务的能力。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [29] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache是一个资源感知的张量缓存和迁移系统，通过智能协调GPU、CPU和NVMe之间的内存使用来加速大语言模型训练，相比现有方法可达到2倍训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决云端大语言模型训练面临的内存瓶颈问题，现有GPU内存卸载方法存在高张量迁移延迟和设备内存利用率低的问题，导致训练时间增加和云成本上升。

Method: 10Cache通过分析张量执行顺序构建预取策略，根据张量大小分布在固定内存中分配内存缓冲区，并重用内存缓冲区以最小化分配开销。

Result: 在多样化LLM工作负载中，相比最先进的卸载方法，10Cache实现了高达2倍的训练速度提升，GPU缓存命中率提高86.6倍，CPU和GPU内存利用率分别提高2.15倍和1.33倍。

Conclusion: 10Cache是优化云端LLM训练吞吐量和资源效率的实用且可扩展的解决方案。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [30] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 本文研究了跨组织联邦学习中参与者故障对模型质量的影响，重点关注故障时机、数据分布以及评估结果的影响，发现在高度数据倾斜情况下评估结果过于乐观，故障时机也会影响模型质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在跨组织协作场景中需要保证系统可靠性，但参与者可能因各种原因故障。虽然跨设备联邦学习中已有相关研究，但跨组织联邦学习中的参与者故障影响研究相对较少。

Method: 通过广泛研究分析跨组织联邦学习中参与者故障对模型质量的影响，重点关注故障时机、数据分布以及评估结果等关键因素。

Result: 研究表明，在高度数据倾斜情况下，评估结果过于乐观，掩盖了真实影响；同时故障时机对训练模型质量有显著影响。

Conclusion: 研究结果为构建鲁棒联邦学习系统的研究人员和软件架构师提供了重要见解，强调了在跨组织联邦学习中考虑参与者故障影响的重要性。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [31] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks是一种新颖的锁算法，具有简单性、恒定时间到达和解锁路径、FIFO准入顺序、空间效率高、在争用情况下产生较少一致性流量等特点。


<details>
  <summary>Details</summary>
Motivation: 开发一种性能与最先进锁算法相当，但对运行时环境约束更少、更容易集成到现有系统的锁算法。

Method: 提出Hapax Locks算法，该算法在线程间不转移指针所有权，提供恒定时间的到达和解锁操作。

Result: Hapax Locks在延迟和可扩展性方面与最先进的锁算法性能相当，同时减少了运行时环境的依赖和约束。

Conclusion: Hapax Locks是一种简单高效的锁算法，特别适合集成到现有系统或现有API下，具有优异的性能和易用性。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [32] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer系统通过利用相同提示请求间的输出长度和生成模式相似性，解决了强化学习中rollout阶段的长尾延迟和资源利用问题，显著提升了吞吐量和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有同步强化学习系统在rollout阶段面临严重性能瓶颈，包括长尾延迟和资源利用率低下，主要由于工作负载不平衡导致。

Method: Seer引入了三项关键技术：分割rollout实现动态负载均衡、上下文感知调度、自适应分组推测解码，利用相同提示请求间的相似性来优化性能。

Result: 在生产级RL工作负载上的评估显示，相比最先进的同步RL系统，Seer将端到端rollout吞吐量提高了74%到97%，长尾延迟降低了75%到93%。

Conclusion: Seer系统通过创新的在线上下文学习机制，显著加速了强化学习训练迭代，有效解决了rollout阶段的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>
